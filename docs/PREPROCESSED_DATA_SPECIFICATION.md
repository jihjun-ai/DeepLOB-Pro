# é è™•ç†æ•¸æ“šè¦æ ¼èªªæ˜æ›¸

**ç‰ˆæœ¬**: v2.0
**æœ€å¾Œæ›´æ–°**: 2025-10-23
**é©ç”¨è…³æœ¬**: `scripts/preprocess_single_day.py`
**ç›®æ¨™**: æä¾›å®Œæ•´çš„é è™•ç†æ•¸æ“šæ ¼å¼ã€å…§å®¹ã€ç”¨é€”èªªæ˜

---

## ğŸ“‹ ç›®éŒ„

- [æ¦‚è¿°](#æ¦‚è¿°)
- [ç›®éŒ„çµæ§‹](#ç›®éŒ„çµæ§‹)
- [NPZ æ–‡ä»¶æ ¼å¼](#npz-æ–‡ä»¶æ ¼å¼)
- [é è™•ç†æµç¨‹](#é è™•ç†æµç¨‹)
- [æ•¸æ“šè®€å–ç¯„ä¾‹](#æ•¸æ“šè®€å–ç¯„ä¾‹)
- [å¸¸è¦‹å•é¡Œ](#å¸¸è¦‹å•é¡Œ)

---

## æ¦‚è¿°

### ä»€éº¼æ˜¯é è™•ç†æ•¸æ“šï¼Ÿ

é è™•ç†æ•¸æ“šæ˜¯å¾åŸå§‹ LOB (Limit Order Book) TXT æ–‡ä»¶è™•ç†å¾Œçš„**ä¸­é–“æ•¸æ“š**ï¼Œç”¨æ–¼ï¼š
1. **å¿«é€Ÿæª¢æŸ¥**ï¼šæŸ¥çœ‹æ¯æ—¥ã€æ¯æª”è‚¡ç¥¨çš„æ¨™ç±¤åˆ†å¸ƒ
2. **çŸ¥æƒ…é¸æ“‡**ï¼šåœ¨ç”Ÿæˆè¨“ç·´æ•¸æ“šå‰ï¼Œä¾æ¨™ç±¤åˆ†å¸ƒé¸æ“‡åˆé©çš„è‚¡ç¥¨
3. **é¿å…é‡è¤‡è™•ç†**ï¼šé è™•ç†ä¸€æ¬¡ï¼Œå¯å¤šæ¬¡ç”¨æ–¼ä¸åŒè¨“ç·´é…ç½®

### è™•ç†éšæ®µ

```
åŸå§‹ TXT â†’ preprocess_single_day.py â†’ NPZ (é è™•ç†æ•¸æ“š) â†’ extract_tw_stock_data_v6.py â†’ è¨“ç·´æ•¸æ“š
         (æ¸…æ´—ã€èšåˆã€æ¨™ç±¤é è¦½)                      (æ™‚é–“åºåˆ—çª—å£ã€æ¨™æº–åŒ–)
```

---

## ç›®éŒ„çµæ§‹

```
data/
â””â”€â”€ preprocessed_v5_1hz/              # é è™•ç†æ•¸æ“šæ ¹ç›®éŒ„
    â””â”€â”€ daily/                        # æŒ‰æ—¥æœŸçµ„ç¹”
        â”œâ”€â”€ 20250901/                 # å–®ä¸€äº¤æ˜“æ—¥
        â”‚   â”œâ”€â”€ 0050.npz              # å…ƒå¤§å°ç£50
        â”‚   â”œâ”€â”€ 2330.npz              # å°ç©é›»
        â”‚   â”œâ”€â”€ 2317.npz              # é´»æµ·
        â”‚   â”œâ”€â”€ ...                   # å…¶ä»–è‚¡ç¥¨
        â”‚   â””â”€â”€ summary.json          # ç•¶æ—¥è™•ç†æ‘˜è¦
        â”œâ”€â”€ 20250902/
        â”‚   â””â”€â”€ ...
        â””â”€â”€ ...
```

### ç›®éŒ„èªªæ˜

| è·¯å¾‘ | èªªæ˜ |
|------|------|
| `preprocessed_v5_1hz` | é è™•ç†æ ¹ç›®éŒ„ï¼ˆv5 é…ç½® + 1Hz èšåˆï¼‰ |
| `daily/` | æŒ‰æ—¥æœŸçµ„ç¹”çš„å­ç›®éŒ„ |
| `daily/20250901/` | å–®ä¸€äº¤æ˜“æ—¥ç›®éŒ„ |
| `*.npz` | å€‹è‚¡é è™•ç†æ•¸æ“šï¼ˆNumPy å£“ç¸®æ ¼å¼ï¼‰ |
| `summary.json` | ç•¶æ—¥è™•ç†æ‘˜è¦ï¼ˆè‚¡ç¥¨æ•¸ã€æ¨™ç±¤çµ±è¨ˆç­‰ï¼‰ |

---

## NPZ æ–‡ä»¶æ ¼å¼

### æ–‡ä»¶å…§å®¹

æ¯å€‹ `.npz` æ–‡ä»¶åŒ…å«ä»¥ä¸‹é™£åˆ—å’Œå…ƒæ•¸æ“šï¼š

```python
import numpy as np

data = np.load('data/preprocessed_v5_1hz/daily/20250901/2330.npz', allow_pickle=True)

# å¯ç”¨çš„ keys:
data.keys()
# dict_keys(['features', 'mids', 'bucket_event_count', 'bucket_mask', 'metadata', 'labels'])
```

### 1. features (å¿…è¦)

**é¡å‹**: `np.ndarray`, shape `(T, 20)`, dtype `float32`
**èªªæ˜**: LOB ç‰¹å¾µçŸ©é™£ï¼ˆ5 æª”è²·è³£åƒ¹é‡ï¼‰

| åˆ—ç´¢å¼• | ç‰¹å¾µåç¨± | èªªæ˜ |
|-------|---------|------|
| 0-4   | ask_price_1 ~ ask_price_5 | è³£æ–¹ 5 æª”åƒ¹æ ¼ |
| 5-9   | ask_volume_1 ~ ask_volume_5 | è³£æ–¹ 5 æª”é‡ |
| 10-14 | bid_price_1 ~ bid_price_5 | è²·æ–¹ 5 æª”åƒ¹æ ¼ |
| 15-19 | bid_volume_1 ~ bid_volume_5 | è²·æ–¹ 5 æª”é‡ |

**ç‰¹é»**:
- âœ… å·²æ¸…æ´—ç•°å¸¸å€¼ï¼ˆåƒ¹æ ¼/é‡ç‚º 0 æˆ–ç•°å¸¸è·³å‹•ï¼‰
- âœ… æŒ‰æ™‚é–“èšåˆï¼ˆ1Hz æ¨¡å¼ï¼šæ¯ç§’ 1 å€‹å¿«ç…§ï¼‰
- âŒ æœªæ¨™æº–åŒ–ï¼ˆåŸå§‹åƒ¹æ ¼å’Œé‡ï¼‰

**ç¯„ä¾‹**:
```python
features = data['features']
print(f"Shape: {features.shape}")  # (15957, 20)
print(f"ç¬¬ 1 ç­†è³£ 1 åƒ¹: {features[0, 0]}")  # 587.0
print(f"ç¬¬ 1 ç­†è²· 1 åƒ¹: {features[0, 10]}")  # 586.0
```

---

### 2. mids (å¿…è¦)

**é¡å‹**: `np.ndarray`, shape `(T,)`, dtype `float64`
**èªªæ˜**: ä¸­é–“åƒ¹æ™‚åºï¼ˆè²· 1 åƒ¹ + è³£ 1 åƒ¹ï¼‰/ 2

**å…¬å¼**:
```python
mid_price = (bid_price_1 + ask_price_1) / 2
```

**ç”¨é€”**:
- è¨ˆç®—æ¨™ç±¤ï¼ˆTriple-Barrierï¼‰
- è¦–è¦ºåŒ–åƒ¹æ ¼èµ°å‹¢
- è¨ˆç®—å ±é…¬ç‡

**ç¯„ä¾‹**:
```python
mids = data['mids']
print(f"Shape: {mids.shape}")  # (15957,)
print(f"åƒ¹æ ¼ç¯„åœ: {mids.min():.2f} ~ {mids.max():.2f}")
# åƒ¹æ ¼ç¯„åœ: 585.50 ~ 592.00
```

---

### 3. bucket_event_count (é¸ç”¨)

**é¡å‹**: `np.ndarray`, shape `(T,)`, dtype `int32`
**èªªæ˜**: æ¯å€‹æ™‚é–“æ¡¶ï¼ˆbucketï¼‰å…§çš„äº‹ä»¶æ•¸é‡

**ç”¨é€”**:
- æª¢æŸ¥æ•¸æ“šè³ªé‡ï¼ˆäº‹ä»¶æ•¸å¤ªå°‘å¯èƒ½ä¸å¯é ï¼‰
- è­˜åˆ¥é«˜é »äº¤æ˜“æ™‚æ®µ

**ç¯„ä¾‹**:
```python
bucket_event_count = data['bucket_event_count']
print(f"å¹³å‡æ¯ç§’äº‹ä»¶æ•¸: {bucket_event_count.mean():.2f}")  # 5.3
print(f"æœ€å¤§äº‹ä»¶æ•¸: {bucket_event_count.max()}")  # 120
```

---

### 4. bucket_mask (é¸ç”¨)

**é¡å‹**: `np.ndarray`, shape `(T,)`, dtype `int32`
**èªªæ˜**: æ™‚é–“æ¡¶é®ç½©ï¼ˆ0=ç¼ºå¤±æ•¸æ“š, 1=æœ‰æ•ˆæ•¸æ“šï¼‰

**ç”¨é€”**:
- æ¨™è¨˜ç¼ºå¤±æ•¸æ“šä½ç½®
- éæ¿¾ç„¡æ•ˆæ¨£æœ¬

**ç¯„ä¾‹**:
```python
bucket_mask = data['bucket_mask']
valid_ratio = bucket_mask.mean()
print(f"æœ‰æ•ˆæ•¸æ“šæ¯”ä¾‹: {valid_ratio:.2%}")  # 98.5%
```

---

### 5. metadata (å¿…è¦)

**é¡å‹**: JSON å­—ä¸²ï¼ˆéœ€è§£æï¼‰
**èªªæ˜**: å…ƒæ•¸æ“šï¼ŒåŒ…å«è‚¡ç¥¨è³‡è¨Šã€éæ¿¾ç‹€æ…‹ã€æ¨™ç±¤é è¦½ç­‰

**è§£ææ–¹å¼**:
```python
import json
metadata_str = str(data['metadata'])
metadata = json.loads(metadata_str)
```

**å®Œæ•´æ¬„ä½åˆ—è¡¨**:

| æ¬„ä½ | é¡å‹ | èªªæ˜ | ç¯„ä¾‹ |
|------|------|------|------|
| **åŸºæœ¬è³‡è¨Š** |
| `symbol` | str | è‚¡ç¥¨ä»£ç¢¼ | `"2330"` |
| `date` | str | äº¤æ˜“æ—¥æœŸ | `"20250901"` |
| `n_points` | int | æ•¸æ“šé»æ•¸ | `15957` |
| **åƒ¹æ ¼çµ±è¨ˆ** |
| `high` | float | æœ€é«˜åƒ¹ | `592.0` |
| `low` | float | æœ€ä½åƒ¹ | `585.5` |
| `open` | float | é–‹ç›¤åƒ¹ | `587.0` |
| `close` | float | æ”¶ç›¤åƒ¹ | `590.0` |
| `range_pct` | float | åƒ¹æ ¼ç¯„åœï¼ˆ%ï¼‰ | `1.11` |
| `return_pct` | float | æ—¥å ±é…¬ç‡ï¼ˆ%ï¼‰ | `0.51` |
| **éæ¿¾è³‡è¨Š** |
| `pass_filter` | bool | æ˜¯å¦é€šééæ¿¾ | `true` |
| `filter_threshold` | float | éæ¿¾é–¾å€¼ | `0.005` |
| `filter_method` | str | éæ¿¾æ–¹æ³• | `"range_pct"` |
| `filter_reason` | str | éæ¿¾åŸå›  | `"pass"` æˆ– `"range_pct too low"` |
| **è™•ç†åƒæ•¸** |
| `sampling_mode` | str | èšåˆæ¨¡å¼ | `"time_agg"` |
| `bucket_seconds` | int | æ™‚é–“æ¡¶å¤§å°ï¼ˆç§’ï¼‰ | `1` |
| `agg_reducer` | str | èšåˆæ–¹å¼ | `"last"` |
| `ffill_limit` | int | å‰å‘å¡«å……ä¸Šé™ | `10` |
| **æ•¸æ“šè³ªé‡** |
| `raw_events` | int | åŸå§‹äº‹ä»¶æ•¸ | `85432` |
| `aggregated_points` | int | èšåˆå¾Œé»æ•¸ | `15957` |
| `ffill_ratio` | float | å¡«å……æ¯”ä¾‹ | `0.015` |
| `missing_ratio` | float | ç¼ºå¤±æ¯”ä¾‹ | `0.012` |
| `multi_event_ratio` | float | å¤šäº‹ä»¶æ¯”ä¾‹ | `0.65` |
| `max_gap_sec` | int | æœ€å¤§é–“éš”ï¼ˆç§’ï¼‰ | `5` |
| `n_seconds` | int | ç¸½ç§’æ•¸ | `16200` |
| **æ™‚é–“æˆ³è¨˜** |
| `processed_at` | str | è™•ç†æ™‚é–“ | `"2025-10-23 10:30:45"` |
| **æ¨™ç±¤é è¦½** |
| `label_preview` | dict | æ¨™ç±¤çµ±è¨ˆ | è¦‹ä¸‹æ–¹ |

**label_preview çµæ§‹**:
```json
{
    "total_labels": 15956,
    "down_count": 2273,
    "neutral_count": 11395,
    "up_count": 2288,
    "down_pct": 0.1425,
    "neutral_pct": 0.7142,
    "up_pct": 0.1434
}
```

**weight_strategies çµæ§‹** ğŸ†• (v2.0):
```json
{
    "balanced": {
        "class_weights": {"-1": 1.11, "0": 2.38, "1": 1.11},
        "description": "Standard balanced weights (total / (n_classes * count))",
        "type": "class_weight"
    },
    "sqrt_balanced": {
        "class_weights": {"-1": 1.05, "0": 1.54, "1": 1.05},
        "description": "Square root of balanced weights (gentler, more stable)",
        "type": "class_weight"
    },
    "effective_num_0999": {
        "class_weights": {"-1": 1.02, "0": 1.92, "1": 1.04},
        "description": "Effective Number of Samples (beta=0.999, CVPR 2019)",
        "type": "class_weight"
    },
    "uniform": {
        "class_weights": {"-1": 1.0, "0": 1.0, "1": 1.0},
        "description": "No weighting (all classes equal)",
        "type": "class_weight"
    },
    "focal_loss": {
        "type": "focal",
        "gamma": 2.0,
        "class_weights": {"-1": 1.0, "0": 1.0, "1": 1.0},
        "description": "Use Focal Loss (gamma=2.0) during training"
    }
}
```

**weight_strategies èªªæ˜**:
- **ç”¨é€”**: é å…ˆè¨ˆç®—å¤šç¨®æ¬Šé‡ç­–ç•¥ï¼Œè¨“ç·´æ™‚éˆæ´»é¸æ“‡
- **é¡å‹**: åŒ…å« 11 ç¨®ç­–ç•¥ï¼ˆbalanced, sqrt_balanced, log_balanced, effective_num_09/099/0999/09999, cb_focal_099/0999, uniform, focal_lossï¼‰
- **å„ªé»**: é¿å…è¨“ç·´æ™‚é‡è¤‡è¨ˆç®—ï¼Œæ”¯æ´å¿«é€Ÿå¯¦é©—ä¸åŒæ¬Šé‡ç­–ç•¥
- **æ–‡ä»¶å¢åŠ **: æ¯å€‹ç­–ç•¥ç´„ 100 bytesï¼Œ11 å€‹ç­–ç•¥ç´„ 1.1 KB/è‚¡ç¥¨ï¼ˆå¹¾ä¹å¯å¿½ç•¥ï¼‰

**ä½¿ç”¨ç¯„ä¾‹**:
```python
# è¼‰å…¥æ¬Šé‡ç­–ç•¥
metadata = json.loads(str(data['metadata']))
weight_strategies = metadata.get('weight_strategies', {})

# é¸æ“‡ç­–ç•¥
strategy_name = 'effective_num_0999'  # æˆ–å¾é…ç½®æ–‡ä»¶è®€å–
strategy = weight_strategies[strategy_name]

# ä½¿ç”¨æ¬Šé‡
class_weights = strategy['class_weights']
print(f"Down weight: {class_weights['-1']}")
print(f"Neutral weight: {class_weights['0']}")
print(f"Up weight: {class_weights['1']}")

# ç”¨æ–¼è¨“ç·´
import torch
weight_tensor = torch.FloatTensor([
    class_weights['-1'],
    class_weights['0'],
    class_weights['1']
])
criterion = nn.CrossEntropyLoss(weight=weight_tensor)
```

**è®€å–ç¯„ä¾‹**:
```python
# åŸºæœ¬è³‡è¨Š
print(f"è‚¡ç¥¨: {metadata['symbol']}")
print(f"æ—¥æœŸ: {metadata['date']}")
print(f"æ•¸æ“šé»æ•¸: {metadata['n_points']}")

# åƒ¹æ ¼çµ±è¨ˆ
print(f"é–‹ç›¤: {metadata['open']}, æ”¶ç›¤: {metadata['close']}")
print(f"æ—¥å ±é…¬ç‡: {metadata['return_pct']:.2%}")

# éæ¿¾ç‹€æ…‹
if metadata['pass_filter']:
    print("âœ… é€šééæ¿¾")
else:
    print(f"âŒ æœªé€šééæ¿¾: {metadata['filter_reason']}")

# æ¨™ç±¤é è¦½
lp = metadata['label_preview']
print(f"\næ¨™ç±¤åˆ†å¸ƒ:")
print(f"  Down: {lp['down_pct']:.2%} ({lp['down_count']})")
print(f"  Neutral: {lp['neutral_pct']:.2%} ({lp['neutral_count']})")
print(f"  Up: {lp['up_pct']:.2%} ({lp['up_count']})")
```

---

### 6. labels (é¸ç”¨ï¼Œv2.0 æ–°å¢) ğŸ†•

**é¡å‹**: `np.ndarray`, shape `(T,)`, dtype `float32`
**èªªæ˜**: Triple-Barrier æ¨™ç±¤é™£åˆ—

**æ¨™ç±¤å€¼**:
- `-1`: Downï¼ˆåƒ¹æ ¼ä¸‹è·Œï¼‰
- `0`: Neutralï¼ˆåƒ¹æ ¼æŒå¹³ï¼‰
- `1`: Upï¼ˆåƒ¹æ ¼ä¸Šæ¼²ï¼‰
- `NaN`: æœªè¨ˆç®—ï¼ˆé‚Šç•Œé»ï¼‰

**è¨ˆç®—æ–¹å¼**:
- ä½¿ç”¨ `compute_label_preview()` å‡½æ•¸
- åŸºæ–¼ Triple-Barrier æ–¹æ³•
- åƒæ•¸ä¾†è‡ª `config_pro_v5_ml_optimal.yaml`

**ç¯„ä¾‹**:
```python
labels = data.get('labels')  # å¯èƒ½ä¸å­˜åœ¨ï¼ˆèˆŠç‰ˆ NPZï¼‰

if labels is not None:
    # çµ±è¨ˆæ¨™ç±¤åˆ†å¸ƒ
    unique, counts = np.unique(labels[~np.isnan(labels)], return_counts=True)
    for label, count in zip(unique, counts):
        print(f"Label {int(label)}: {count}")

    # Label -1: 2273
    # Label 0: 11395
    # Label 1: 2288
else:
    print("æ­¤ NPZ æ–‡ä»¶ä¸åŒ…å«æ¨™ç±¤æ•¸æ“šï¼ˆèˆŠç‰ˆæ ¼å¼ï¼‰")
```

**æ³¨æ„äº‹é …**:
- âš ï¸ åªæœ‰**é€šééæ¿¾**çš„è‚¡ç¥¨æ‰æœ‰æ¨™ç±¤
- âš ï¸ èˆŠç‰ˆ NPZï¼ˆv1.0ï¼‰ä¸åŒ…å«æ­¤æ¬„ä½
- âš ï¸ æ¨™ç±¤æ•¸é‡å¯èƒ½æ¯” `T` å°‘ï¼ˆé‚Šç•Œé»è¢«è¨­ç‚º NaNï¼‰

---

## é è™•ç†æµç¨‹

### å®Œæ•´æµç¨‹åœ–

```
åŸå§‹ TXT æ–‡ä»¶ (Ticker_20250901.txt)
    â”‚
    â”œâ”€ Step 1: æ¸…æ´—ç•°å¸¸å€¼
    â”‚   â”œâ”€ ç§»é™¤åƒ¹æ ¼/é‡ç‚º 0 çš„è¨˜éŒ„
    â”‚   â”œâ”€ ç§»é™¤ç•°å¸¸åƒ¹æ ¼è·³å‹•
    â”‚   â””â”€ ç§»é™¤ç„¡æ•ˆè‚¡ç¥¨ä»£ç¢¼
    â”‚
    â”œâ”€ Step 2: æ™‚é–“èšåˆï¼ˆ1Hzï¼‰
    â”‚   â”œâ”€ æŒ‰è‚¡ç¥¨ä»£ç¢¼åˆ†çµ„
    â”‚   â”œâ”€ æŒ‰ç§’èšåˆï¼ˆæ¯ç§’å–æœ€å¾Œä¸€ç­†ï¼‰
    â”‚   â”œâ”€ å‰å‘å¡«å……ç¼ºå¤±å€¼ï¼ˆæœ€å¤š 10 ç§’ï¼‰
    â”‚   â””â”€ è¨ˆç®—ä¸­é–“åƒ¹
    â”‚
    â”œâ”€ Step 3: å‹•æ…‹éæ¿¾é–¾å€¼æ±ºç­–
    â”‚   â”œâ”€ è¨ˆç®—å„è‚¡ç¥¨æ³¢å‹•ç‡ (range_pct)
    â”‚   â”œâ”€ å˜—è©¦å¤šå€‹é–¾å€¼ï¼ˆ0.003 ~ 0.010ï¼‰
    â”‚   â”œâ”€ æ¨¡æ“¬æ¨™ç±¤åˆ†å¸ƒ
    â”‚   â””â”€ é¸æ“‡æœ€æ¥è¿‘ç›®æ¨™åˆ†å¸ƒçš„é–¾å€¼ï¼ˆ30/40/30ï¼‰
    â”‚
    â”œâ”€ Step 4: è¨ˆç®—æ¨™ç±¤é è¦½ ğŸ†•
    â”‚   â”œâ”€ å°é€šééæ¿¾çš„è‚¡ç¥¨è¨ˆç®— Triple-Barrier æ¨™ç±¤
    â”‚   â”œâ”€ çµ±è¨ˆ down/neutral/up æ•¸é‡å’Œæ¯”ä¾‹
    â”‚   â””â”€ ä¿å­˜æ¨™ç±¤é™£åˆ—åˆ° NPZ
    â”‚
    â”œâ”€ Step 5: ä¿å­˜ NPZ
    â”‚   â”œâ”€ features, mids, bucket_event_count, bucket_mask
    â”‚   â”œâ”€ metadata (JSON)
    â”‚   â””â”€ labels (æ¨™ç±¤é™£åˆ—) ğŸ†•
    â”‚
    â””â”€ Step 6: ç”Ÿæˆ summary.json
        â”œâ”€ è™•ç†çµ±è¨ˆï¼ˆè‚¡ç¥¨æ•¸ã€éæ¿¾æ•¸ï¼‰
        â”œâ”€ æ•´é«”æ¨™ç±¤åˆ†å¸ƒ
        â””â”€ å€‹è‚¡åˆ—è¡¨
```

### é—œéµåƒæ•¸ï¼ˆä¾†è‡ª config_pro_v5_ml_optimal.yamlï¼‰

#### Triple-Barrier é…ç½®
```yaml
triple_barrier:
  pt_mult: 2.0              # ä¸Šé‚Šç•Œå€æ•¸
  sl_mult: 2.0              # ä¸‹é‚Šç•Œå€æ•¸
  max_holding: 200          # æœ€å¤§æŒæœ‰æ™‚é–“ï¼ˆç§’ï¼‰
  min_return: 0.0001        # æœ€å°å ±é…¬ç‡é–¾å€¼
  ewma_halflife: 60         # EWMA åŠè¡°æœŸï¼ˆç”¨æ–¼æ³¢å‹•ç‡ï¼‰
```

#### éæ¿¾é…ç½®
```yaml
filter:
  target_down_pct: 0.30     # ç›®æ¨™ Down æ¨™ç±¤æ¯”ä¾‹
  target_neutral_pct: 0.40  # ç›®æ¨™ Neutral æ¨™ç±¤æ¯”ä¾‹
  target_up_pct: 0.30       # ç›®æ¨™ Up æ¨™ç±¤æ¯”ä¾‹
  threshold_candidates:     # å€™é¸é–¾å€¼
    - 0.003
    - 0.005
    - 0.007
    - 0.010
```

#### èšåˆé…ç½®
```yaml
preprocessing:
  sampling_mode: "time_agg" # æ™‚é–“èšåˆæ¨¡å¼
  bucket_seconds: 1         # 1Hz (æ¯ç§’ 1 å€‹å¿«ç…§)
  agg_reducer: "last"       # èšåˆæ–¹å¼ï¼ˆå–æœ€å¾Œä¸€ç­†ï¼‰
  ffill_limit: 10           # å‰å‘å¡«å……ä¸Šé™ï¼ˆç§’ï¼‰
```

---

## æ•¸æ“šè®€å–ç¯„ä¾‹

### å®Œæ•´è®€å–ç¯„ä¾‹

```python
import numpy as np
import json
from pathlib import Path

def load_preprocessed_stock(npz_path: str):
    """
    è¼‰å…¥é è™•ç†è‚¡ç¥¨æ•¸æ“š

    Args:
        npz_path: NPZ æ–‡ä»¶è·¯å¾‘

    Returns:
        dict: åŒ…å«æ‰€æœ‰æ•¸æ“šçš„å­—å…¸
    """
    # è¼‰å…¥ NPZ
    data = np.load(npz_path, allow_pickle=True)

    # è§£æ metadata
    metadata_str = str(data['metadata'])
    metadata = json.loads(metadata_str)

    return {
        'features': data['features'],
        'mids': data['mids'],
        'bucket_event_count': data.get('bucket_event_count'),
        'bucket_mask': data.get('bucket_mask'),
        'labels': data.get('labels'),  # å¯èƒ½ç‚º None
        'metadata': metadata
    }

# ä½¿ç”¨ç¯„ä¾‹
npz_path = 'data/preprocessed_v5_1hz/daily/20250901/2330.npz'
stock_data = load_preprocessed_stock(npz_path)

# æª¢æŸ¥æ•¸æ“š
print(f"è‚¡ç¥¨: {stock_data['metadata']['symbol']}")
print(f"æ•¸æ“šé»æ•¸: {len(stock_data['mids'])}")
print(f"æ˜¯å¦æœ‰æ¨™ç±¤: {'æ˜¯' if stock_data['labels'] is not None else 'å¦'}")
```

### æ‰¹æ¬¡è®€å–ç¯„ä¾‹

```python
from pathlib import Path

def scan_daily_directory(daily_dir: str):
    """æƒææ—¥æœŸç›®éŒ„ï¼Œè¿”å›æ‰€æœ‰è‚¡ç¥¨"""
    daily_path = Path(daily_dir)
    npz_files = list(daily_path.glob("*.npz"))

    stocks = {}
    for npz_file in npz_files:
        symbol = npz_file.stem
        stocks[symbol] = str(npz_file)

    return stocks

# ä½¿ç”¨ç¯„ä¾‹
daily_dir = 'data/preprocessed_v5_1hz/daily/20250901'
stocks = scan_daily_directory(daily_dir)

print(f"æ‰¾åˆ° {len(stocks)} æª”è‚¡ç¥¨:")
for symbol in sorted(stocks.keys())[:10]:
    print(f"  - {symbol}")
```

### æ¨™ç±¤åˆ†å¸ƒåˆ†æç¯„ä¾‹

```python
import numpy as np

def analyze_label_distribution(daily_dir: str):
    """åˆ†ææ•´å€‹æ—¥æœŸç›®éŒ„çš„æ¨™ç±¤åˆ†å¸ƒ"""
    stocks = scan_daily_directory(daily_dir)

    overall_counts = {-1: 0, 0: 0, 1: 0}
    stocks_with_labels = 0

    for symbol, npz_path in stocks.items():
        data = load_preprocessed_stock(npz_path)
        labels = data.get('labels')

        if labels is not None:
            stocks_with_labels += 1
            # çµ±è¨ˆæ¨™ç±¤ï¼ˆæ’é™¤ NaNï¼‰
            valid_labels = labels[~np.isnan(labels)]
            unique, counts = np.unique(valid_labels, return_counts=True)

            for label, count in zip(unique, counts):
                overall_counts[int(label)] += count

    # è¨ˆç®—æ¯”ä¾‹
    total = sum(overall_counts.values())
    print(f"ç¸½è‚¡ç¥¨æ•¸: {len(stocks)}")
    print(f"æœ‰æ¨™ç±¤è‚¡ç¥¨: {stocks_with_labels}")
    print(f"\næ•´é«”æ¨™ç±¤åˆ†å¸ƒ:")
    print(f"  Down (-1): {overall_counts[-1]:,} ({overall_counts[-1]/total:.2%})")
    print(f"  Neutral (0): {overall_counts[0]:,} ({overall_counts[0]/total:.2%})")
    print(f"  Up (1): {overall_counts[1]:,} ({overall_counts[1]/total:.2%})")

# ä½¿ç”¨ç¯„ä¾‹
analyze_label_distribution('data/preprocessed_v5_1hz/daily/20250901')
```

è¼¸å‡º:
```
ç¸½è‚¡ç¥¨æ•¸: 195
æœ‰æ¨™ç±¤è‚¡ç¥¨: 187

æ•´é«”æ¨™ç±¤åˆ†å¸ƒ:
  Down (-1): 58,341 (30.12%)
  Neutral (0): 77,485 (40.01%)
  Up (1): 57,882 (29.87%)
```

---

## å¸¸è¦‹å•é¡Œ

### Q1: ç‚ºä»€éº¼æœ‰äº›è‚¡ç¥¨æ²’æœ‰æ¨™ç±¤ï¼Ÿ

**A**: åªæœ‰**é€šééæ¿¾**çš„è‚¡ç¥¨æ‰æœƒè¨ˆç®—æ¨™ç±¤ã€‚æœªé€šééæ¿¾çš„åŸå› åŒ…æ‹¬ï¼š
- æ³¢å‹•ç‡å¤ªä½ï¼ˆ`range_pct < filter_threshold`ï¼‰
- æ•¸æ“šé»æ•¸å¤ªå°‘
- ç¼ºå¤±å€¼éå¤š

æª¢æŸ¥æ–¹å¼ï¼š
```python
metadata = stock_data['metadata']
if not metadata['pass_filter']:
    print(f"æœªé€šééæ¿¾: {metadata['filter_reason']}")
```

---

### Q2: labels é™£åˆ—é•·åº¦ç‚ºä½•èˆ‡ mids ä¸åŒï¼Ÿ

**A**: `labels` é™£åˆ—å¯èƒ½åŒ…å« `NaN`ï¼ˆæœªè¨ˆç®—çš„é‚Šç•Œé»ï¼‰ã€‚

è™•ç†æ–¹å¼ï¼š
```python
labels = stock_data['labels']
mids = stock_data['mids']

# åªä½¿ç”¨æœ‰æ•ˆæ¨™ç±¤
valid_mask = ~np.isnan(labels)
valid_labels = labels[valid_mask]
valid_mids = mids[valid_mask]

print(f"ç¸½æ•¸æ“šé»: {len(mids)}")
print(f"æœ‰æ•ˆæ¨™ç±¤: {len(valid_labels)}")
```

---

### Q3: å¦‚ä½•çŸ¥é“ NPZ æ˜¯æ–°ç‰ˆï¼ˆæœ‰æ¨™ç±¤ï¼‰é‚„æ˜¯èˆŠç‰ˆï¼Ÿ

**A**: æª¢æŸ¥ `labels` æ¬„ä½æ˜¯å¦å­˜åœ¨ï¼š

```python
data = np.load(npz_path, allow_pickle=True)
if 'labels' in data.keys():
    print("âœ… æ–°ç‰ˆ NPZï¼ˆv2.0ï¼Œå«æ¨™ç±¤ï¼‰")
else:
    print("âš ï¸ èˆŠç‰ˆ NPZï¼ˆv1.0ï¼Œåƒ…æ¨™ç±¤é è¦½ï¼‰")
```

---

### Q4: label_preview å’Œ labels æœ‰ä»€éº¼å·®åˆ¥ï¼Ÿ

| é …ç›® | label_preview | labels |
|------|--------------|--------|
| **é¡å‹** | metadata ä¸­çš„çµ±è¨ˆè³‡è¨Š | NPZ ä¸­çš„å¯¦éš›é™£åˆ— |
| **å…§å®¹** | æ¨™ç±¤è¨ˆæ•¸å’Œæ¯”ä¾‹ | é€é»æ¨™ç±¤å€¼ (-1, 0, 1, NaN) |
| **ç”¨é€”** | å¿«é€Ÿæª¢æŸ¥åˆ†å¸ƒ | è¨“ç·´ã€è¦–è¦ºåŒ– |
| **å¤§å°** | å›ºå®šï¼ˆå¹¾å bytesï¼‰ | èˆ‡æ•¸æ“šç­‰é•·ï¼ˆå¹¾ KB ~ MBï¼‰ |
| **ç‰ˆæœ¬** | v1.0+ | v2.0+ |

**ä½¿ç”¨å»ºè­°**:
- å¿«é€Ÿçµ±è¨ˆ â†’ ä½¿ç”¨ `label_preview`
- è©³ç´°åˆ†æ â†’ ä½¿ç”¨ `labels` é™£åˆ—

---

### Q5: å¦‚ä½•é¸æ“‡åˆé©çš„è‚¡ç¥¨ç”¨æ–¼è¨“ç·´ï¼Ÿ

**A**: ä½¿ç”¨ `analyze_label_distribution.py` è…³æœ¬ï¼š

```bash
python scripts/analyze_label_distribution.py \
    --preprocessed-dir data/preprocessed_v5_1hz \
    --mode recommend \
    --target-dist 0.30,0.40,0.30
```

æœƒæ ¹æ“šæ¨™ç±¤åˆ†å¸ƒæ¨è–¦åˆé©çš„è‚¡ç¥¨çµ„åˆã€‚

---

### Q6: é è™•ç†æ•¸æ“šå¯ä»¥ç›´æ¥ç”¨æ–¼è¨“ç·´å—ï¼Ÿ

**A**: âŒ **ä¸è¡Œï¼** é è™•ç†æ•¸æ“šé‚„éœ€è¦ï¼š

1. **æ™‚é–“åºåˆ—çª—å£åŒ–**ï¼ˆä¾‹å¦‚ï¼š100 timestepsï¼‰
2. **æ¨™æº–åŒ–** (Z-Score)
3. **æŒ‰è‚¡ç¥¨åŠƒåˆ†** train/val/test

é€™äº›æ­¥é©Ÿç”± `extract_tw_stock_data_v6.py` å®Œæˆã€‚

**å®Œæ•´æµç¨‹**:
```bash
# Step 1: é è™•ç†ï¼ˆæœ¬æ–‡æª”ï¼‰
python scripts/preprocess_single_day.py ...

# Step 2: ç”Ÿæˆè¨“ç·´æ•¸æ“š
python scripts/extract_tw_stock_data_v6.py \
    --preprocessed-dir data/preprocessed_v5_1hz \
    --output-dir data/processed_v6

# Step 3: è¨“ç·´
python scripts/train_deeplob_generic.py ...
```

---

### Q7: summary.json åŒ…å«ä»€éº¼è³‡è¨Šï¼Ÿ

**A**: ç•¶æ—¥è™•ç†æ‘˜è¦ï¼Œç¯„ä¾‹ï¼š

```json
{
  "date": "20250901",
  "total_symbols": 195,
  "symbols_passed": 187,
  "symbols_filtered": 8,
  "filter_threshold": 0.005,
  "filter_method": "range_pct",
  "overall_label_dist": {
    "down_pct": 0.3012,
    "neutral_pct": 0.4001,
    "up_pct": 0.2987
  },
  "stocks": ["0050", "2330", "2317", ...]
}
```

---

### Q8: å¦‚ä½•è¦–è¦ºåŒ–é è™•ç†æ•¸æ“šï¼Ÿ

**A**: ä½¿ç”¨ `label_viewer` å·¥å…·ï¼š

```bash
cd label_viewer
python app_preprocessed.py
```

åœ¨ç€è¦½å™¨æ‰“é–‹ http://localhost:8051ï¼Œå¯ä»¥ï¼š
- æŸ¥çœ‹ä¸­é–“åƒ¹æ™‚åºåœ–
- æŸ¥çœ‹æ¨™ç±¤åˆ†å¸ƒï¼ˆåœ“é¤…åœ–/æŸ±ç‹€åœ–ï¼‰
- æŸ¥çœ‹æ¨™ç±¤ç–ŠåŠ åœ¨åƒ¹æ ¼åœ–ä¸Š
- æŸ¥çœ‹å…ƒæ•¸æ“šè¡¨æ ¼

---

### Q9: æ•¸æ“šä½”ç”¨å¤šå°‘ç©ºé–“ï¼Ÿ

**A**: å–®æª”è‚¡ç¥¨å–®æ—¥æ•¸æ“šï¼š

- **ç„¡æ¨™ç±¤ç‰ˆ (v1.0)**: ~200-500 KB
- **æœ‰æ¨™ç±¤ç‰ˆ (v2.0)**: ~250-600 KB
- **å…¨éƒ¨ 195 æª”å–®æ—¥**: ~50-100 MB
- **å…¨å¹´æ•¸æ“šï¼ˆç´„ 250 äº¤æ˜“æ—¥ï¼‰**: ~12-25 GB

---

### Q10: å¦‚ä½•æ›´æ–°èˆŠç‰ˆ NPZ åˆ°æ–°ç‰ˆï¼ˆæ·»åŠ æ¨™ç±¤ï¼‰ï¼Ÿ

**A**: é‡æ–°é‹è¡Œé è™•ç†è…³æœ¬ï¼š

```bash
# å–®ä¸€å¤©
python scripts/preprocess_single_day.py \
    --input data/raw/FI2010/Ticker_20250901.txt \
    --output data/preprocessed_v5_1hz \
    --config configs/config_pro_v5_ml_optimal.yaml

# æ‰¹æ¬¡è™•ç†
scripts\batch_preprocess.bat
```

é è™•ç†è…³æœ¬æœƒè¦†è“‹èˆŠæ–‡ä»¶ä¸¦ç”ŸæˆåŒ…å«æ¨™ç±¤çš„æ–°ç‰ˆ NPZã€‚

---

## ç‰ˆæœ¬æ­·å²

### v2.0 (2025-10-23) ğŸ†•
- âœ… æ–°å¢ `labels` é™£åˆ—åˆ° NPZ
- âœ… ä¿®æ”¹ `compute_label_preview()` æ”¯æŒè¿”å›å®Œæ•´æ¨™ç±¤
- âœ… æ›´æ–° `save_preprocessed_npz()` ä¿å­˜æ¨™ç±¤
- âœ… Label Viewer æ”¯æŒæ¨™ç±¤è¦–è¦ºåŒ–

### v1.0 (2025-10-15)
- âœ… åŸºç¤é è™•ç†æµç¨‹
- âœ… å‹•æ…‹éæ¿¾é–¾å€¼æ±ºç­–
- âœ… label_preview çµ±è¨ˆ
- âœ… summary.json ç”Ÿæˆ

---

## ç›¸é—œæ–‡æª”

- **[LABEL_PREVIEW_GUIDE.md](LABEL_PREVIEW_GUIDE.md)** - æ¨™ç±¤é è¦½å®Œæ•´æŒ‡å—
- **[V6_TWO_STAGE_PIPELINE_GUIDE.md](V6_TWO_STAGE_PIPELINE_GUIDE.md)** - V6 é›™éšæ®µè™•ç†æµç¨‹
- **[CLAUDE.md](../CLAUDE.md)** - å°ˆæ¡ˆç¸½é«”èªªæ˜

---

## æŠ€è¡“æ”¯æ´

å¦‚æœ‰å•é¡Œï¼Œè«‹åƒè€ƒï¼š
1. æœ¬æ–‡æª”çš„[å¸¸è¦‹å•é¡Œ](#å¸¸è¦‹å•é¡Œ)ç« ç¯€
2. é‹è¡Œ `python scripts/preprocess_single_day.py --help`
3. æŸ¥çœ‹æ—¥èªŒæ–‡ä»¶ `logs/preprocess_*.log`

**æœ€å¾Œæ›´æ–°**: 2025-10-23
**æ–‡æª”ç‰ˆæœ¬**: v2.0
