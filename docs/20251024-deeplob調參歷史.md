# DeepLOB 調參歷史記錄

每次保留數據及成果,簡要說明(其他廢話不用),這是改下次調參參考用

## 格式:

**日期**:
**配置**:
**結果**:
**問題**:
**結論**:
---

## 實驗 1: V5 基線測試（2025-10-24）

**日期**: 2025-10-24
**配置**: `configs/train_v5.yaml`
- 數據: processed_v7 (Triple-Barrier 標籤)
- 模型: DeepLOB (32-32-32-32-32)
- Dropout: 0.7
- LR: 4e-6 (Cosine, warmup=0.40, min=3e-6)
- Batch Size: 384
- Grad Clip: 0.8
- Epochs: 30
- Weight Decay: 0.001

**結果**:
```
| Epoch | Train Loss | Val Loss | Train Acc | Val Acc | Val F1 (W) | Val F1 (U) | Grad Norm |
|-------|-----------|----------|-----------|---------|------------|------------|-----------|
| 7     | 0.9722    | 1.0364   | 53.01%    | 44.49%  | 0.4371     | 0.4261     | 3.79      |
| 10    | 0.8579    | 1.0831   | 61.05%    | 43.86%  | 0.4307     | 0.4275     | 10.61     |
```
- **最佳驗證準確率**: 44.49% @ Epoch 7
- **最佳驗證 F1 (Weighted)**: 0.4371 @ Epoch 7

**問題**:
1. **嚴重過擬合**: Epoch 7 後 Train-Val 差距從 8.5% 擴大到 17.2%
2. **梯度爆炸**: Grad Norm 從 1.86 飆升到 10.61（指數級增長）
3. **驗證性能下降**: Epoch 7 是頂點，之後驗證性能持續惡化
4. **Loss 發散**: Val Loss 從 1.0364 → 1.0831 (上升 4.5%)

**結論**:
- **實際使用趨勢標籤**（經檢查，標籤分布 30.86%/42.96%/26.19%）
- **過擬合根源**: 梯度裁剪不夠 + Dropout 不足 + 訓練太長 + 模型容量過大
- **44.49% 驗證準確率偏低**（三分類隨機 33%，僅好 11%）
- **下一步**: 實驗 2 激進抑制過擬合

---

## 實驗 2: 激進抑制過擬合（2025-10-24）⏳

**日期**: 2025-10-24
**配置**: `configs/train_v5.yaml` (已修改)
- 數據: processed_v7 (趨勢標籤，分布 30.86%/42.96%/26.19%)
- 模型: DeepLOB (32-32-32-**24**-**24**) ← 減小容量
- Dropout: **0.8** ← 增強正則化
- LR: **3e-6** (Cosine, warmup=0.40, min=2e-6) ← 降低學習率
- Batch Size: 384
- Grad Clip: **0.5** ← 更嚴格控制
- Epochs: **15** ← 縮短訓練期
- Weight Decay: **0.01** ← 10倍增強
- Early Stop: patience=3, min_delta=0.001 ← 更早停止

**核心策略**:
1. **減小模型容量**: LSTM/FC 32→24（降低記憶能力）
2. **增強正則化**: Dropout 0.7→0.8, Weight Decay 0.001→0.01
3. **降低學習速度**: LR 4e-6→3e-6, Grad Clip 0.8→0.5
4. **提前停止**: Epochs 30→15, Patience 5→3

**預期目標**:
- Train Acc: 50-55% (下降，減少過擬合)
- Val Acc: 47-50% (提升 2-5%)
- Train-Val Gap: 5-7% (縮小)
- Grad Norm: <5.0 (穩定)

**訓練指令**:
```bash
conda activate deeplob-pro
python scripts/train_deeplob_v5.py --config configs/train_v5.yaml
```

**結果**:
```
早停於 Epoch 5，最佳 Epoch 2
Epoch 5 性能:
- Train Acc: 43.28%, Loss: 1.0334, Grad: 2.07
- Val Acc: 43.78% (unweighted)
- Val F1 (W): 0.4180
- Val F1 (U): 0.4065
- Per-Class:
    Class 0: P=0.4542, R=0.7139
    Class 1: P=0.6603, R=0.0949  ← 召回率極低！
    Class 2: P=0.3890, R=0.6932
```

**問題**:
1. **嚴重欠擬合**: Train Acc 43.28% < Val Acc 43.78%（負過擬合！）
2. **正則化過強**: Dropout 0.8 + Weight Decay 0.01 導致模型學不動
3. **Class 1 崩潰**: 召回率僅 9.49%（模型幾乎不預測持平類）
4. **學習太慢**: LR 3e-6 太小，Epoch 2 即達頂點後停滯
5. **Early Stop 太嚴**: Patience=3 + min_delta=0.001 在慢學習下過早終止

**結論**:
- **矯枉過正**：從過擬合（實驗 1）跳到欠擬合（實驗 2）
- **需要中庸之道**：在實驗 1 和實驗 2 之間找平衡
- **關鍵洞察**: Train-Val Gap 為負 → 正則化太強，非過擬合
- **下一步**: 實驗 3 採用折衷參數

---

## 實驗 3: 中庸之道（2025-10-24）⏳

**日期**: 2025-10-24
**配置**: `configs/train_v5.yaml` (再次修改)
- 數據: processed_v7 (趨勢標籤)
- 模型: DeepLOB (32-32-32-**28**-**28**) ← 折衷 (24↑28↓32)
- Dropout: **0.7** ← 降回實驗 1 (0.8→0.7)
- LR: **5e-6** ← 加快學習 (3e-6→5e-6)
- Batch Size: 384
- Grad Clip: **0.6** ← 折衷 (0.5→0.6)
- Epochs: **20** ← 延長 (15→20)
- Weight Decay: **0.005** ← 減半 (0.01→0.005)
- Early Stop: patience=**5**, min_delta=**0.0005** ← 放寬

**核心策略**:
1. **在實驗 1 和 2 之間找平衡**
2. **降低正則化**: Dropout 0.8→0.7, WD 0.01→0.005
3. **加快學習**: LR 3e-6→5e-6（介於實驗 1 的 4e-6）
4. **模型容量折衷**: 28 (介於 24-32)
5. **更寬容早停**: Patience 3→5, min_delta 降低 50%

**預期目標**:
- Train Acc: 48-52%
- Val Acc: 45-48%
- Train-Val Gap: 3-5% (正值但不過大)
- Grad Norm: <6.0
- Class 1 Recall: >30% (改善 Class 1 學習)

**結果**: (待填寫)

---

## 實驗 4: Warmup + Cosine Decay 穩定訓練（2025-10-24）⏳

**日期**: 2025-10-24
**配置**: `configs/train_v5_experiment4.yaml` (新建)
- 數據: processed_v7 (趨勢標籤)
- 模型: DeepLOB (32-32-32-**32**-**32**) ← 回到標準容量
- Dropout: **0.75** ← 輕度提升 (0.7→0.75)
- **LR 策略**: **Warmup + Cosine Decay** ← 核心改動
  - Epoch 0-2: **1e-6 → 3e-6** (Warmup)
  - Epoch 3-4: **3e-6** (峰值)
  - Epoch 5-12: **3e-6 → 8e-7** (Cosine Decay)
- Batch Size: 384
- Grad Clip: **2.0** ← 嚴格控制 (0.6→2.0)
- Epochs: **12** ← 縮短 (20→12)
- Weight Decay: **0.0001** ← 大幅降低 (0.005→0.0001)
- Early Stop: patience=3, min_delta=0.0003
- **Class Weights**: **"auto"** ← 啟用 (處理不平衡)
- **Label Smoothing**: **0.02** ← 新增 (抑制過度自信)
- Balance Sampler: **false** ← 關閉 (避免過擬合少數類)

**核心策略** (基於 ChatGPT 方案 A + 實驗 1-3 經驗):
1. **Warmup + Cosine Decay**: 避免後期學習率過高導致發散
2. **嚴格梯度控制**: Grad Clip 2.0（對抗梯度爆炸）
3. **輕正則化**: Dropout +0.05, WD 大幅降低（配合 Dropout）
4. **溫和類別平衡**: class_weights="auto"（不用 balance_sampler）
5. **標籤平滑**: 0.02（抑制過度自信）
6. **縮短訓練期**: 12 epochs（避免後期過擬合）
7. **回到標準容量**: 32（實驗 3 的 28 欠擬合）

**預期目標**:
- Train Acc: 50-55%
- Val Acc: 47-50% (提升 3-6%)
- Train-Val Gap: 3-5% (大幅縮小)
- Grad Norm: <5.0 (穩定，不爆炸)
- Val Loss: 持續下降或穩定（不發散）
- Best Epoch: 8-10（後期仍保持性能）

**訓練指令**:
```bash
conda activate deeplob-pro
python scripts/train_deeplob_v5.py --config configs/train_v5_experiment4.yaml
```

**監控重點**:
1. Epoch 6-8 梯度範數（應 <5.0）
2. Val Loss 曲線（應不發散）
3. Train-Val Gap（應 <5%）
4. Class 1 召回率（應 >40%）

**結果**: (待填寫)

**備註**: 此配置基於 ChatGPT 理論建議，可能過於保守。

---

## 實驗 4b: 務實版（基於實際數據）⭐⭐⭐ 推薦

**日期**: 2025-10-24
**配置**: `configs/train_v5_experiment4_practical.yaml` (新建)
- 數據: processed_v7 (趨勢標籤)
- 模型: DeepLOB (32-32-32-32-32) ← 標準容量
- Dropout: **0.75** ← 適度提升
- **LR**: **2.5e-6** ← 核心！降低 50%（5e-6 太高）
- Warmup: **16.7%**（2/10 epochs）
- eta_min: **1e-6** ← 不要降太低
- Batch Size: 384
- Grad Clip: **1.5** ← 嚴格但不過度
- Epochs: **10** ← Epoch 5 後沒用
- Weight Decay: **0.001** ← 適度
- Early Stop: patience=**1**, min_delta=0.0005 ← 激進早停
- Class Weights: **none** ← 實驗 1-3 沒用
- Sample Weights: **false** ← 實驗 1-3 沒用
- Label Smoothing: **0.02** ← 輕度

**核心策略**（基於實際訓練數據，不是理論）:
1. **降低學習率峰值**（最關鍵）
   - 觀察：Epoch 6-10 的 LR 在 4e-6~5e-6，梯度爆炸
   - 策略：2.5e-6（降低 50%），避免後期過高

2. **嚴格梯度裁剪**
   - 觀察：梯度從 3.10 飆到 17.34（5.6倍）
   - 策略：1.5（容忍小波動，嚴控大爆炸）

3. **更早停止訓練**
   - 觀察：Epoch 5 後 Val Loss 只上升（1.0411 → 1.1604）
   - 策略：10 epochs, Patience=1（提早停止）

**預期目標**:
- Best Epoch: 5-7
- Val Acc: 45-48% (提升 2-5%)
- Grad Norm: <5.0 (穩定)
- Val Loss: 持續下降或穩定
- Train-Val Gap: 5-8% (合理)

**訓練指令**:
```bash
conda activate deeplob-pro
python scripts/train_deeplob_v5.py --config configs/train_v5_experiment4_practical.yaml
```

**監控重點**:
1. Epoch 5-7 梯度範數（應 <5.0）
2. Val Loss 在 Epoch 5 後是否持續下降
3. 是否在 Epoch 6-8 提早停止（正常）

**結果**: (待填寫)

---

## 實驗 5:


