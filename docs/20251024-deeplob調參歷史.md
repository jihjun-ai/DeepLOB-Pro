# DeepLOB 調參歷史記錄

每次保留數據及成果,簡要說明(其他廢話不用),這是改下次調參參考用

## 格式:

**日期**:
**配置**:
**結果**:
**問題**:
**結論**:
---

## 實驗 1: V5 基線測試（2025-10-24）

**日期**: 2025-10-24
**配置**: `configs/train_v5.yaml`
- 數據: processed_v7 (Triple-Barrier 標籤)
- 模型: DeepLOB (32-32-32-32-32)
- Dropout: 0.7
- LR: 4e-6 (Cosine, warmup=0.40, min=3e-6)
- Batch Size: 384
- Grad Clip: 0.8
- Epochs: 30
- Weight Decay: 0.001

**結果**:
```
| Epoch | Train Loss | Val Loss | Train Acc | Val Acc | Val F1 (W) | Val F1 (U) | Grad Norm |
|-------|-----------|----------|-----------|---------|------------|------------|-----------|
| 7     | 0.9722    | 1.0364   | 53.01%    | 44.49%  | 0.4371     | 0.4261     | 3.79      |
| 10    | 0.8579    | 1.0831   | 61.05%    | 43.86%  | 0.4307     | 0.4275     | 10.61     |
```
- **最佳驗證準確率**: 44.49% @ Epoch 7
- **最佳驗證 F1 (Weighted)**: 0.4371 @ Epoch 7

**問題**:
1. **嚴重過擬合**: Epoch 7 後 Train-Val 差距從 8.5% 擴大到 17.2%
2. **梯度爆炸**: Grad Norm 從 1.86 飆升到 10.61（指數級增長）
3. **驗證性能下降**: Epoch 7 是頂點，之後驗證性能持續惡化
4. **Loss 發散**: Val Loss 從 1.0364 → 1.0831 (上升 4.5%)

**結論**:
- **實際使用趨勢標籤**（經檢查，標籤分布 30.86%/42.96%/26.19%）
- **過擬合根源**: 梯度裁剪不夠 + Dropout 不足 + 訓練太長 + 模型容量過大
- **44.49% 驗證準確率偏低**（三分類隨機 33%，僅好 11%）
- **下一步**: 實驗 2 激進抑制過擬合

---

## 實驗 2: 激進抑制過擬合（2025-10-24）⏳

**日期**: 2025-10-24
**配置**: `configs/train_v5.yaml` (已修改)
- 數據: processed_v7 (趨勢標籤，分布 30.86%/42.96%/26.19%)
- 模型: DeepLOB (32-32-32-**24**-**24**) ← 減小容量
- Dropout: **0.8** ← 增強正則化
- LR: **3e-6** (Cosine, warmup=0.40, min=2e-6) ← 降低學習率
- Batch Size: 384
- Grad Clip: **0.5** ← 更嚴格控制
- Epochs: **15** ← 縮短訓練期
- Weight Decay: **0.01** ← 10倍增強
- Early Stop: patience=3, min_delta=0.001 ← 更早停止

**核心策略**:
1. **減小模型容量**: LSTM/FC 32→24（降低記憶能力）
2. **增強正則化**: Dropout 0.7→0.8, Weight Decay 0.001→0.01
3. **降低學習速度**: LR 4e-6→3e-6, Grad Clip 0.8→0.5
4. **提前停止**: Epochs 30→15, Patience 5→3

**預期目標**:
- Train Acc: 50-55% (下降，減少過擬合)
- Val Acc: 47-50% (提升 2-5%)
- Train-Val Gap: 5-7% (縮小)
- Grad Norm: <5.0 (穩定)

**訓練指令**:
```bash
conda activate deeplob-pro
python scripts/train_deeplob_v5.py --config configs/train_v5.yaml
```

**結果**:
```
早停於 Epoch 5，最佳 Epoch 2
Epoch 5 性能:
- Train Acc: 43.28%, Loss: 1.0334, Grad: 2.07
- Val Acc: 43.78% (unweighted)
- Val F1 (W): 0.4180
- Val F1 (U): 0.4065
- Per-Class:
    Class 0: P=0.4542, R=0.7139
    Class 1: P=0.6603, R=0.0949  ← 召回率極低！
    Class 2: P=0.3890, R=0.6932
```

**問題**:
1. **嚴重欠擬合**: Train Acc 43.28% < Val Acc 43.78%（負過擬合！）
2. **正則化過強**: Dropout 0.8 + Weight Decay 0.01 導致模型學不動
3. **Class 1 崩潰**: 召回率僅 9.49%（模型幾乎不預測持平類）
4. **學習太慢**: LR 3e-6 太小，Epoch 2 即達頂點後停滯
5. **Early Stop 太嚴**: Patience=3 + min_delta=0.001 在慢學習下過早終止

**結論**:
- **矯枉過正**：從過擬合（實驗 1）跳到欠擬合（實驗 2）
- **需要中庸之道**：在實驗 1 和實驗 2 之間找平衡
- **關鍵洞察**: Train-Val Gap 為負 → 正則化太強，非過擬合
- **下一步**: 實驗 3 採用折衷參數

---

## 實驗 3: 中庸之道（2025-10-24）⏳

**日期**: 2025-10-24
**配置**: `configs/train_v5.yaml` (再次修改)
- 數據: processed_v7 (趨勢標籤)
- 模型: DeepLOB (32-32-32-**28**-**28**) ← 折衷 (24↑28↓32)
- Dropout: **0.7** ← 降回實驗 1 (0.8→0.7)
- LR: **5e-6** ← 加快學習 (3e-6→5e-6)
- Batch Size: 384
- Grad Clip: **0.6** ← 折衷 (0.5→0.6)
- Epochs: **20** ← 延長 (15→20)
- Weight Decay: **0.005** ← 減半 (0.01→0.005)
- Early Stop: patience=**5**, min_delta=**0.0005** ← 放寬

**核心策略**:
1. **在實驗 1 和 2 之間找平衡**
2. **降低正則化**: Dropout 0.8→0.7, WD 0.01→0.005
3. **加快學習**: LR 3e-6→5e-6（介於實驗 1 的 4e-6）
4. **模型容量折衷**: 28 (介於 24-32)
5. **更寬容早停**: Patience 3→5, min_delta 降低 50%

**預期目標**:
- Train Acc: 48-52%
- Val Acc: 45-48%
- Train-Val Gap: 3-5% (正值但不過大)
- Grad Norm: <6.0
- Class 1 Recall: >30% (改善 Class 1 學習)

**結果**: (待填寫)

---

## 實驗 4:


