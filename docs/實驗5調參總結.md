# 實驗 5：突破 60% 準確率調參總結

**日期**: 2025-10-24
**目標**: Test Acc 從 51.14% → >60%
**配置**: `configs/train_v5.yaml`

---

## 📊 問題診斷（實驗 4b 結果）

| 問題 | 數據 | 分析 |
|------|------|------|
| **測試準確率偏低** | 51.14% | 距離 60% 目標差 8.86% |
| **Class 0 召回率低** | 40.72% | 40% 的下跌樣本被漏掉 |
| **Class 2 召回率低** | 36.13% | 64% 的上漲樣本被漏掉 |
| **模型過於保守** | 偏好預測 Class 1 | 51-52% 誤判為持平 |
| **可能容量不足** | 小模型 (32-32-32) | 接近上限 |

---

## 🎯 14 個核心改動

### 1️⃣ 增強模型容量（最關鍵，+150% 參數量）

```yaml
# 舊配置（實驗 4b）
conv1_filters: 32
conv2_filters: 32
conv3_filters: 32
lstm_hidden_size: 32
fc_hidden_size: 32

# 新配置（實驗 5）
conv1_filters: 48      # +50%
conv2_filters: 48      # +50%
conv3_filters: 48      # +50%
lstm_hidden_size: 64   # +100%
fc_hidden_size: 64     # +100%
```

**理由**：51% 可能接近模型容量上限，需要更大網絡學習複雜模式。

**參數量對比**：
- 實驗 4b: ~250K 參數
- 實驗 5: ~600K 參數 (+140%)

---

### 2️⃣ 延長訓練時間

```yaml
# 舊配置
epochs: 10
patience: 1

# 新配置
epochs: 20        # +100%
patience: 3       # +200%
```

**理由**：容量增大後，需要更多時間收斂。實驗 4b 只訓練 3 個 epoch 就停止，可能過早。

---

### 3️⃣ 減弱正則化（避免欠擬合）

```yaml
# 舊配置
dropout: 0.75
weight_decay: 0.001
label_smoothing: 0.02

# 新配置
dropout: 0.65         # -13%
weight_decay: 0.0005  # -50%
label_smoothing: 0.01 # -50%
```

**理由**：容量增大後，過強正則化會欠擬合。小模型需要強正則化，大模型需要弱正則化。

---

### 4️⃣ 改善 Class 0/2 學習

```yaml
# 舊配置
use_sample_weights: false
class_weights: "none"

# 新配置
use_sample_weights: true   # ✅ 啟用
class_weights: "auto"      # ✅ 自動計算
```

**理由**：針對性改善召回率低的類別（40.72% / 36.13%）。

**Class Weights 預計**（基於標籤分布 30.86%/42.96%/26.19%）：
- Class 0 (下跌): ~1.1x
- Class 1 (持平): ~0.8x
- Class 2 (上漲): ~1.2x

---

### 5️⃣ 優化學習率策略

```yaml
# 舊配置
lr: 0.0000025          # 2.5e-6
warmup_ratio: 0.167    # 16.7%
eta_min: 0.000001      # 1e-6

# 新配置
lr: 0.000003           # 3e-6 (+20%)
warmup_ratio: 0.25     # 25% (+50%)
eta_min: 0.0000005     # 5e-7 (-50%)
```

**理由**：
- 容量增大需要更高學習率（更多參數需要更新）
- 更長 warmup 提升穩定性（5 epochs vs 2 epochs）
- 更低 eta_min 延長衰減期

**學習率曲線對比**：
```
實驗 4b:
  Epoch 0-2:  1e-6 → 2.5e-6 (Warmup)
  Epoch 2-10: 2.5e-6 → 1e-6 (Cosine Decay)

實驗 5:
  Epoch 0-5:  1e-6 → 3e-6 (Warmup，更長)
  Epoch 5-20: 3e-6 → 5e-7 (Cosine Decay，更長)
```

---

### 6️⃣ 提升訓練穩定性

```yaml
# 舊配置
batch_size: 384
grad_clip: 1.5

# 新配置
batch_size: 512    # +33%
grad_clip: 2.0     # +33%
```

**理由**：
- 大模型需要更大 batch（減少梯度噪音）
- 更寬容梯度裁剪（大模型梯度本身較大）

---

## 📋 完整配置對比表

| 參數 | 實驗 4b | 實驗 5 | 變化 | 理由 |
|------|---------|--------|------|------|
| **Conv filters** | 32 | **48** | +50% | 提升特徵提取能力 |
| **LSTM hidden** | 32 | **64** | +100% | 提升時序建模能力 |
| **FC hidden** | 32 | **64** | +100% | 提升決策能力 |
| **Dropout** | 0.75 | **0.65** | -13% | 避免欠擬合 |
| **LR** | 2.5e-6 | **3e-6** | +20% | 匹配容量增加 |
| **Weight Decay** | 0.001 | **0.0005** | -50% | 減弱正則化 |
| **Batch Size** | 384 | **512** | +33% | 提升穩定性 |
| **Epochs** | 10 | **20** | +100% | 更多收斂時間 |
| **Patience** | 1 | **3** | +200% | 避免過早停止 |
| **Sample Weights** | false | **true** | ✅ | 改善 Class 0/2 |
| **Class Weights** | none | **auto** | ✅ | 改善 Class 0/2 |
| **Label Smoothing** | 0.02 | **0.01** | -50% | 減弱邊界模糊 |
| **Warmup Ratio** | 16.7% | **25%** | +50% | 更長穩定期 |
| **Grad Clip** | 1.5 | **2.0** | +33% | 匹配容量增加 |

---

## 🎯 預期目標

| 指標 | 實驗 4b | 實驗 5 目標 | 改善 |
|------|---------|-------------|------|
| **Val Acc** | 50.09% | **55-60%** | +5-10% |
| **Test Acc** | 51.14% | **>60%** | +9%+ |
| **Class 0 Recall** | 40.72% | **>50%** | +10%+ |
| **Class 2 Recall** | 36.13% | **>50%** | +14%+ |
| **Train-Val Gap** | 2.57% | **<8%** | 可接受 |
| **Grad Norm** | 2.63 | **<5.0** | 穩定 |
| **Best Epoch** | 2 | **8-15** | 後期達最佳 |
| **訓練時間** | 2 分鐘 | **10-15 分鐘** | +5-7x |

---

## ⚠️ 風險控制

### 1. 過擬合風險（容量翻倍）

**監控指標**：Train-Val Gap
- ✅ 健康：<5%
- ⚠️ 警告：5-10%
- 🔥 危險：>10%

**應對策略**：
- 如果 Train-Val Gap >10%：提高 Dropout（0.65 → 0.70）
- 如果 Val Acc 停滯：降低 Epochs（20 → 15）

### 2. 梯度爆炸風險（LR 提高）

**監控指標**：Grad Norm
- ✅ 健康：<3.0
- ⚠️ 警告：3.0-5.0
- 🔥 危險：>5.0

**應對策略**：
- 如果 Grad Norm >5.0：降低 LR（3e-6 → 2.5e-6）
- 如果梯度爆炸：提高 Grad Clip（2.0 → 1.5）

### 3. 訓練時間過長

**預計**：10-15 分鐘（RTX 5090）

**應對策略**：
- 如果 >20 分鐘：降低 Batch Size（512 → 384）
- 如果顯存不足：啟用 AMP（混合精度）

---

## 📈 訓練監控檢查表

### Epoch 1-5 (Warmup 階段)

- [ ] Grad Norm <3.0（穩定）
- [ ] Train Loss 快速下降（學習正常）
- [ ] Val Acc 逐步提升（泛化良好）
- [ ] LR 從 1e-6 → 3e-6（Warmup 正常）

### Epoch 6-10 (早期訓練)

- [ ] Train Acc 50-55%（不要太高）
- [ ] Val Acc 52-57%（超越實驗 4b）
- [ ] Train-Val Gap <5%（未過擬合）
- [ ] Class 0/2 Recall 逐步提升

### Epoch 11-15 (中期訓練)

- [ ] Val Acc 持續提升或穩定（找到最佳點）
- [ ] Train-Val Gap <8%（可接受範圍）
- [ ] Grad Norm 仍 <5.0（穩定）
- [ ] 準備觸發早停（Patience=3）

### Epoch 16-20 (後期訓練)

- [ ] 早停觸發或接近（最佳點已找到）
- [ ] Test Acc >60%（目標達成）
- [ ] Class 0/2 Recall >50%（平衡改善）

---

## 🚀 訓練指令

```bash
# 激活環境
conda activate deeplob-pro

# 訓練（預計 10-15 分鐘）
python scripts/train_deeplob_v5.py \
  --config configs/train_v5.yaml \
  --data-dir data/processed_v7/npz

# 監控（另一個終端）
tensorboard --logdir logs/deeplob_v5_exp5/

# 即時查看日誌
tail -f logs/deeplob_v5_exp5/*/train.log
```

---

## 🔄 備用策略（如果失敗）

### Plan A（當前策略）
✅ 容量翻倍 + 延長訓練 + 改善 Class 0/2

### Plan B（如果過擬合）
- 保持 Conv=32（不增大）
- 僅增大 LSTM/FC（64 → 96）
- 提高 Dropout（0.65 → 0.75）

### Plan C（如果梯度爆炸）
- 降低 LR（3e-6 → 2.5e-6）
- 更嚴格 Grad Clip（2.0 → 1.5）
- 減小 Batch Size（512 → 384）

### Plan D（如果收斂太慢）
- 提高 LR（3e-6 → 4e-6）
- 縮短 Warmup（25% → 20%）
- 降低 Weight Decay（0.0005 → 0.0001）

### Plan E（數據增強，需要新腳本）
- 時間軸抖動（±5 timesteps）
- 特徵軸噪音（Gaussian noise 0.01）
- Mixup（alpha=0.2）

### Plan F（改變數據，需要重新生成）
- 更長時間窗口（100 → 150 timesteps）
- 更多特徵（20 → 30 features）
- 不同標籤方法（趨勢 → Triple-Barrier）

---

## 📝 訓練後檢查清單

訓練完成後，檢查以下內容：

### 1. 基本性能
- [ ] Test Acc >60%（目標達成）
- [ ] Val Acc >55%（驗證集性能）
- [ ] Train-Val-Test 一致性（泛化良好）

### 2. Per-Class 性能
- [ ] Class 0 Recall >50%（改善 +10%）
- [ ] Class 1 Recall 60-70%（保持優勢）
- [ ] Class 2 Recall >50%（改善 +14%）
- [ ] 三類 F1 Score 平衡（差距 <10%）

### 3. 訓練穩定性
- [ ] 無梯度爆炸（Grad Norm <5.0）
- [ ] 無過擬合（Train-Val Gap <8%）
- [ ] 早停合理（Best Epoch 8-15）

### 4. 檔案完整性
- [ ] 最佳模型：`checkpoints/v5/deeplob_v5_best.pth`
- [ ] 測試指標：`checkpoints/v5/test_metrics.json`
- [ ] 混淆矩陣：`checkpoints/v5/confusion_matrix_test.png`
- [ ] 訓練日誌：`logs/deeplob_v5_exp5/*/train.log`

---

## 💡 經驗總結（待補充）

**成功經驗**：
- (待訓練後填寫)

**失敗教訓**：
- (待訓練後填寫)

**關鍵洞察**：
- (待訓練後填寫)

---

**最後更新**: 2025-10-24
**狀態**: ⏳ 待訓練
**預計完成**: 2025-10-24 18:00
