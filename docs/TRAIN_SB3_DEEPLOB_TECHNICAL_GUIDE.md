# train_sb3_deeplob.py æŠ€è¡“æŒ‡å—

## æ–‡æª”æ¦‚è¿°

æœ¬æ–‡æª”è©³ç´°èªªæ˜ `train_sb3_deeplob.py` çš„è¨“ç·´æµç¨‹ã€æŠ€è¡“æ¶æ§‹ã€è‡ªå®šç¾©å‡½æ•¸åŠæ¨¡å‹å­¸ç¿’æ©Ÿåˆ¶ã€‚

**æ–‡æª”ç‰ˆæœ¬**: v1.0
**æ›´æ–°æ—¥æœŸ**: 2025-10-26
**é©ç”¨ç‰ˆæœ¬**: SB3-DeepLOB v4.0

---

## ç›®éŒ„

1. [å°ˆæ¡ˆèƒŒæ™¯èˆ‡ç›®æ¨™](#1-å°ˆæ¡ˆèƒŒæ™¯èˆ‡ç›®æ¨™)
2. [æ•´é«”æ¶æ§‹](#2-æ•´é«”æ¶æ§‹)
3. [è¨“ç·´æµç¨‹è©³è§£](#3-è¨“ç·´æµç¨‹è©³è§£)
4. [æ ¸å¿ƒæ¨¡çµ„æŠ€è¡“èªªæ˜](#4-æ ¸å¿ƒæ¨¡çµ„æŠ€è¡“èªªæ˜)
5. [è‡ªå®šç¾©å‡½æ•¸èªªæ˜](#5-è‡ªå®šç¾©å‡½æ•¸èªªæ˜)
6. [æ¨¡å‹å­¸ç¿’æ©Ÿåˆ¶](#6-æ¨¡å‹å­¸ç¿’æ©Ÿåˆ¶)
7. [é…ç½®åƒæ•¸è©³è§£](#7-é…ç½®åƒæ•¸è©³è§£)
8. [ä½¿ç”¨ç¯„ä¾‹](#8-ä½¿ç”¨ç¯„ä¾‹)
9. [å¸¸è¦‹å•é¡Œèˆ‡èª¿è©¦](#9-å¸¸è¦‹å•é¡Œèˆ‡èª¿è©¦)

---

## 1. å°ˆæ¡ˆèƒŒæ™¯èˆ‡ç›®æ¨™

### 1.1 å°ˆæ¡ˆç›®æ¨™

å¯¦ç¾åŸºæ–¼å°è‚¡ LOB (Limit Order Book) æ•¸æ“šçš„é«˜é »äº¤æ˜“ç³»çµ±ï¼Œä½¿ç”¨é›™å±¤å­¸ç¿’æ¶æ§‹ï¼š

- **ç¬¬ä¸€å±¤ (DeepLOB)**: CNN-LSTM æ¨¡å‹å­¸ç¿’åƒ¹æ ¼è®Šå‹•é æ¸¬
- **ç¬¬äºŒå±¤ (PPO)**: å¼·åŒ–å­¸ç¿’ç®—æ³•å­¸ç¿’æœ€å„ªäº¤æ˜“ç­–ç•¥

### 1.2 æŠ€è¡“é¸å‹

| çµ„ä»¶ | æŠ€è¡“é¸æ“‡ | åŸå›  |
|------|---------|------|
| æ·±åº¦å­¸ç¿’æ¡†æ¶ | PyTorch 2.5 | éˆæ´»æ€§ã€GPU æ”¯æŒ |
| å¼·åŒ–å­¸ç¿’æ¡†æ¶ | Stable-Baselines3 | ç°¡å–®ç©©å®šã€LSTM æ”¯æŒå®Œå–„ |
| ç’°å¢ƒæ¨™æº– | Gymnasium | OpenAI Gym å®˜æ–¹å‡ç´šç‰ˆ |
| ç¡¬é«” | NVIDIA RTX 5090 (32GB) | å¤§æ‰¹é‡è¨“ç·´ã€æ··åˆç²¾åº¦ |

### 1.3 æ€§èƒ½æŒ‡æ¨™

- DeepLOB æº–ç¢ºç‡: **72.98%** âœ… (å·²é”æˆ)
- ç›®æ¨™ Sharpe Ratio: **> 2.0** (å¾…é©—è­‰)
- GPU åˆ©ç”¨ç‡: **> 85%** (å¾…é©—è­‰)

---

## 2. æ•´é«”æ¶æ§‹

### 2.1 ç³»çµ±æ¶æ§‹åœ–

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     train_sb3_deeplob.py                        â”‚
â”‚                     ä¸»è¨“ç·´è…³æœ¬ (Orchestrator)                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â”‚ åˆå§‹åŒ–ä¸¦å”èª¿
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        ä¸‰å¤§æ ¸å¿ƒæ¨¡çµ„                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚  â”‚  1. DeepLOBExtractor (ç‰¹å¾µæå–å™¨)                 â”‚         â”‚
â”‚  â”‚  [deeplob_feature_extractor.py]                   â”‚         â”‚
â”‚  â”‚                                                    â”‚         â”‚
â”‚  â”‚  - è¼‰å…¥é è¨“ç·´ DeepLOB æ¨¡å‹                        â”‚         â”‚
â”‚  â”‚  - å‡çµæ¬Šé‡ (ä¸åƒèˆ‡ PPO è¨“ç·´)                     â”‚         â”‚
â”‚  â”‚  - æå–æ·±å±¤ç‰¹å¾µ (LSTM hidden / é æ¸¬æ¦‚ç‡)          â”‚         â”‚
â”‚  â”‚  - ç‰¹å¾µèåˆ MLP (28ç¶­ â†’ features_dim)            â”‚         â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â”‚                              â”‚                                  â”‚
â”‚                              â”‚ è¼¸å‡ºç‰¹å¾µ                         â”‚
â”‚                              â–¼                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚  â”‚  2. TaiwanLOBTradingEnv (äº¤æ˜“ç’°å¢ƒ)                â”‚         â”‚
â”‚  â”‚  [tw_lob_trading_env.py]                          â”‚         â”‚
â”‚  â”‚                                                    â”‚         â”‚
â”‚  â”‚  è§€æ¸¬ç©ºé–“: 28 ç¶­                                   â”‚         â”‚
â”‚  â”‚    - LOB åŸå§‹ç‰¹å¾µ (20ç¶­): 5æª”è²·è³£åƒ¹é‡             â”‚         â”‚
â”‚  â”‚    - DeepLOB é æ¸¬ (3ç¶­): ä¸‹è·Œ/æŒå¹³/ä¸Šæ¼²æ¦‚ç‡       â”‚         â”‚
â”‚  â”‚    - äº¤æ˜“ç‹€æ…‹ (5ç¶­): æŒå€‰/åº«å­˜/æˆæœ¬/æ™‚é–“/å‹•ä½œ     â”‚         â”‚
â”‚  â”‚                                                    â”‚         â”‚
â”‚  â”‚  å‹•ä½œç©ºé–“: Discrete(3)                             â”‚         â”‚
â”‚  â”‚    - 0: Hold (æŒæœ‰)                                â”‚         â”‚
â”‚  â”‚    - 1: Buy (è²·å…¥)                                 â”‚         â”‚
â”‚  â”‚    - 2: Sell (è³£å‡º)                                â”‚         â”‚
â”‚  â”‚                                                    â”‚         â”‚
â”‚  â”‚  çå‹µå‡½æ•¸: å¤šçµ„ä»¶è¨­è¨ˆ                              â”‚         â”‚
â”‚  â”‚    - PnL çå‹µ (åƒ¹æ ¼è®Šå‹• Ã— æŒå€‰)                    â”‚         â”‚
â”‚  â”‚    - äº¤æ˜“æˆæœ¬æ‡²ç½° (-0.1% æ‰‹çºŒè²»)                   â”‚         â”‚
â”‚  â”‚    - åº«å­˜æ‡²ç½° (é¿å…é•·æœŸæŒå€‰)                       â”‚         â”‚
â”‚  â”‚    - é¢¨éšªèª¿æ•´é … (æ³¢å‹•ç‡æ‡²ç½°)                       â”‚         â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â”‚                              â”‚                                  â”‚
â”‚                              â”‚ ç‹€æ…‹ & çå‹µ                      â”‚
â”‚                              â–¼                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚  â”‚  3. PPO (å¼·åŒ–å­¸ç¿’ç®—æ³•)                             â”‚         â”‚
â”‚  â”‚  [Stable-Baselines3]                              â”‚         â”‚
â”‚  â”‚                                                    â”‚         â”‚
â”‚  â”‚  Policy: MlpPolicy + DeepLOBExtractor              â”‚         â”‚
â”‚  â”‚    - Actor (Ï€): [256, 128] â†’ å‹•ä½œæ¦‚ç‡              â”‚         â”‚
â”‚  â”‚    - Critic (V): [256, 128] â†’ ç‹€æ…‹åƒ¹å€¼             â”‚         â”‚
â”‚  â”‚                                                    â”‚         â”‚
â”‚  â”‚  æ ¸å¿ƒè¶…åƒæ•¸:                                        â”‚         â”‚
â”‚  â”‚    - Learning Rate: 3e-4                           â”‚         â”‚
â”‚  â”‚    - Gamma (æŠ˜æ‰£å› å­): 0.99                        â”‚         â”‚
â”‚  â”‚    - N Steps (rollout): 2048                       â”‚         â”‚
â”‚  â”‚    - Batch Size: 64                                â”‚         â”‚
â”‚  â”‚    - Clip Range: 0.2                               â”‚         â”‚
â”‚  â”‚    - Entropy Coef: 0.01                            â”‚         â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â”‚ è¨“ç·´å¾ªç’°
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                       è¨“ç·´å›èª¿ (Callbacks)                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  - CheckpointCallback: æ¯ 50K steps ä¿å­˜æ¨¡å‹                     â”‚
â”‚  - EvalCallback: æ¯ 10K steps åœ¨é©—è­‰é›†è©•ä¼°                       â”‚
â”‚  - è‡ªå‹•ä¿å­˜æœ€ä½³æ¨¡å‹ (based on mean reward)                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2.2 æ•¸æ“šæµå‘

```
å°è‚¡æ•¸æ“š (NPZ)
     â”‚
     â”œâ”€ LOB æ™‚åºæ•¸æ“š (100 timesteps Ã— 20 features)
     â”œâ”€ æ¨™ç±¤ (0/1/2: ä¸‹è·Œ/æŒå¹³/ä¸Šæ¼²)
     â””â”€ åƒ¹æ ¼æ•¸æ“š (ä¸­åƒ¹)
     â”‚
     â–¼
TaiwanLOBTradingEnv
     â”‚
     â”œâ”€ DeepLOB é æ¸¬ (3ç¶­æ¦‚ç‡)
     â”œâ”€ LOB ç•¶å‰ç‰¹å¾µ (20ç¶­)
     â””â”€ äº¤æ˜“ç‹€æ…‹ (5ç¶­)
     â”‚
     â–¼ è§€æ¸¬ (28ç¶­)
DeepLOBExtractor
     â”‚
     â”œâ”€ ç‰¹å¾µèåˆ MLP
     â””â”€ è¼¸å‡º features_dim (128ç¶­)
     â”‚
     â–¼
PPO Policy (Actor-Critic)
     â”‚
     â”œâ”€ Actor â†’ å‹•ä½œæ¦‚ç‡ P(a|s)
     â””â”€ Critic â†’ ç‹€æ…‹åƒ¹å€¼ V(s)
     â”‚
     â–¼
Action (0/1/2)
     â”‚
     â–¼
Environment Step
     â”‚
     â”œâ”€ åŸ·è¡Œäº¤æ˜“
     â”œâ”€ è¨ˆç®—çå‹µ
     â””â”€ æ›´æ–°ç‹€æ…‹
     â”‚
     â–¼
PPO å­¸ç¿’
     â”‚
     â”œâ”€ æ”¶é›† rollout (n_steps=2048)
     â”œâ”€ è¨ˆç®— Advantage
     â””â”€ æ›´æ–°ç­–ç•¥ (clip gradient)
```

---

## 3. è¨“ç·´æµç¨‹è©³è§£

### 3.1 ä¸»æµç¨‹ (main å‡½æ•¸)

```python
def main():
    """è¨“ç·´æµç¨‹ï¼ˆ9å€‹æ­¥é©Ÿï¼‰"""

    # 1. è¼‰å…¥é…ç½®æ–‡ä»¶
    config = load_config(args.config)

    # 2. é©—è­‰ DeepLOB æª¢æŸ¥é»
    verify_deeplob_checkpoint(args.deeplob_checkpoint)

    # 3. å‰µå»ºè¨“ç·´ç’°å¢ƒï¼ˆå‘é‡åŒ–ï¼‰
    env = create_vec_env(config, n_envs=args.n_envs)

    # 4. å‰µå»ºè©•ä¼°ç’°å¢ƒï¼ˆé©—è­‰é›†ï¼‰
    eval_env = create_eval_env(config)

    # 5. è¨­ç½®è¨“ç·´å›èª¿
    callbacks = create_callbacks(config, eval_env)

    # 6. å‰µå»º PPO + DeepLOB æ¨¡å‹
    model = create_ppo_deeplob_model(env, config, deeplob_checkpoint)

    # 7. é–‹å§‹è¨“ç·´
    model = train_model(model, total_timesteps, callbacks)

    # 8. ä¿å­˜æœ€çµ‚æ¨¡å‹
    save_path = save_final_model(model, config)

    # 9. è¼¸å‡ºç¸½çµå ±å‘Š
    logger.info(f"æœ€ä½³æ¨¡å‹: {save_path}")
```

### 3.2 è¨“ç·´éšæ®µç´°ç¯€

#### éšæ®µ A: åˆå§‹åŒ– (0-5 åˆ†é˜)

1. **æª¢æŸ¥ GPU å¯ç”¨æ€§**
   ```python
   if torch.cuda.is_available():
       device = 'cuda'
       logger.info(f"GPU: {torch.cuda.get_device_name(0)}")
   ```

2. **è¼‰å…¥ DeepLOB æ¨¡å‹**
   ```python
   checkpoint = torch.load(checkpoint_path)
   deeplob_model = DeepLOB(**model_config)
   deeplob_model.load_state_dict(checkpoint['model_state_dict'])
   deeplob_model.eval()  # è¨­ç‚ºè©•ä¼°æ¨¡å¼
   ```

3. **å‡çµ DeepLOB æ¬Šé‡**
   ```python
   for param in deeplob_model.parameters():
       param.requires_grad = False
   ```

#### éšæ®µ B: ç’°å¢ƒå‰µå»º (5-10 åˆ†é˜)

1. **è¼‰å…¥å°è‚¡æ•¸æ“š**
   - è¨“ç·´é›†: 5,584,553 æ¨£æœ¬
   - é©—è­‰é›†: 828,011 æ¨£æœ¬
   - æ”¯æŒæ•¸æ“šæ¡æ¨£ (é è¨­ 10%) ä»¥æ¸›å°‘è¨˜æ†¶é«”

2. **å‰µå»ºå‘é‡åŒ–ç’°å¢ƒ**
   ```python
   # å–®ç’°å¢ƒ
   env = DummyVecEnv([lambda: TaiwanLOBTradingEnv(config)])

   # å¤šç’°å¢ƒ (n_envs=4)
   env_fns = [make_env(config, i) for i in range(4)]
   env = SubprocVecEnv(env_fns)
   ```

#### éšæ®µ C: è¨“ç·´å¾ªç’° (ä¸»è¦æ™‚é–“)

**PPO è¨“ç·´å¾ªç’°**ï¼ˆæ¯ 2048 steps ä¸€æ¬¡æ›´æ–°ï¼‰ï¼š

```
For timestep in [0, total_timesteps]:
    # 1. æ”¶é›† Rollout (n_steps=2048)
    For step in [0, n_steps]:
        action = policy.predict(observation)
        next_obs, reward, done, info = env.step(action)
        buffer.add(obs, action, reward, value, log_prob)

    # 2. è¨ˆç®— Advantage & Returns
    advantages = compute_gae(rewards, values, gamma=0.99, gae_lambda=0.95)
    returns = advantages + values

    # 3. ç­–ç•¥æ›´æ–° (n_epochs=10)
    For epoch in [0, n_epochs]:
        For batch in get_batches(buffer, batch_size=64):
            # 3.1 è¨ˆç®—ç­–ç•¥æå¤±
            log_prob_new, entropy = policy.evaluate_actions(batch.obs, batch.actions)
            ratio = torch.exp(log_prob_new - batch.log_probs)

            # PPO Clip
            surr1 = ratio * batch.advantages
            surr2 = torch.clamp(ratio, 1-clip_range, 1+clip_range) * batch.advantages
            policy_loss = -torch.min(surr1, surr2).mean()

            # 3.2 è¨ˆç®—åƒ¹å€¼æå¤±
            value_pred = policy.predict_values(batch.obs)
            value_loss = F.mse_loss(value_pred, batch.returns)

            # 3.3 è¨ˆç®—ç†µæå¤±ï¼ˆé¼“å‹µæ¢ç´¢ï¼‰
            entropy_loss = -entropy.mean()

            # 3.4 ç¸½æå¤±
            loss = policy_loss + vf_coef*value_loss + ent_coef*entropy_loss

            # 3.5 æ¢¯åº¦æ›´æ–°
            optimizer.zero_grad()
            loss.backward()
            torch.nn.utils.clip_grad_norm_(policy.parameters(), max_grad_norm)
            optimizer.step()

    # 4. Callbacks
    if timestep % eval_freq == 0:
        evaluate_policy(eval_env)
        save_best_model()

    if timestep % checkpoint_freq == 0:
        save_checkpoint()
```

#### éšæ®µ D: è©•ä¼°èˆ‡ä¿å­˜

1. **é©—è­‰é›†è©•ä¼°** (æ¯ 10K steps)
   ```python
   mean_reward, std_reward = evaluate_policy(
       model, eval_env,
       n_eval_episodes=10
   )
   ```

2. **ä¿å­˜æª¢æŸ¥é»** (æ¯ 50K steps)
   ```python
   model.save(f"checkpoints/ppo_model_{timestep}_steps")
   ```

3. **ä¿å­˜æœ€ä½³æ¨¡å‹**
   ```python
   if mean_reward > best_mean_reward:
       model.save("checkpoints/best_model")
   ```

---

## 4. æ ¸å¿ƒæ¨¡çµ„æŠ€è¡“èªªæ˜

### 4.1 DeepLOBExtractor (ç‰¹å¾µæå–å™¨)

**æ–‡ä»¶**: [src/models/deeplob_feature_extractor.py](src/models/deeplob_feature_extractor.py:54)

#### æ ¸å¿ƒåŠŸèƒ½

å°‡é è¨“ç·´ DeepLOB æ¨¡å‹æ•´åˆåˆ° SB3 ç­–ç•¥ä¸­ï¼Œä½œç‚ºå‡çµçš„ç‰¹å¾µæå–å™¨ã€‚

#### é¡åˆ¥ç¹¼æ‰¿é—œä¿‚

```python
class DeepLOBExtractor(BaseFeaturesExtractor):
    """
    BaseFeaturesExtractor (SB3 æŠ½è±¡åŸºé¡)
        â”‚
        â”œâ”€ å®šç¾©æ¥å£: forward(observations) -> features
        â””â”€ å¿…é ˆå¯¦ç¾: __init__, forward
    """
```

#### åˆå§‹åŒ–åƒæ•¸

```python
DeepLOBExtractor(
    observation_space: gym.spaces.Box,  # è§€æ¸¬ç©ºé–“ (28ç¶­)
    features_dim: int = 128,             # è¼¸å‡ºç‰¹å¾µç¶­åº¦
    deeplob_checkpoint: str = None,      # DeepLOB æ¨¡å‹è·¯å¾‘
    use_lstm_hidden: bool = True,        # æ˜¯å¦ä½¿ç”¨ LSTM éš±è—å±¤
    freeze_deeplob: bool = True,         # æ˜¯å¦å‡çµ DeepLOB
    extractor_net_arch: list = None      # MLP ç¶²çµ¡æ¶æ§‹
)
```

#### æ¶æ§‹è¨­è¨ˆ

**æ¨¡å¼é¸æ“‡**:

1. **Mode 1 (ç°¡å–®æ¨¡å¼)** - `use_lstm_hidden=False`
   ```
   è§€æ¸¬ (28ç¶­) â†’ MLP â†’ features_dim

   è¼¸å…¥: [LOB(20) + DeepLOB_Pred(3) + State(5)]
   ç¶²çµ¡: Linear(28 â†’ 256) â†’ ReLU â†’ Linear(256 â†’ 128) â†’ ReLU
         â†’ Linear(128 â†’ features_dim) â†’ ReLU
   è¼¸å‡º: features (features_dim ç¶­)
   ```

2. **Mode 2 (ç‰¹å¾µæå–æ¨¡å¼)** - `use_lstm_hidden=True`
   ```
   LOB â†’ DeepLOB.LSTM â†’ Hidden(64) â”
   State (5ç¶­)                      â”œâ†’ Concat â†’ MLP â†’ features_dim
                                    â”˜

   ç†è«–è¼¸å…¥: [LOB(20) + State(5) + LSTM_Hidden(64)] = 89ç¶­
   å¯¦éš›: ç°¡åŒ–ç‚º Mode 1ï¼ˆç•¶å‰å¯¦ç¾ï¼‰
   ```

   **è¨»**: Mode 2 å®Œæ•´å¯¦ç¾éœ€è¦ä¿®æ”¹ç’°å¢ƒä»¥å‚³é LOB æ™‚åºæ•¸æ“šã€‚

#### æ¬Šé‡å‡çµæ©Ÿåˆ¶

```python
def _load_deeplob(self, checkpoint_path: str):
    """è¼‰å…¥ä¸¦å‡çµ DeepLOB"""
    # 1. è¼‰å…¥æª¢æŸ¥é»
    checkpoint = torch.load(checkpoint_path, weights_only=False)

    # 2. æå–é…ç½®
    model_config = checkpoint['config']['model']

    # 3. åˆå§‹åŒ–æ¨¡å‹
    model = DeepLOB(**model_config)
    model.load_state_dict(checkpoint['model_state_dict'])

    # 4. å‡çµæ¬Šé‡
    for param in model.parameters():
        param.requires_grad = False

    # 5. è©•ä¼°æ¨¡å¼
    model.eval()

    return model
```

#### Forward å‚³æ’­

```python
def forward(self, observations: torch.Tensor) -> torch.Tensor:
    """ç‰¹å¾µæå–

    è¼¸å…¥: observations (batch_size, 28)
    è¼¸å‡º: features (batch_size, features_dim)
    """
    # Mode 1: ä½¿ç”¨å®Œæ•´è§€æ¸¬
    combined_features = observations  # (batch, 28)

    # é€šé MLP æå–å™¨
    features = self.extractor_net(combined_features)  # (batch, features_dim)

    return features
```

---

### 4.2 TaiwanLOBTradingEnv (äº¤æ˜“ç’°å¢ƒ)

**æ–‡ä»¶**: [src/envs/tw_lob_trading_env.py](src/envs/tw_lob_trading_env.py:31)

#### ç’°å¢ƒè¦æ ¼

| å±¬æ€§ | è¦æ ¼ | èªªæ˜ |
|------|------|------|
| è§€æ¸¬ç©ºé–“ | Box(28,) | é€£çºŒå‘é‡ |
| å‹•ä½œç©ºé–“ | Discrete(3) | {0: Sell, 1: Hold, 2: Buy} |
| çå‹µç¯„åœ | [-âˆ, +âˆ] | ç„¡ç•Œï¼ˆPnL basedï¼‰ |
| Episode é•·åº¦ | 500 steps | å¯é…ç½® |

#### è§€æ¸¬çµ„æˆ (28ç¶­)

```python
observation = [
    # ===== LOB åŸå§‹ç‰¹å¾µ (20ç¶­) =====
    # 5æª”è²·è³£åƒ¹é‡ï¼ˆäº¤éŒ¯å¼ï¼‰
    best_bid_price,    # æœ€ä½³è²·åƒ¹
    best_bid_volume,   # æœ€ä½³è²·é‡
    bid_2_price,       # ç¬¬2æª”è²·åƒ¹
    bid_2_volume,      # ç¬¬2æª”è²·é‡
    ...,               # è²·æ–¹ 3-5 æª”
    best_ask_price,    # æœ€ä½³è³£åƒ¹
    best_ask_volume,   # æœ€ä½³è³£é‡
    ask_2_price,       # ç¬¬2æª”è³£åƒ¹
    ask_2_volume,      # ç¬¬2æª”è³£é‡
    ...,               # è³£æ–¹ 3-5 æª”

    # ===== DeepLOB é æ¸¬ (3ç¶­) =====
    prob_down,         # ä¸‹è·Œæ¦‚ç‡
    prob_stationary,   # æŒå¹³æ¦‚ç‡
    prob_up,           # ä¸Šæ¼²æ¦‚ç‡

    # ===== äº¤æ˜“ç‹€æ…‹ (5ç¶­) =====
    normalized_position,      # æŒå€‰ / max_position
    normalized_inventory,     # åº«å­˜ / initial_balance
    normalized_cost,          # æˆæœ¬ / initial_balance
    time_progress,            # current_step / max_steps
    prev_action_normalized    # prev_action / 2.0
]
```

#### å‹•ä½œç©ºé–“

```python
action_space = Discrete(3)

# å‹•ä½œæ˜ å°„
0 â†’ Sell (è³£å‡º 1 å–®ä½)
1 â†’ Hold (æŒæœ‰ï¼Œä¸å‹•)
2 â†’ Buy (è²·å…¥ 1 å–®ä½)

# æŒå€‰é™åˆ¶
position âˆˆ [-max_position, +max_position]  # é è¨­ [-1, +1]
```

#### çå‹µå‡½æ•¸ (å¤šçµ„ä»¶è¨­è¨ˆ)

**çå‹µè¨ˆç®—ç”± `RewardShaper` åŸ·è¡Œ**ï¼Œä¸»è¦çµ„ä»¶ï¼š

1. **åŸºç¤ PnL çå‹µ**
   ```python
   pnl_reward = position * (next_price - prev_price)
   ```
   - æŒå¤šå€‰ï¼šåƒ¹æ ¼ä¸Šæ¼² â†’ æ­£çå‹µ
   - æŒç©ºå€‰ï¼šåƒ¹æ ¼ä¸‹è·Œ â†’ æ­£çå‹µ

2. **äº¤æ˜“æˆæœ¬æ‡²ç½°**
   ```python
   cost_penalty = -transaction_cost
   transaction_cost = |position_change| * price * 0.001
   ```
   - æ‰‹çºŒè²»ç‡: 0.1% (å¯é…ç½®)

3. **åº«å­˜æ‡²ç½°**
   ```python
   inventory_penalty = -abs(inventory) * inventory_weight
   ```
   - é¿å…é•·æ™‚é–“æŒå€‰
   - é¼“å‹µå¿«é€Ÿå¹³å€‰

4. **é¢¨éšªèª¿æ•´é …**
   ```python
   risk_penalty = -abs(position) * volatility * risk_weight
   ```
   - é«˜æ³¢å‹•ç‡ â†’ å¢åŠ æ‡²ç½°
   - é¼“å‹µé¢¨éšªæ§åˆ¶

**ç¸½çå‹µ**:
```python
total_reward = pnl_reward + cost_penalty + inventory_penalty + risk_penalty
```

#### æ ¸å¿ƒæ–¹æ³•

**1. reset() - é‡ç½®ç’°å¢ƒ**

```python
def reset(self, seed=None, options=None) -> Tuple[obs, info]:
    """é‡ç½®ç’°å¢ƒåˆ°åˆå§‹ç‹€æ…‹"""
    # 1. é‡ç½®ç‹€æ…‹è®Šæ•¸
    self.current_step = 0
    self.position = 0
    self.entry_price = 0.0
    self.balance = self.initial_balance
    self.inventory = 0.0
    self.total_cost = 0.0
    self.prev_action = 0

    # 2. éš¨æ©Ÿé¸æ“‡èµ·å§‹æ¨£æœ¬
    max_start = self.data_length - self.max_steps
    self.current_data_idx = np.random.randint(0, max_start)

    # 3. åˆå§‹åŒ– LOB æ­·å² (100 timesteps)
    self.lob_history = self.lob_data[self.current_data_idx].tolist()

    # 4. ç”Ÿæˆè§€æ¸¬
    obs = self._get_observation()
    info = self._get_info()

    return obs, info
```

**2. step() - åŸ·è¡Œå‹•ä½œ**

```python
def step(self, action: int) -> Tuple[obs, reward, terminated, truncated, info]:
    """åŸ·è¡Œäº¤æ˜“å‹•ä½œä¸¦æ›´æ–°ç’°å¢ƒ"""
    # 1. ç²å–ç•¶å‰åƒ¹æ ¼
    current_price = self.prices[self.current_data_idx]

    # 2. åŸ·è¡Œäº¤æ˜“é‚è¼¯
    if action == 0:  # Sell
        self.position = max(-self.max_position, self.position - 1)
    elif action == 2:  # Buy
        self.position = min(self.max_position, self.position + 1)
    # action == 1: Hold (ä¸å‹•)

    # 3. è¨ˆç®—äº¤æ˜“æˆæœ¬
    if action != prev_position + 1:
        position_change = abs(action - (prev_position + 1))
        transaction_cost = position_change * current_price * 0.001
        self.total_cost += transaction_cost

    # 4. æ›´æ–°åº«å­˜
    if self.position != 0:
        self.inventory = self.position * (current_price - self.entry_price)

    # 5. æ¨é€²æ™‚é–“
    self.current_step += 1
    self.current_data_idx += 1
    next_price = self.prices[self.current_data_idx]

    # 6. è¨ˆç®—çå‹µ
    reward = self.reward_shaper.calculate_reward(
        prev_state, action, new_state, transaction_cost
    )

    # 7. æª¢æŸ¥çµ‚æ­¢
    terminated = False  # ç„¡æå‰çµ‚æ­¢æ¢ä»¶
    truncated = (self.current_step >= self.max_steps)

    # 8. ç”Ÿæˆæ–°è§€æ¸¬
    obs = self._get_observation()
    info = self._get_info()

    return obs, reward, terminated, truncated, info
```

**3. _get_observation() - ç”Ÿæˆè§€æ¸¬**

```python
def _get_observation(self) -> np.ndarray:
    """ç”Ÿæˆ 28 ç¶­è§€æ¸¬"""
    # 1. ç•¶å‰ LOB ç‰¹å¾µï¼ˆæœ€å¾Œä¸€å€‹æ™‚é–“æ­¥ï¼‰
    current_lob = np.array(self.lob_history[-1], dtype=np.float32)  # (20,)

    # 2. DeepLOB é æ¸¬
    if self.deeplob_model is not None:
        with torch.no_grad():
            lob_seq = torch.FloatTensor(self.lob_history).unsqueeze(0)  # (1, 100, 20)
            deeplob_probs = self.deeplob_model.predict_proba(lob_seq)[0].numpy()  # (3,)
    else:
        deeplob_probs = np.random.rand(3).astype(np.float32)
        deeplob_probs /= deeplob_probs.sum()

    # 3. äº¤æ˜“ç‹€æ…‹
    state_features = np.array([
        self.position / self.max_position,
        self.inventory / self.initial_balance,
        self.total_cost / self.initial_balance,
        self.current_step / self.max_steps,
        self.prev_action / 2.0
    ], dtype=np.float32)  # (5,)

    # 4. ä¸²æ¥æ‰€æœ‰ç‰¹å¾µ
    obs = np.concatenate([current_lob, deeplob_probs, state_features])  # (28,)

    return obs
```

---

### 4.3 PPO (Proximal Policy Optimization)

**æ¡†æ¶**: Stable-Baselines3

#### ç®—æ³•åŸç†

PPO æ˜¯ä¸€ç¨® **on-policy** å¼·åŒ–å­¸ç¿’ç®—æ³•ï¼Œé€šéé™åˆ¶ç­–ç•¥æ›´æ–°å¹…åº¦ä¾†ä¿è­‰è¨“ç·´ç©©å®šæ€§ã€‚

**æ ¸å¿ƒå…¬å¼**:

1. **ç­–ç•¥æ¢¯åº¦ç›®æ¨™ï¼ˆæœ‰ç´„æŸï¼‰**
   ```
   L^CLIP(Î¸) = E_t[ min(r_t(Î¸)Â·A_t, clip(r_t(Î¸), 1-Îµ, 1+Îµ)Â·A_t) ]

   å…¶ä¸­:
   r_t(Î¸) = Ï€_Î¸(a_t|s_t) / Ï€_Î¸_old(a_t|s_t)  # é‡è¦æ€§æ¡æ¨£æ¯”
   A_t = å„ªå‹¢å‡½æ•¸ (Advantage)
   Îµ = clip_range (é è¨­ 0.2)
   ```

2. **å„ªå‹¢å‡½æ•¸ï¼ˆGAEï¼‰**
   ```
   A_t = Î´_t + (Î³Î»)Î´_{t+1} + (Î³Î»)^2Â·Î´_{t+2} + ...

   å…¶ä¸­:
   Î´_t = r_t + Î³Â·V(s_{t+1}) - V(s_t)  # TD èª¤å·®
   Î³ = æŠ˜æ‰£å› å­ (0.99)
   Î» = GAE lambda (0.95)
   ```

3. **åƒ¹å€¼å‡½æ•¸æå¤±**
   ```
   L^VF(Î¸) = E_t[ (V_Î¸(s_t) - V_t^target)^2 ]

   V_t^target = A_t + V(s_t)  # Returns
   ```

4. **ç†µæå¤±ï¼ˆé¼“å‹µæ¢ç´¢ï¼‰**
   ```
   L^ENT(Î¸) = E_t[ -H(Ï€_Î¸(Â·|s_t)) ]

   H(Ï€) = -Î£ Ï€(a|s)Â·log(Ï€(a|s))  # ç­–ç•¥ç†µ
   ```

5. **ç¸½æå¤±**
   ```
   L(Î¸) = -L^CLIP(Î¸) + c1Â·L^VF(Î¸) - c2Â·L^ENT(Î¸)

   c1 = vf_coef (0.5)
   c2 = ent_coef (0.01)
   ```

#### PPO é…ç½®

```python
model = PPO(
    "MlpPolicy",           # ç­–ç•¥é¡å‹ï¼ˆå¤šå±¤æ„ŸçŸ¥å™¨ï¼‰
    env,                   # ç’°å¢ƒ

    # ===== å­¸ç¿’ç‡é…ç½® =====
    learning_rate=3e-4,    # å„ªåŒ–å™¨å­¸ç¿’ç‡

    # ===== Rollout é…ç½® =====
    n_steps=2048,          # Rollout buffer å¤§å°ï¼ˆæ¯æ¬¡æ”¶é›† 2048 æ­¥ï¼‰
    batch_size=64,         # Mini-batch å¤§å°
    n_epochs=10,           # æ¯æ¬¡ rollout æ›´æ–° 10 æ¬¡

    # ===== æŠ˜æ‰£èˆ‡å„ªå‹¢ä¼°è¨ˆ =====
    gamma=0.99,            # æŠ˜æ‰£å› å­
    gae_lambda=0.95,       # GAE Î»

    # ===== PPO Clip =====
    clip_range=0.2,        # ç­–ç•¥æ›´æ–°è£å‰ªç¯„åœ [0.8, 1.2]

    # ===== æå¤±ä¿‚æ•¸ =====
    ent_coef=0.01,         # ç†µä¿‚æ•¸ï¼ˆé¼“å‹µæ¢ç´¢ï¼‰
    vf_coef=0.5,           # åƒ¹å€¼å‡½æ•¸ä¿‚æ•¸
    max_grad_norm=0.5,     # æ¢¯åº¦è£å‰ª

    # ===== ç­–ç•¥ç¶²çµ¡é…ç½® =====
    policy_kwargs=dict(
        features_extractor_class=DeepLOBExtractor,
        features_extractor_kwargs=dict(
            features_dim=128,
            deeplob_checkpoint="checkpoints/deeplob_v5_best.pth"
        ),
        net_arch=dict(
            pi=[256, 128],  # Actor: 128 â†’ 256 â†’ 128 â†’ 3
            vf=[256, 128]   # Critic: 128 â†’ 256 â†’ 128 â†’ 1
        )
    ),

    # ===== æ—¥èªŒèˆ‡è¨­å‚™ =====
    tensorboard_log="logs/sb3_deeplob/",
    verbose=1,
    device="cuda"
)
```

#### Actor-Critic æ¶æ§‹

```
è§€æ¸¬ (28ç¶­)
    â”‚
    â–¼
DeepLOBExtractor (å‡çµ DeepLOB + MLP)
    â”‚
    â”œâ”€ DeepLOB (å‡çµ) â†’ LOB æ·±å±¤ç‰¹å¾µ
    â”œâ”€ MLP: 28 â†’ 256 â†’ 128
    â””â”€ è¼¸å‡º: features (128ç¶­)
    â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚              â”‚              â”‚
    â–¼              â–¼              â–¼
Actor (Ï€)     Critic (V)     (å…±äº«ç‰¹å¾µ)
    â”‚              â”‚
Linear(128â†’256) Linear(128â†’256)
ReLU            ReLU
Linear(256â†’128) Linear(256â†’128)
ReLU            ReLU
Linear(128â†’3)   Linear(128â†’1)
Softmax         -
    â”‚              â”‚
    â–¼              â–¼
å‹•ä½œæ¦‚ç‡        ç‹€æ…‹åƒ¹å€¼
P(a|s)          V(s)
```

---

## 5. è‡ªå®šç¾©å‡½æ•¸èªªæ˜

### 5.1 é…ç½®èˆ‡é©—è­‰å‡½æ•¸

#### load_config()

**ä½ç½®**: [train_sb3_deeplob.py:76](train_sb3_deeplob.py:76)

```python
def load_config(config_path: str) -> dict:
    """è¼‰å…¥ YAML é…ç½®æ–‡ä»¶

    åƒæ•¸:
        config_path: é…ç½®æ–‡ä»¶è·¯å¾‘ (å¦‚ 'configs/sb3_config.yaml')

    è¿”å›:
        config: é…ç½®å­—å…¸

    é…ç½®çµæ§‹:
        {
            'env_config': {...},           # ç’°å¢ƒé…ç½®
            'ppo': {...},                  # PPO è¶…åƒæ•¸
            'deeplob_extractor': {...},    # DeepLOB ç‰¹å¾µæå–å™¨é…ç½®
            'training': {...},             # è¨“ç·´é…ç½®
            'callbacks': {...},            # å›èª¿é…ç½®
            'test_mode': {...}             # æ¸¬è©¦æ¨¡å¼é…ç½®
        }
    """
    with open(config_path, 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)
    return config
```

#### verify_deeplob_checkpoint()

**ä½ç½®**: [train_sb3_deeplob.py:83](train_sb3_deeplob.py:83)

```python
def verify_deeplob_checkpoint(checkpoint_path: str):
    """é©—è­‰ DeepLOB æª¢æŸ¥é»å­˜åœ¨ä¸”å¯è¼‰å…¥

    åŠŸèƒ½:
        1. æª¢æŸ¥æ–‡ä»¶å­˜åœ¨æ€§
        2. å˜—è©¦è¼‰å…¥æª¢æŸ¥é»
        3. é¡¯ç¤ºæ¨¡å‹ä¿¡æ¯ï¼ˆepoch, æº–ç¢ºç‡ç­‰ï¼‰

    åƒæ•¸:
        checkpoint_path: DeepLOB æª¢æŸ¥é»è·¯å¾‘

    æ‹‹å‡º:
        FileNotFoundError: æ–‡ä»¶ä¸å­˜åœ¨
        RuntimeError: æª¢æŸ¥é»è¼‰å…¥å¤±æ•—

    æª¢æŸ¥é»æ ¼å¼:
        {
            'epoch': int,
            'model_state_dict': OrderedDict,
            'optimizer_state_dict': OrderedDict,
            'val_acc': float,
            'test_acc': float,
            'config': {
                'model': {
                    'input_shape': [100, 20],
                    'num_classes': 3,
                    'conv1_filters': 32,
                    ...
                }
            }
        }
    """
    if not os.path.exists(checkpoint_path):
        raise FileNotFoundError(f"DeepLOB æª¢æŸ¥é»ä¸å­˜åœ¨: {checkpoint_path}")

    try:
        checkpoint = torch.load(checkpoint_path, map_location='cpu', weights_only=False)
        logger.info(f"âœ… DeepLOB æª¢æŸ¥é»é©—è­‰æˆåŠŸ: {checkpoint_path}")

        if isinstance(checkpoint, dict):
            if 'epoch' in checkpoint:
                logger.info(f"  - Epoch: {checkpoint['epoch']}")
            if 'val_acc' in checkpoint:
                logger.info(f"  - é©—è­‰æº–ç¢ºç‡: {checkpoint['val_acc']:.4f}")

    except Exception as e:
        raise RuntimeError(f"DeepLOB æª¢æŸ¥é»è¼‰å…¥å¤±æ•—: {e}")
```

---

### 5.2 ç’°å¢ƒå‰µå»ºå‡½æ•¸

#### make_env()

**ä½ç½®**: [train_sb3_deeplob.py:109](train_sb3_deeplob.py:109)

```python
def make_env(env_config: dict, rank: int = 0):
    """ç’°å¢ƒå·¥å» å‡½æ•¸ï¼ˆç”¨æ–¼å‘é‡åŒ–ç’°å¢ƒï¼‰

    åƒæ•¸:
        env_config: ç’°å¢ƒé…ç½®å­—å…¸
        rank: ç’°å¢ƒç·¨è™Ÿï¼ˆç”¨æ–¼å¤šé€²ç¨‹ç’°å¢ƒï¼‰

    è¿”å›:
        _init: ç’°å¢ƒåˆå§‹åŒ–å‡½æ•¸

    ä½¿ç”¨å ´æ™¯:
        - å‰µå»ºå¤šå€‹ä¸¦è¡Œç’°å¢ƒï¼ˆSubprocVecEnvï¼‰
        - æ¯å€‹ç’°å¢ƒæœ‰ç¨ç«‹çš„éš¨æ©Ÿç¨®å­

    ç¯„ä¾‹:
        env_fns = [make_env(config, i) for i in range(4)]
        env = SubprocVecEnv(env_fns)
    """
    def _init():
        env = TaiwanLOBTradingEnv(env_config)
        env = Monitor(env)  # åŒ…è£ Monitorï¼ˆè¨˜éŒ„ episode çµ±è¨ˆï¼‰
        return env
    return _init
```

#### create_vec_env()

**ä½ç½®**: [train_sb3_deeplob.py:118](train_sb3_deeplob.py:118)

```python
def create_vec_env(config: dict, n_envs: int = 1, vec_type: str = "dummy"):
    """å‰µå»ºå‘é‡åŒ–ç’°å¢ƒ

    åƒæ•¸:
        config: å®Œæ•´é…ç½®å­—å…¸
        n_envs: ä¸¦è¡Œç’°å¢ƒæ•¸é‡
        vec_type: å‘é‡åŒ–é¡å‹
            - "dummy": DummyVecEnvï¼ˆå–®é€²ç¨‹ï¼Œé©åˆèª¿è©¦ï¼‰
            - "subproc": SubprocVecEnvï¼ˆå¤šé€²ç¨‹ï¼Œé©åˆè¨“ç·´ï¼‰

    è¿”å›:
        env: å‘é‡åŒ–ç’°å¢ƒ

    å‘é‡åŒ–ç’°å¢ƒé¡å‹å°æ¯”:

        DummyVecEnv:
            - å–®é€²ç¨‹é †åºåŸ·è¡Œ
            - è¨˜æ†¶é«”å…±äº«
            - é©åˆèª¿è©¦ã€å°è¦æ¨¡è¨“ç·´
            - æ€§èƒ½: â­â­

        SubprocVecEnv:
            - å¤šé€²ç¨‹ä¸¦è¡ŒåŸ·è¡Œ
            - æ¯å€‹ç’°å¢ƒç¨ç«‹é€²ç¨‹
            - é©åˆå¤§è¦æ¨¡è¨“ç·´ï¼ˆå……åˆ†åˆ©ç”¨å¤šæ ¸ CPUï¼‰
            - æ€§èƒ½: â­â­â­â­

    ç¯„ä¾‹:
        # å–®ç’°å¢ƒ
        env = create_vec_env(config, n_envs=1)

        # 4 å€‹ä¸¦è¡Œç’°å¢ƒï¼ˆå¤šé€²ç¨‹ï¼‰
        env = create_vec_env(config, n_envs=4, vec_type="subproc")
    """
    env_config = config['env_config']

    if n_envs == 1:
        env = TaiwanLOBTradingEnv(env_config)
        env = Monitor(env)
        env = DummyVecEnv([lambda: env])
        logger.info("âœ… å‰µå»ºå–®ä¸€ç’°å¢ƒ")
    else:
        env_fns = [make_env(env_config, i) for i in range(n_envs)]

        if vec_type == "subproc":
            env = SubprocVecEnv(env_fns)
            logger.info(f"âœ… å‰µå»º SubprocVecEnv ({n_envs} å€‹ç’°å¢ƒ)")
        else:
            env = DummyVecEnv(env_fns)
            logger.info(f"âœ… å‰µå»º DummyVecEnv ({n_envs} å€‹ç’°å¢ƒ)")

    return env
```

#### create_eval_env()

**ä½ç½®**: [train_sb3_deeplob.py:140](train_sb3_deeplob.py:140)

```python
def create_eval_env(config: dict):
    """å‰µå»ºè©•ä¼°ç’°å¢ƒï¼ˆä½¿ç”¨é©—è­‰é›†ï¼‰

    åƒæ•¸:
        config: å®Œæ•´é…ç½®å­—å…¸

    è¿”å›:
        env: è©•ä¼°ç’°å¢ƒï¼ˆå–®å€‹ï¼Œéå‘é‡åŒ–ï¼‰

    é…ç½®è¦†è“‹:
        eval_env_config:
            data_mode: 'val'          # ä½¿ç”¨é©—è­‰é›†
            data_sample_ratio: 0.1    # 10% æ¡æ¨£ï¼ˆæ¸›å°‘è©•ä¼°æ™‚é–“ï¼‰

    æ³¨æ„:
        - è©•ä¼°ç’°å¢ƒä¸å‘é‡åŒ–ï¼ˆä¸éœ€è¦ä¸¦è¡Œï¼‰
        - ä½¿ç”¨é©—è­‰é›†æ•¸æ“šï¼ˆé¿å…éæ“¬åˆæ¸¬è©¦é›†ï¼‰
    """
    eval_config = config.get('evaluation', {}).get('eval_env_config', {})
    env_config = config['env_config'].copy()
    env_config.update(eval_config)

    env = TaiwanLOBTradingEnv(env_config)
    env = Monitor(env)
    logger.info("âœ… å‰µå»ºè©•ä¼°ç’°å¢ƒï¼ˆé©—è­‰é›†ï¼‰")

    return env
```

---

### 5.3 å›èª¿å‰µå»ºå‡½æ•¸

#### create_callbacks()

**ä½ç½®**: [train_sb3_deeplob.py:153](train_sb3_deeplob.py:153)

```python
def create_callbacks(config: dict, eval_env):
    """å‰µå»ºè¨“ç·´å›èª¿

    åƒæ•¸:
        config: å®Œæ•´é…ç½®å­—å…¸
        eval_env: è©•ä¼°ç’°å¢ƒ

    è¿”å›:
        CallbackList: å›èª¿åˆ—è¡¨ï¼ˆæˆ– Noneï¼‰

    åŒ…å«å›èª¿:
        1. CheckpointCallback: å®šæœŸä¿å­˜æ¨¡å‹
        2. EvalCallback: å®šæœŸè©•ä¼°ä¸¦ä¿å­˜æœ€ä½³æ¨¡å‹

    é…ç½®ç¯„ä¾‹:
        callbacks:
            checkpoint:
                enabled: true
                save_freq: 50000        # æ¯ 50K steps
                save_path: 'checkpoints/sb3/ppo_deeplob'
                name_prefix: 'ppo_model'

            eval:
                enabled: true
                eval_freq: 10000        # æ¯ 10K steps
                n_eval_episodes: 10     # è©•ä¼° 10 å€‹ episodes
                best_model_save_path: 'checkpoints/sb3/ppo_deeplob'
                deterministic: true     # è©•ä¼°æ™‚ä½¿ç”¨ç¢ºå®šæ€§ç­–ç•¥
    """
    callbacks = []

    # CheckpointCallback
    checkpoint_config = config.get('callbacks', {}).get('checkpoint', {})
    if checkpoint_config.get('enabled', True):
        checkpoint_callback = CheckpointCallback(
            save_freq=checkpoint_config.get('save_freq', 50000),
            save_path=checkpoint_config.get('save_path', 'checkpoints/sb3/ppo_deeplob'),
            name_prefix=checkpoint_config.get('name_prefix', 'ppo_model'),
            save_replay_buffer=False,
            save_vecnormalize=False,
        )
        callbacks.append(checkpoint_callback)

    # EvalCallback
    eval_config = config.get('callbacks', {}).get('eval', {})
    if eval_config.get('enabled', True) and eval_env is not None:
        eval_callback = EvalCallback(
            eval_env,
            eval_freq=eval_config.get('eval_freq', 10000),
            n_eval_episodes=eval_config.get('n_eval_episodes', 10),
            best_model_save_path=eval_config.get('best_model_save_path', 'checkpoints/sb3/ppo_deeplob'),
            log_path=eval_config.get('log_path', 'logs/sb3_eval'),
            deterministic=eval_config.get('deterministic', True),
            render=False,
        )
        callbacks.append(eval_callback)

    return CallbackList(callbacks) if len(callbacks) > 0 else None
```

---

### 5.4 æ¨¡å‹å‰µå»ºèˆ‡è¨“ç·´å‡½æ•¸

#### create_ppo_deeplob_model()

**ä½ç½®**: [train_sb3_deeplob.py:191](train_sb3_deeplob.py:191)

```python
def create_ppo_deeplob_model(env, config: dict, deeplob_checkpoint: str, device: str = "cuda"):
    """å‰µå»ºæ•´åˆ DeepLOB çš„ PPO æ¨¡å‹

    åƒæ•¸:
        env: å‘é‡åŒ–ç’°å¢ƒ
        config: å®Œæ•´é…ç½®å­—å…¸
        deeplob_checkpoint: DeepLOB æª¢æŸ¥é»è·¯å¾‘
        device: è¨­å‚™ ('cuda' / 'cpu')

    è¿”å›:
        model: PPO æ¨¡å‹å¯¦ä¾‹

    æ ¸å¿ƒæµç¨‹:
        1. å‰µå»º DeepLOB ç‰¹å¾µæå–å™¨é…ç½®
        2. é…ç½® PPO è¶…åƒæ•¸
        3. åˆå§‹åŒ– PPO æ¨¡å‹

    Policy æ¶æ§‹:
        è§€æ¸¬ (28) â†’ DeepLOBExtractor (128) â†’ Actor/Critic

        Actor: 128 â†’ 256 â†’ 128 â†’ 3 (å‹•ä½œæ¦‚ç‡)
        Critic: 128 â†’ 256 â†’ 128 â†’ 1 (ç‹€æ…‹åƒ¹å€¼)
    """
    ppo_config = config.get('ppo', {})
    deeplob_config = config.get('deeplob_extractor', {})

    # å‰µå»º policy_kwargs
    if deeplob_config.get('use_deeplob', True):
        policy_kwargs = make_deeplob_policy_kwargs(
            deeplob_checkpoint=deeplob_checkpoint,
            features_dim=deeplob_config.get('features_dim', 128),
            use_lstm_hidden=deeplob_config.get('use_lstm_hidden', False),
            net_arch=ppo_config.get('net_arch', dict(pi=[256, 128], vf=[256, 128]))
        )
    else:
        # å›é€€åˆ°åŸºç¤ MlpPolicy
        policy_kwargs = dict(
            net_arch=ppo_config.get('net_arch', dict(pi=[256, 128], vf=[256, 128]))
        )

    # å‰µå»º PPO æ¨¡å‹
    model = PPO(
        "MlpPolicy",
        env,
        learning_rate=ppo_config.get('learning_rate', 3e-4),
        n_steps=ppo_config.get('n_steps', 2048),
        batch_size=ppo_config.get('batch_size', 64),
        n_epochs=ppo_config.get('n_epochs', 10),
        gamma=ppo_config.get('gamma', 0.99),
        gae_lambda=ppo_config.get('gae_lambda', 0.95),
        clip_range=ppo_config.get('clip_range', 0.2),
        ent_coef=ppo_config.get('ent_coef', 0.01),
        vf_coef=ppo_config.get('vf_coef', 0.5),
        max_grad_norm=ppo_config.get('max_grad_norm', 0.5),
        policy_kwargs=policy_kwargs,
        tensorboard_log=config.get('training', {}).get('tensorboard_log', 'logs/sb3_deeplob'),
        verbose=ppo_config.get('verbose', 1),
        seed=ppo_config.get('seed', 42),
        device=device
    )

    logger.info("âœ… PPO + DeepLOB æ¨¡å‹å‰µå»ºæˆåŠŸ")
    return model
```

#### train_model()

**ä½ç½®**: [train_sb3_deeplob.py:253](train_sb3_deeplob.py:253)

```python
def train_model(model, total_timesteps: int, callbacks=None, log_interval: int = 10):
    """è¨“ç·´æ¨¡å‹

    åƒæ•¸:
        model: PPO æ¨¡å‹å¯¦ä¾‹
        total_timesteps: ç¸½è¨“ç·´æ­¥æ•¸
        callbacks: å›èª¿åˆ—è¡¨
        log_interval: æ—¥èªŒè¼¸å‡ºé–“éš”

    è¿”å›:
        model: è¨“ç·´å¾Œçš„æ¨¡å‹

    è¨“ç·´çµ±è¨ˆ:
        - è¨“ç·´æ™‚é–“
        - è¨“ç·´é€Ÿåº¦ (steps/sec)
        - TensorBoard æ—¥èªŒ

    æ—¥èªŒå…§å®¹:
        - rollout/ep_rew_mean: Episode å¹³å‡çå‹µ
        - rollout/ep_len_mean: Episode å¹³å‡é•·åº¦
        - train/policy_loss: ç­–ç•¥æå¤±
        - train/value_loss: åƒ¹å€¼æå¤±
        - train/entropy_loss: ç†µæå¤±
        - train/clip_fraction: Clip æ¯”ä¾‹ï¼ˆç­–ç•¥æ›´æ–°è¢«è£å‰ªçš„æ¯”ä¾‹ï¼‰
    """
    logger.info("=" * 60)
    logger.info("ğŸš€ é–‹å§‹è¨“ç·´ (PPO + DeepLOB)")
    logger.info("=" * 60)
    logger.info(f"ç¸½æ­¥æ•¸: {total_timesteps:,}")

    start_time = datetime.now()

    model.learn(
        total_timesteps=total_timesteps,
        callback=callbacks,
        log_interval=log_interval,
        progress_bar=True
    )

    end_time = datetime.now()
    duration = (end_time - start_time).total_seconds()

    logger.info("âœ… è¨“ç·´å®Œæˆ")
    logger.info(f"è¨“ç·´æ™‚é–“: {duration/60:.2f} åˆ†é˜ ({duration/3600:.2f} å°æ™‚)")
    logger.info(f"è¨“ç·´é€Ÿåº¦: {total_timesteps/duration:.1f} steps/sec")

    return model
```

#### save_final_model()

**ä½ç½®**: [train_sb3_deeplob.py:281](train_sb3_deeplob.py:281)

```python
def save_final_model(model, config: dict):
    """ä¿å­˜æœ€çµ‚æ¨¡å‹

    åƒæ•¸:
        model: è¨“ç·´å¾Œçš„ PPO æ¨¡å‹
        config: å®Œæ•´é…ç½®å­—å…¸

    è¿”å›:
        save_path: ä¿å­˜è·¯å¾‘

    ä¿å­˜å…§å®¹:
        - æ¨¡å‹æ¬Šé‡ (policy + value network)
        - å„ªåŒ–å™¨ç‹€æ…‹
        - ç’°å¢ƒæ¨™æº–åŒ–åƒæ•¸ï¼ˆå¦‚æœæœ‰ï¼‰

    æ–‡ä»¶æ ¼å¼:
        .zip å£“ç¸®æ–‡ä»¶ï¼ˆSB3 æ¨™æº–æ ¼å¼ï¼‰

    ç¯„ä¾‹:
        checkpoints/sb3/ppo_deeplob/ppo_deeplob_final.zip
    """
    save_dir = config.get('training', {}).get('checkpoint_dir', 'checkpoints/sb3/ppo_deeplob')
    model_name = config.get('training', {}).get('final_model_name', 'ppo_deeplob_final')

    os.makedirs(save_dir, exist_ok=True)
    save_path = os.path.join(save_dir, model_name)

    model.save(save_path)
    logger.info(f"âœ… æœ€çµ‚æ¨¡å‹å·²ä¿å­˜: {save_path}.zip")

    return save_path
```

---

## 6. æ¨¡å‹å­¸ç¿’æ©Ÿåˆ¶

### 6.1 é›™å±¤å­¸ç¿’æ¶æ§‹

#### ç¬¬ä¸€å±¤ï¼šDeepLOB (å‡çµï¼Œä¸å­¸ç¿’)

**åŠŸèƒ½**: ç‰¹å¾µæå–å™¨ï¼ˆFeature Extractorï¼‰

**è¨“ç·´ç‹€æ…‹**: å‡çµï¼ˆ`requires_grad=False`ï¼‰

**ä½œç”¨**:
- è¼¸å…¥: LOB æ™‚åºæ•¸æ“š (100 Ã— 20)
- è¼¸å‡º: åƒ¹æ ¼é æ¸¬æ¦‚ç‡ (3ç¶­) æˆ– LSTM éš±è—å±¤ (64ç¶­)
- æä¾›: æ·±å±¤ LOB ç‰¹å¾µè¡¨ç¤º

**ç‚ºä»€éº¼å‡çµï¼Ÿ**
1. **å·²ç¶“è¨“ç·´å¥½**: DeepLOB åœ¨ 560 è¬æ¨£æœ¬ä¸Šè¨“ç·´ï¼Œæº–ç¢ºç‡ 72.98%
2. **é¿å…ç½é›£æ€§éºå¿˜**: å¦‚æœç¹¼çºŒè¨“ç·´ï¼Œå¯èƒ½ç ´å£å·²å­¸ç¿’çš„åƒ¹æ ¼é æ¸¬èƒ½åŠ›
3. **æé«˜æ•ˆç‡**: æ¸›å°‘å¯è¨“ç·´åƒæ•¸ï¼ŒåŠ å¿«è¨“ç·´é€Ÿåº¦

#### ç¬¬äºŒå±¤ï¼šPPO (å­¸ç¿’)

**åŠŸèƒ½**: ç­–ç•¥å­¸ç¿’å™¨ï¼ˆPolicy Learnerï¼‰

**è¨“ç·´ç‹€æ…‹**: å­¸ç¿’ä¸­ï¼ˆ`requires_grad=True`ï¼‰

**å­¸ç¿’å…§å®¹**:
1. **Actor (ç­–ç•¥ç¶²çµ¡)**: å­¸ç¿’æœ€å„ªäº¤æ˜“ç­–ç•¥
   - è¼¸å…¥: DeepLOB ç‰¹å¾µ + äº¤æ˜“ç‹€æ…‹
   - è¼¸å‡º: å‹•ä½œæ¦‚ç‡ P(Buy|s), P(Hold|s), P(Sell|s)
   - ç›®æ¨™: æœ€å¤§åŒ–æœŸæœ›å›å ±

2. **Critic (åƒ¹å€¼ç¶²çµ¡)**: å­¸ç¿’ç‹€æ…‹åƒ¹å€¼è©•ä¼°
   - è¼¸å…¥: DeepLOB ç‰¹å¾µ + äº¤æ˜“ç‹€æ…‹
   - è¼¸å‡º: ç‹€æ…‹åƒ¹å€¼ V(s)
   - ç›®æ¨™: æº–ç¢ºä¼°è¨ˆæœªä¾†å›å ±

---

### 6.2 PPO å­¸ç¿’éç¨‹è©³è§£

#### æ­¥é©Ÿ 1: æ•¸æ“šæ”¶é›†ï¼ˆRolloutï¼‰

**n_steps = 2048**ï¼ˆæ¯ 2048 æ­¥æ›´æ–°ä¸€æ¬¡ç­–ç•¥ï¼‰

```python
for step in range(n_steps):
    # 1. ç­–ç•¥æ¡æ¨£å‹•ä½œ
    action, value, log_prob = policy.predict(observation)

    # 2. åŸ·è¡Œå‹•ä½œ
    next_obs, reward, done, info = env.step(action)

    # 3. å­˜å…¥ Rollout Buffer
    buffer.add(
        obs=observation,
        action=action,
        reward=reward,
        value=value,
        log_prob=log_prob
    )

    observation = next_obs
```

**Rollout Buffer çµæ§‹**:
```
buffer = {
    'observations': (2048, 28),      # è§€æ¸¬åºåˆ—
    'actions': (2048,),              # å‹•ä½œåºåˆ—
    'rewards': (2048,),              # çå‹µåºåˆ—
    'values': (2048,),               # åƒ¹å€¼ä¼°è¨ˆ
    'log_probs': (2048,),            # å‹•ä½œå°æ•¸æ¦‚ç‡
    'dones': (2048,)                 # çµ‚æ­¢æ¨™èªŒ
}
```

---

#### æ­¥é©Ÿ 2: å„ªå‹¢ä¼°è¨ˆï¼ˆAdvantage Calculationï¼‰

**ä½¿ç”¨ GAE (Generalized Advantage Estimation)**:

```python
def compute_gae(rewards, values, gamma=0.99, gae_lambda=0.95):
    """è¨ˆç®— GAE å„ªå‹¢å‡½æ•¸"""
    advantages = np.zeros_like(rewards)
    last_gae = 0

    for t in reversed(range(len(rewards))):
        # TD èª¤å·®
        if t == len(rewards) - 1:
            next_value = 0  # æœ€å¾Œä¸€æ­¥
        else:
            next_value = values[t + 1]

        delta = rewards[t] + gamma * next_value - values[t]

        # GAE
        advantages[t] = last_gae = delta + gamma * gae_lambda * last_gae

    # Returnsï¼ˆç›®æ¨™åƒ¹å€¼ï¼‰
    returns = advantages + values

    # æ¨™æº–åŒ–å„ªå‹¢ï¼ˆæé«˜ç©©å®šæ€§ï¼‰
    advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)

    return advantages, returns
```

**GAE å„ªå‹¢**:
- å¹³è¡¡åå·® (bias) èˆ‡æ–¹å·® (variance)
- `Î»=0`: ä½æ–¹å·®ï¼Œé«˜åå·® (TD(0))
- `Î»=1`: é«˜æ–¹å·®ï¼Œä½åå·® (è’™ç‰¹å¡æ´›)
- `Î»=0.95`: å¹³è¡¡é»ï¼ˆå¸¸ç”¨å€¼ï¼‰

---

#### æ­¥é©Ÿ 3: ç­–ç•¥æ›´æ–°ï¼ˆPolicy Optimizationï¼‰

**n_epochs = 10**ï¼ˆæ¯å€‹ rollout æ›´æ–° 10 æ¬¡ï¼‰
**batch_size = 64**ï¼ˆæ¯æ¬¡æ›´æ–°ä½¿ç”¨ 64 æ¨£æœ¬ï¼‰

```python
for epoch in range(n_epochs):
    # æ‰“äº‚æ•¸æ“š
    indices = np.random.permutation(n_steps)

    # Mini-batch æ›´æ–°
    for start in range(0, n_steps, batch_size):
        end = start + batch_size
        batch_indices = indices[start:end]

        # ç²å– batch æ•¸æ“š
        obs_batch = buffer.observations[batch_indices]
        actions_batch = buffer.actions[batch_indices]
        old_log_probs_batch = buffer.log_probs[batch_indices]
        advantages_batch = buffer.advantages[batch_indices]
        returns_batch = buffer.returns[batch_indices]

        # ===== Forward Pass =====
        # 1. é‡æ–°è©•ä¼°å‹•ä½œ
        new_log_probs, entropy = policy.evaluate_actions(obs_batch, actions_batch)

        # 2. é‡æ–°è©•ä¼°åƒ¹å€¼
        new_values = policy.predict_values(obs_batch)

        # ===== Loss Calculation =====
        # 1. ç­–ç•¥æå¤±ï¼ˆPPO Clipï¼‰
        ratio = torch.exp(new_log_probs - old_log_probs_batch)
        surr1 = ratio * advantages_batch
        surr2 = torch.clamp(ratio, 1 - clip_range, 1 + clip_range) * advantages_batch
        policy_loss = -torch.min(surr1, surr2).mean()

        # 2. åƒ¹å€¼æå¤±ï¼ˆMSEï¼‰
        value_loss = F.mse_loss(new_values, returns_batch)

        # 3. ç†µæå¤±ï¼ˆé¼“å‹µæ¢ç´¢ï¼‰
        entropy_loss = -entropy.mean()

        # 4. ç¸½æå¤±
        loss = policy_loss + vf_coef * value_loss + ent_coef * entropy_loss

        # ===== Backward Pass =====
        optimizer.zero_grad()
        loss.backward()

        # æ¢¯åº¦è£å‰ªï¼ˆé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸ï¼‰
        torch.nn.utils.clip_grad_norm_(policy.parameters(), max_grad_norm)

        optimizer.step()
```

---

#### æ­¥é©Ÿ 4: è©•ä¼°èˆ‡ä¿å­˜

**EvalCallback** (æ¯ 10K steps):

```python
def evaluate_policy(policy, eval_env, n_eval_episodes=10):
    """è©•ä¼°ç­–ç•¥æ€§èƒ½"""
    episode_rewards = []

    for i in range(n_eval_episodes):
        obs, info = eval_env.reset()
        done = False
        episode_reward = 0

        while not done:
            # ç¢ºå®šæ€§ç­–ç•¥ï¼ˆå–æœ€å¤§æ¦‚ç‡å‹•ä½œï¼‰
            action, _states = policy.predict(obs, deterministic=True)
            obs, reward, terminated, truncated, info = eval_env.step(action)
            episode_reward += reward
            done = terminated or truncated

        episode_rewards.append(episode_reward)

    mean_reward = np.mean(episode_rewards)
    std_reward = np.std(episode_rewards)

    # ä¿å­˜æœ€ä½³æ¨¡å‹
    if mean_reward > best_mean_reward:
        best_mean_reward = mean_reward
        policy.save('checkpoints/best_model')

    return mean_reward, std_reward
```

---

### 6.3 æ¢¯åº¦æµå‘åœ–

```
è§€æ¸¬ (28)
    â”‚
    â–¼
DeepLOBExtractor (å‡çµ âŒ)
    â”‚
    â”‚ [æ¢¯åº¦ä¸å›å‚³]
    â”‚
    â”œâ”€ DeepLOB (å‡çµ)
    â””â”€ MLP (å‡çµ)
    â”‚
    â–¼ features (128)
    â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚              â”‚              â”‚
    â–¼              â–¼              â–¼
Actor MLP      Critic MLP     (å¯è¨“ç·´ âœ…)
    â”‚              â”‚
    â”‚ [æ¢¯åº¦å›å‚³]   â”‚ [æ¢¯åº¦å›å‚³]
    â”‚              â”‚
    â–¼              â–¼
Policy Loss    Value Loss
    â”‚              â”‚
    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
    Total Loss
           â”‚
           â–¼
    Backward Pass
           â”‚
           â–¼
    Update Weights (åªæ›´æ–° Actor & Critic)
```

**å¯è¨“ç·´åƒæ•¸çµ±è¨ˆ**:
```
DeepLOB:              ~250K (å‡çµ)
DeepLOBExtractor MLP: ~50K  (å‡çµ)
Actor Network:        ~100K (è¨“ç·´)
Critic Network:       ~100K (è¨“ç·´)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ç¸½åƒæ•¸:               ~500K
å¯è¨“ç·´åƒæ•¸:           ~200K (40%)
```

---

### 6.4 å­¸ç¿’æ›²ç·šç›£æ§

**TensorBoard æŒ‡æ¨™**:

1. **rollout/ep_rew_mean** (Episode å¹³å‡çå‹µ)
   - ä¸Šå‡ â†’ ç­–ç•¥æ”¹é€² âœ…
   - ä¸‹é™ â†’ ç­–ç•¥é€€åŒ– âŒ
   - ç›®æ¨™: æŒçºŒä¸Šå‡è‡³æ”¶æ–‚

2. **train/policy_loss** (ç­–ç•¥æå¤±)
   - åˆæœŸ: è¼ƒé«˜ï¼ˆç­–ç•¥ä¸ç©©å®šï¼‰
   - å¾ŒæœŸ: é™ä½ï¼ˆç­–ç•¥æ”¶æ–‚ï¼‰

3. **train/value_loss** (åƒ¹å€¼æå¤±)
   - æ‡‰é€æ­¥é™ä½ï¼ˆåƒ¹å€¼ä¼°è¨ˆè¶Šä¾†è¶Šæº–ç¢ºï¼‰

4. **train/entropy** (ç­–ç•¥ç†µ)
   - åˆæœŸ: é«˜ï¼ˆæ¢ç´¢å¤šï¼‰
   - å¾ŒæœŸ: é™ä½ï¼ˆé–‹ç™¼å¤šï¼‰
   - éä½: å¯èƒ½é™·å…¥å±€éƒ¨æœ€å„ª

5. **train/clip_fraction** (Clip æ¯”ä¾‹)
   - 0.1-0.3: æ­£å¸¸ï¼ˆé©åº¦æ›´æ–°ï¼‰
   - > 0.5: æ›´æ–°éæ¿€ï¼ˆå¯èƒ½ä¸ç©©å®šï¼‰

6. **eval/mean_reward** (é©—è­‰é›†çå‹µ)
   - èˆ‡è¨“ç·´é›†çå‹µå°æ¯”
   - å·®è·éå¤§ â†’ éæ“¬åˆ

---

## 7. é…ç½®åƒæ•¸è©³è§£

### 7.1 é…ç½®æ–‡ä»¶çµæ§‹

**æ–‡ä»¶**: [configs/sb3_config.yaml](configs/sb3_config.yaml)

```yaml
# ===== ç’°å¢ƒé…ç½® =====
env_config:
  data_dir: 'data/processed_v5/npz'
  max_steps: 500                      # Episode é•·åº¦
  initial_balance: 10000.0
  transaction_cost_rate: 0.001        # 0.1% æ‰‹çºŒè²»
  max_position: 1                     # æœ€å¤§æŒå€‰
  data_mode: 'train'                  # train/val/test
  data_sample_ratio: 0.1              # 10% æ¡æ¨£ï¼ˆæ¸›å°‘è¨˜æ†¶é«”ï¼‰

  deeplob_checkpoint: 'checkpoints/v5/deeplob_v5_best.pth'

  reward_config:
    pnl_weight: 1.0
    cost_weight: 1.0
    inventory_weight: 0.1
    risk_weight: 0.05

# ===== PPO è¶…åƒæ•¸ =====
ppo:
  learning_rate: 0.0003               # 3e-4
  n_steps: 2048                       # Rollout buffer size
  batch_size: 64
  n_epochs: 10
  gamma: 0.99                         # æŠ˜æ‰£å› å­
  gae_lambda: 0.95
  clip_range: 0.2                     # PPO clip [0.8, 1.2]
  ent_coef: 0.01                      # ç†µä¿‚æ•¸
  vf_coef: 0.5                        # åƒ¹å€¼å‡½æ•¸ä¿‚æ•¸
  max_grad_norm: 0.5

  net_arch:
    pi: [256, 128]                    # Actor
    vf: [256, 128]                    # Critic

  verbose: 1
  seed: 42

# ===== DeepLOB ç‰¹å¾µæå–å™¨ =====
deeplob_extractor:
  use_deeplob: true
  features_dim: 128                   # è¼¸å‡ºç‰¹å¾µç¶­åº¦
  use_lstm_hidden: false              # ç°¡å–®æ¨¡å¼
  freeze_deeplob: true                # å‡çµæ¬Šé‡

# ===== è¨“ç·´é…ç½® =====
training:
  total_timesteps: 1000000            # 1M steps
  tensorboard_log: 'logs/sb3_deeplob/'
  checkpoint_dir: 'checkpoints/sb3/ppo_deeplob'
  final_model_name: 'ppo_deeplob_final'
  log_interval: 10

# ===== å›èª¿é…ç½® =====
callbacks:
  checkpoint:
    enabled: true
    save_freq: 50000                  # æ¯ 50K steps
    save_path: 'checkpoints/sb3/ppo_deeplob'
    name_prefix: 'ppo_model'

  eval:
    enabled: true
    eval_freq: 10000                  # æ¯ 10K steps
    n_eval_episodes: 10
    best_model_save_path: 'checkpoints/sb3/ppo_deeplob'
    log_path: 'logs/sb3_eval'
    deterministic: true

# ===== è©•ä¼°ç’°å¢ƒé…ç½® =====
evaluation:
  eval_env_config:
    data_mode: 'val'                  # ä½¿ç”¨é©—è­‰é›†
    data_sample_ratio: 0.1

# ===== æ¸¬è©¦æ¨¡å¼ =====
test_mode:
  total_timesteps: 10000              # å¿«é€Ÿæ¸¬è©¦
  save_freq: 5000
  eval_freq: 5000
  n_steps: 512
  batch_size: 32
```

### 7.2 é—œéµåƒæ•¸èª¿å„ªå»ºè­°

#### PPO è¶…åƒæ•¸

| åƒæ•¸ | é è¨­å€¼ | èª¿å„ªç¯„åœ | å½±éŸ¿ |
|------|--------|---------|------|
| learning_rate | 3e-4 | [1e-4, 1e-3] | å­¸ç¿’é€Ÿåº¦ |
| gamma | 0.99 | [0.95, 0.999] | é•·æœŸ vs çŸ­æœŸå›å ± |
| n_steps | 2048 | [512, 4096] | Rollout é•·åº¦ |
| batch_size | 64 | [32, 256] | æ›´æ–°ç©©å®šæ€§ |
| n_epochs | 10 | [5, 20] | æ•¸æ“šåˆ©ç”¨ç‡ |
| clip_range | 0.2 | [0.1, 0.3] | ç­–ç•¥æ›´æ–°å¹…åº¦ |
| ent_coef | 0.01 | [0.001, 0.05] | æ¢ç´¢ vs é–‹ç™¼ |

**èª¿å„ªç­–ç•¥**:

1. **å­¸ç¿’ç‡** (learning_rate)
   - éé«˜: è¨“ç·´ä¸ç©©å®šï¼Œç­–ç•¥éœ‡ç›ª
   - éä½: è¨“ç·´éæ…¢
   - å»ºè­°: å¾ 3e-4 é–‹å§‹ï¼Œè§€å¯Ÿ policy_loss æ›²ç·š

2. **æŠ˜æ‰£å› å­** (gamma)
   - æ¥è¿‘ 1: é‡è¦–é•·æœŸå›å ±ï¼ˆé©åˆæŒå€‰ç­–ç•¥ï¼‰
   - é é›¢ 1: é‡è¦–çŸ­æœŸå›å ±ï¼ˆé©åˆé«˜é »äº¤æ˜“ï¼‰
   - å»ºè­°: 0.99 (é«˜é ») æˆ– 0.995 (æ³¢æ®µ)

3. **Rollout æ­¥æ•¸** (n_steps)
   - æ›´å¤§: æ›´ç©©å®šï¼Œä½†è¨˜æ†¶é«”æ¶ˆè€—é«˜
   - æ›´å°: æ›´æ–°é »ç¹ï¼Œä½†æ–¹å·®å¤§
   - å»ºè­°: 2048 (å¹³è¡¡é»)

4. **ç†µä¿‚æ•¸** (ent_coef)
   - æ›´å¤§: æ›´å¤šæ¢ç´¢ï¼ˆåˆæœŸï¼‰
   - æ›´å°: æ›´å¤šé–‹ç™¼ï¼ˆå¾ŒæœŸï¼‰
   - å»ºè­°: 0.01 (åˆæœŸ) â†’ 0.001 (å¾ŒæœŸ)ï¼Œå¯ä½¿ç”¨ Schedule

---

## 8. ä½¿ç”¨ç¯„ä¾‹

### 8.1 å¿«é€Ÿæ¸¬è©¦ï¼ˆ10 åˆ†é˜ï¼‰

```bash
conda activate deeplob-pro

# æ¸¬è©¦æµç¨‹ï¼ˆ10K stepsï¼‰
python scripts/train_sb3_deeplob.py \
    --config configs/sb3_config.yaml \
    --deeplob-checkpoint checkpoints/v5/deeplob_v5_best.pth \
    --timesteps 10000 \
    --test \
    --device cuda

# é æœŸè¼¸å‡º
# âœ… DeepLOB æª¢æŸ¥é»é©—è­‰æˆåŠŸ
# âœ… å‰µå»ºå–®ä¸€ç’°å¢ƒ
# âœ… PPO + DeepLOB æ¨¡å‹å‰µå»ºæˆåŠŸ
# ğŸš€ é–‹å§‹è¨“ç·´ (10000 steps)
# [é€²åº¦æ¢]
# âœ… è¨“ç·´å®Œæˆ (ç´„ 10 åˆ†é˜)
```

### 8.2 å®Œæ•´è¨“ç·´ï¼ˆ4-8 å°æ™‚ï¼‰

```bash
# 1M steps å®Œæ•´è¨“ç·´ï¼ˆæ¨è–¦ï¼‰
python scripts/train_sb3_deeplob.py \
    --config configs/sb3_config.yaml \
    --timesteps 1000000 \
    --device cuda

# ç›£æ§è¨“ç·´ï¼ˆå¦ä¸€å€‹çµ‚ç«¯ï¼‰
tensorboard --logdir logs/sb3_deeplob/

# è¨ªå• http://localhost:6006
```

### 8.3 é«˜æ€§èƒ½è¨“ç·´ï¼ˆå¤šç’°å¢ƒä¸¦è¡Œï¼‰

```bash
# 4 å€‹ä¸¦è¡Œç’°å¢ƒï¼ˆåŠ é€Ÿè¨“ç·´ï¼‰
python scripts/train_sb3_deeplob.py \
    --config configs/sb3_config.yaml \
    --timesteps 2000000 \
    --n-envs 4 \
    --vec-type subproc \
    --device cuda

# æ€§èƒ½æå‡
# - å–®ç’°å¢ƒ: ~1500 steps/sec
# - 4 ç’°å¢ƒ: ~4500 steps/sec (3x åŠ é€Ÿ)
```

### 8.4 è©•ä¼°è¨“ç·´æ¨¡å‹

```bash
# è©•ä¼°æœ€ä½³æ¨¡å‹
python scripts/evaluate_sb3.py \
    --model checkpoints/sb3/ppo_deeplob/best_model \
    --n_episodes 20 \
    --save_report

# è¼¸å‡ºå ±å‘Š
# results/evaluation_report_YYYYMMDD_HHMMSS.json
```

---

## 9. å¸¸è¦‹å•é¡Œèˆ‡èª¿è©¦

### 9.1 è¨˜æ†¶é«”ä¸è¶³

**å•é¡Œ**: `CUDA out of memory`

**è§£æ±ºæ–¹æ¡ˆ**:
1. é™ä½æ•¸æ“šæ¡æ¨£æ¯”ä¾‹
   ```yaml
   env_config:
     data_sample_ratio: 0.05  # 5% æ¡æ¨£
   ```

2. æ¸›å°‘ batch_size
   ```yaml
   ppo:
     batch_size: 32  # å¾ 64 é™åˆ° 32
   ```

3. æ¸›å°‘ä¸¦è¡Œç’°å¢ƒ
   ```bash
   --n-envs 1  # å–®ç’°å¢ƒ
   ```

### 9.2 è¨“ç·´ä¸ç©©å®š

**å•é¡Œ**: `ep_rew_mean` åŠ‡çƒˆéœ‡ç›ª

**è§£æ±ºæ–¹æ¡ˆ**:
1. é™ä½å­¸ç¿’ç‡
   ```yaml
   ppo:
     learning_rate: 0.0001  # å¾ 3e-4 é™åˆ° 1e-4
   ```

2. æ¸›å°‘ clip_range
   ```yaml
   ppo:
     clip_range: 0.1  # å¾ 0.2 é™åˆ° 0.1
   ```

3. å¢åŠ  n_steps
   ```yaml
   ppo:
     n_steps: 4096  # å¾ 2048 å¢åŠ åˆ° 4096
   ```

### 9.3 çå‹µå§‹çµ‚ç‚ºè² 

**å•é¡Œ**: ç­–ç•¥ç„¡æ³•ç²å¾—æ­£çå‹µ

**æª¢æŸ¥é …ç›®**:
1. çå‹µå‡½æ•¸æ¬Šé‡
   ```yaml
   reward_config:
     pnl_weight: 1.0      # ç¢ºä¿ PnL æ¬Šé‡è¶³å¤ 
     cost_weight: 0.5     # é™ä½æˆæœ¬æ‡²ç½°
   ```

2. äº¤æ˜“æˆæœ¬ç‡
   ```yaml
   env_config:
     transaction_cost_rate: 0.0005  # é™ä½æ‰‹çºŒè²»
   ```

3. æª¢æŸ¥åƒ¹æ ¼æ•¸æ“š
   ```python
   # é©—è­‰åƒ¹æ ¼è®Šå‹•æ˜¯å¦åˆç†
   prices = env.prices
   returns = np.diff(prices) / prices[:-1]
   print(f"æ—¥æ”¶ç›Šç‡: mean={returns.mean():.4f}, std={returns.std():.4f}")
   ```

### 9.4 GPU åˆ©ç”¨ç‡ä½

**å•é¡Œ**: GPU åˆ©ç”¨ç‡ < 50%

**è§£æ±ºæ–¹æ¡ˆ**:
1. å¢åŠ  batch_size
   ```yaml
   ppo:
     batch_size: 128  # å¾ 64 å¢åŠ åˆ° 128
   ```

2. å¢åŠ ä¸¦è¡Œç’°å¢ƒï¼ˆCPU ä¸¦è¡Œæ¡æ¨£ï¼‰
   ```bash
   --n-envs 8 --vec-type subproc
   ```

3. æ··åˆç²¾åº¦è¨“ç·´ï¼ˆéœ€ä¿®æ”¹ä»£ç¢¼ï¼‰
   ```python
   # åœ¨ create_ppo_deeplob_model() ä¸­æ·»åŠ 
   policy_kwargs['use_amp'] = True  # è‡ªå‹•æ··åˆç²¾åº¦
   ```

### 9.5 DeepLOB è¼‰å…¥å¤±æ•—

**å•é¡Œ**: `DeepLOB æª¢æŸ¥é»è¼‰å…¥å¤±æ•—`

**è§£æ±ºæ–¹æ¡ˆ**:
1. æª¢æŸ¥æª¢æŸ¥é»æ ¼å¼
   ```python
   checkpoint = torch.load('checkpoints/v5/deeplob_v5_best.pth', weights_only=False)
   print(checkpoint.keys())
   # æ‡‰åŒ…å«: model_state_dict, config, epoch, val_acc
   ```

2. ç¢ºèªé…ç½®å®Œæ•´
   ```python
   if 'config' in checkpoint and 'model' in checkpoint['config']:
       print(checkpoint['config']['model'])
   ```

3. ä½¿ç”¨ CPU è¼‰å…¥
   ```python
   checkpoint = torch.load(path, map_location='cpu')
   ```

---

## 10. ç¸½çµ

### 10.1 æ ¸å¿ƒè¦é»

1. **é›™å±¤å­¸ç¿’æ¶æ§‹**
   - DeepLOB (å‡çµ): ç‰¹å¾µæå–
   - PPO (å­¸ç¿’): ç­–ç•¥å„ªåŒ–

2. **ç’°å¢ƒè¨­è¨ˆ**
   - è§€æ¸¬: 28 ç¶­ï¼ˆLOB + DeepLOB + Stateï¼‰
   - å‹•ä½œ: 3 å€‹ï¼ˆBuy/Hold/Sellï¼‰
   - çå‹µ: å¤šçµ„ä»¶ï¼ˆPnL + Cost + Inventory + Riskï¼‰

3. **è¨“ç·´æµç¨‹**
   - Rollout â†’ Advantage â†’ Policy Update â†’ Evaluate

4. **è‡ªå®šç¾©å‡½æ•¸**
   - ç’°å¢ƒå‰µå»º: `create_vec_env`, `create_eval_env`
   - æ¨¡å‹å‰µå»º: `create_ppo_deeplob_model`
   - è¨“ç·´åŸ·è¡Œ: `train_model`

### 10.2 ä¸‹ä¸€æ­¥

1. **å®Œæ•´è¨“ç·´** (1M steps)
2. **è¶…åƒæ•¸å„ªåŒ–** (Optuna)
3. **æ€§èƒ½è©•ä¼°** (Sharpe Ratio, Max Drawdown)
4. **æ¨¡å‹éƒ¨ç½²** (æ¨ç†å„ªåŒ–)

---

## åƒè€ƒè³‡æº

- **Stable-Baselines3 æ–‡æª”**: https://stable-baselines3.readthedocs.io/
- **PPO è«–æ–‡**: Schulman et al., "Proximal Policy Optimization Algorithms" (2017)
- **DeepLOB è«–æ–‡**: Zhang et al., "DeepLOB: Deep Convolutional Neural Networks for Limit Order Books" (2019)
- **å°ˆæ¡ˆæ–‡æª”**: [docs/SB3_IMPLEMENTATION_REPORT.md](docs/SB3_IMPLEMENTATION_REPORT.md)

---

**æ–‡æª”çµæŸ**
