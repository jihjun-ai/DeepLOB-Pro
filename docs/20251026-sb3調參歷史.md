# PPO + DeepLOB 調參歷史 (精簡版)

**配置文件**: `configs/sb3_deeplob_config.yaml`
**目的**: 記錄每次實驗的關鍵數據、問題和解決方案，供快速參考

---

## 實驗對比表

| 實驗 | 訓練步數 | LR | 獎勵機制 | Buy% | 平均獎勵 | 最佳獎勵 | 勝率 | KL | 解釋方差 | 結果 |
|------|---------|-----|----------|------|---------|---------|------|-----|---------|------|
| **PPO_5** | 500K | 3e-4 | PnL-成本 | - | -24.37 | -19.91 | - | 0.130 | -0.524 | ❌ 基線 |
| **PPO_8** | 200K | 1e-4 | Long Only | - | -4.59 | -4.59 | - | 0.018 | -0.332 | ⭐ 首次突破 |
| **PPO_9** | 500K | 1e-4 | vf=1.0 | - | -4.05 | 0.00 | - | 0.014 | 0.019 | 🎯 首次盈利 |
| **PPO_11** | 200K | 1e-4 | +持倉激勵0.05 | 99.4% | +24.15 | +24.15 | - | 0.008 | 0.827 | ❌ 過度交易 |
| **PPO_13** | 200K | 1e-4 | 未平倉-10元 | 2.0% | -2.91 | - | - | 0.004 | 0.966 | ❌ 過度保守 |
| **PPO_14** | 200K | 1e-4 | 未平倉-2元 | 77.8% | -3.18 | - | - | 0.017 | 0.064 | ❌ 過度激進 |
| **PPO_15** | 200K | 1e-4 | 未平倉-5元 | 4.4% | -1.31 | 0.00 | - | 0.010 | 0.123 | ❌ 仍保守 |
| **PPO_16** | 200K | 1e-4 | 未平倉-3元 | **7.3%** | **-0.15** | **+1.18** | **40%** | 0.012 | 0.596 | ✅ 唯一成功 |
| **PPO_18** | 1M | 1e-4 | 未平倉-3元 | 3.4% | -0.51 | +0.58 | 20% | 0.010 | 0.989 | ❌ 收斂至保守 |
| **PPO_19** | 1.5M | 1e-4 | 未平倉-2.5元 | 0% | -2.23 | 0.00 | - | 0.005 | 0.297 | ❌ 完全不交易 |
| **PPO_21** | 3M | 1e-4→3e-5 | 未平倉-3元+LR衰減 | 3.4% | -0.51 | +0.58 | 20% | 0.008 | -1.586 | ❌ 與PPO_18相同 |
| **PPO_23** | 500K | 1e-4 | 動態激勵+方向性 | ? | ? | ? | ? | ? | ? | ⏳ 待測試 |

---

## 關鍵發現

### 1. 固定懲罰困境（PPO_13-21）

**問題**: 所有固定未平倉懲罰值都失敗
- **-10元**: Buy 2% (過度保守)
- **-5元**: Buy 4.4% (仍保守)
- **-3元**: Buy 3.4% (長訓練收斂至保守)
- **-2.5元**: Buy 0% (完全不交易)
- **-2元**: Buy 77.8% (過度激進)

**結論**: 黃金區間極窄且不穩定，固定懲罰方案不可行

### 2. 延長訓練無效（PPO_18/21）

| 實驗 | 步數 | Buy% | 結果 |
|------|------|------|------|
| PPO_16 | 200K | 7.3% | ✅ 最佳 |
| PPO_18 | 1M (5x) | 3.4% | ❌ 倒退 |
| PPO_21 | 3M (15x) | 3.4% | ❌ 無改善 |

**結論**: PPO_16 的 7.3% Buy 是「未收斂狀態」，延長訓練反而收斂至局部最優

### 3. 持倉激勵過強（PPO_11）

- 配置: `holding_bonus = 0.05`
- 結果: Buy 99.4% (幾乎永遠持倉)
- 問題: 激勵過強，模型學到「永遠持倉」策略

### 4. 學習率衰減無效（PPO_21）

- 配置: LR linear 衰減 (1e-4 → 3e-5)
- 結果: 與 PPO_18 完全相同
- 結論: 無法阻止策略收斂至「不交易」局部最優

---

## PPO_23 新方案 🎯

**配置變更**:
```yaml
# 移除未平倉懲罰
unclosed_position_penalty: 0  # -3 → 0

# 動態持倉激勵
holding_bonus_initial: 0.02    # 前100步
holding_bonus_final: 0.005     # 100步後

# DeepLOB 方向性獎勵（新增）⭐⭐⭐⭐⭐
use_directional_reward: true
directional_reward_scale: 0.5
```

**核心理念**: 讓模型勇於交易，從交易中學習
1. **動態激勵**: 前期高激勵 (0.02) 鼓勵探索，後期低激勵 (0.005) 維持交易
2. **方向性獎勵**: 直接教模型"低買高賣"
   - Buy + DeepLOB預測上漲 → +獎勵
   - Buy + DeepLOB預測下跌 → -懲罰
   - Sell + 獲利 → +獎勵

**預期結果**:
- Buy 比例: 15-25% (自然平衡)
- 平均獎勵: +2 到 +5 (穩定盈利)
- 勝率: 60-80% (多數 Episodes 盈利)
- 學習速度: 快速收斂（利用 DeepLOB 72.98% 準確率）

---

## 配置模板（重要參數）

### 穩定訓練配置
```yaml
learning_rate: 1e-4          # PPO_8+ 證明穩定
clip_range: 0.1              # PPO_8+ 證明穩定
vf_coef: 1.5                 # PPO_15+ 改善解釋方差
net_arch: [512, 256]         # PPO_8+ 容量足夠
```

### Long Only 配置
```yaml
max_position: 1              # [0, 1] 只做多
action_space: Discrete(2)    # Hold/Buy (無 Sell)
```

### 獎勵函數演進
```yaml
# PPO_5 (基線)
reward = PnL - 交易成本

# PPO_6-9 (簡化)
reward = PnL - 交易成本
- 移除 inventory_penalty
- 移除 risk_penalty

# PPO_11-21 (固定懲罰，失敗)
reward = PnL - 交易成本 + 未平倉懲罰

# PPO_23 (動態激勵 + 方向性，待測試)
reward = PnL - 交易成本 + 動態持倉激勵 + 方向性獎勵
```

---

## 快速參考

### 成功經驗
- ✅ LR=1e-4, clip=0.1 極度穩定（KL < 0.02）
- ✅ vf_coef=1.5 改善解釋方差
- ✅ Long Only 策略簡化問題
- ✅ PPO_16 (200K, -3元) 唯一達到可盈利狀態

### 失敗教訓
- ❌ 固定懲罰無法找到平衡點
- ❌ 延長訓練會收斂至局部最優
- ❌ 過高激勵導致過度交易
- ❌ 過重懲罰導致不交易

### 下一步
1. 訓練 PPO_23（500K steps，~1小時）
2. 驗證方向性獎勵是否有效
3. 若成功: 延長至 1M-2M steps
4. 若失敗: 調整 directional_reward_scale (0.5 → 0.3 或 0.7)

---

**最後更新**: 2025-10-28
**版本**: v5.0 (精簡版)
