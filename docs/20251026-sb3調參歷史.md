# PPO+ DeepLOB 調參歷史記錄
sb3_deeplob_config.yaml
每次保留數據及成果,簡要說明(其他廢話不用),這是改下次調參參考用

## 格式:

**日期**:
**配置**:
**結果**:
**問題**:
**結論**:

---

## PPO_5 (基線實驗)

**日期**: 2025-10-26 21:27 - 22:22

**配置**:
- learning_rate: 3e-4
- clip_range: 0.2
- vf_coef: 0.5
- net_arch: pi/vf=[256,128]
- extractor_net_arch: [256,128]
- total_timesteps: 500K
- 訓練時長: 54 分鐘 (153.5 steps/秒)

**結果**:
- Episode 獎勵: -29.63 → -24.37 (提升 5.26)
- 最佳獎勵: -19.91 (@102K steps)
- KL 散度: 平均 0.1299，最大 0.3026
- 解釋方差: -0.524 (目標 >0.7)
- Clip fraction: 0.393
- 健康度評分: 60/100

**問題**:
1. 🔴 KL 散度過高 (0.1299 > 0.02)：訓練不穩定，策略更新步長過大
2. 🟡 解釋方差低 (-0.524 < 0.5)：價值函數無法有效擬合真實回報
3. 🟡 獎勵波動大：在 -20 到 -29 間波動，可能陷入局部最優

**結論**:
需要降低學習率和 clip_range 以穩定訓練，增加網絡容量以提升價值函數擬合能力。

---

## PPO_6 (穩定性優化 + Long Only)

**日期**: 2025-10-26

**配置** (調整項):
- learning_rate: 3e-4 → **1e-4** (降低 67%)
- clip_range: 0.2 → **0.1** (降低 50%)
- vf_coef: 0.5 → **0.7** (提高 40%)
- net_arch: [256,128] → **[512,256]** (擴大 2倍)
- extractor_net_arch: [256,128] → **[512,256]** (擴大 2倍)
- total_timesteps: 500K → **200K** (降低 60%，加快調參速度)
- **策略模式**: Long Short → **Long Only** (只做多，不做空)
- **動作空間**: Discrete(3) → **Discrete(2)** (Hold/Sell=0, Buy=1)
- **倉位範圍**: [-1, 1] → **[0, 1]** (不允許負倉位)
- **獎勵函數**: 4組件 → **2組件** (移除庫存懲罰和風險懲罰)
  - inventory_penalty: 0.01 → **0.0** (不懲罰持倉)
  - risk_penalty: 0.005 → **0.0** (簡化獎勵函數)

**策略改變原因**:
1. **Long Only 策略**:
   - 台股現貨市場一般散戶無法做空
   - 簡化動作空間，加速訓練收斂
   - 降低風險（避免做空風險無限）
   - 更符合實際交易場景

2. **獎勵函數簡化**:
   - 移除庫存懲罰：不懲罰長時間持倉，讓策略自由決定
   - 移除風險懲罰：簡化獎勵函數，讓數據驅動學習
   - 避免重複懲罰：虧損時 PnL 已反映，不需額外懲罰
   - 專注核心目標：盈利 - 成本

**目標**:
- KL 散度 < 0.05 (目前 0.1299)
- 解釋方差 > 0.5 (目前 -0.524)
- 獎勵穩定上升，波動 < 2.0 (目前 2.9)
- 動作空間簡化後訓練更穩定

**結果**: (待訓練)

**問題**: (待分析)

**結論**: (待評估)

---

