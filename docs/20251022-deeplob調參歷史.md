# DeepLOB 調參歷史記錄

每次保留數據及成果,簡要說明(其他廢話不用),這是改下次調參參考用

基本目的要達到:

-  Val F1_w ≥ 基線 + 0.02（且不靠單一視窗）

-  Train–Val 差距 ≤ 10–12%

-  Grad Norm：P50≈1–2、P95<4

-  滾動視窗中 ≥70% 達標；波動係數 <0.2

- [ ]（可選）ECE<0.08 或 Brier 改善≥5%

- [ ]（若做回測）含成本收益、回撤、夏普均不劣於基線

## 實驗 1: 大 Batch + 高學習率（失敗）

**日期**: 2025-10-22
**配置**:

- batch_size: 2048
- lr: 0.0008
- weight_decay: 0.0005
- dropout: 0.6
- accumulate_steps: 1

**結果**:
| Epoch | Train Acc | Val Acc | Val Loss | 狀態 |
|-------|-----------|---------|----------|------|
| 1 | 50.14% | 51.38% | 0.89 | ✅ 正常 |
| 2 | 60.89% | 50.03% | 1.23 | ⚠️ 開始過擬合 |
| 3 | 79.19% | 49.82% | 1.76 | 🔥 嚴重過擬合 |
| 4 | 85.55% | 49.54% | 2.03 | 🔥🔥 |
| 5 | 88.21% | 49.24% | 2.28 | 🔥🔥🔥 |

**問題**:

- 學習率過高 → 快速記憶訓練集
- Train/Val Acc 差距達 39%
- Val Loss 暴增 157%

**結論**: ❌ 嚴重過擬合，需降低學習率

---

## 實驗 2: 大 Batch + 低學習率 + 強正則化（仍過擬合）

**日期**: 2025-10-22
**配置**:

- batch_size: 2048
- lr: 0.0002 ⬇️ (降低 75%)
- weight_decay: 0.001 ⬆️ (增加 2 倍)
- dropout: 0.7 ⬆️ (增加)
- warmup_ratio: 0.1 ⬆️
- accumulate_steps: 1

**結果**:
| Epoch | Train Acc | Val Acc | Val Loss | Train/Val 差距 | Grad |
|-------|-----------|---------|----------|---------------|------|
| 1 | 47.43% | 51.22% | 0.91 | +3.79% ✅ | 0.76 |
| 2 | 53.78% | 50.06% | 0.94 | -3.72% ⚠️ | 1.90 |
| 3 | 62.30% | 48.78% | 1.08 | -13.52% 🔥 | 4.48 |
| 4 | 69.58% | 48.87% | 1.30 | -20.71% 🔥🔥 | 5.94 |

**問題**:

- 降低 lr 後仍快速過擬合（Epoch 4 差距 21%）
- Val Loss 持續上升（0.91 → 1.30）
- Grad Norm 快速增長（0.76 → 5.94）

**結論**: ❌ Batch 2048 + 小模型 (hidden 32) 過擬合嚴重，需降 batch

---

## 關鍵發現

### 速度問題

- **4.8 it/s 是 LSTM + 序列 100 的正常速度**
- 嘗試啟用 AMP → 導致梯度 NaN（LSTM + 大 batch 不穩定）
- TF32 已啟用，num_workers=12，無進一步優化空間

### 大 Batch 訓練注意事項

- Batch 2048 需要 **極低學習率** (0.0002 或更低)
- 必須配合**強正則化** (dropout 0.7+, weight_decay 0.001+)
- Linear Scaling Rule (lr × 8) **不適用於小模型**

### 下次調參建議（基於實驗 1+2）

1. **必須降低 batch size** → 1024 或 512
2. **對應調整 lr** → batch 1024 用 0.0001，batch 512 用 0.00005
3. **保持強正則化** → dropout 0.7, weight_decay 0.001
4. **早期停止** → 若 Epoch 3-5 差距 >15% 立即停止

**核心教訓**:

- **Batch 2048 對 DeepLOB (hidden 32) 太大** → 無論 lr 多低都快速過擬合
- 小模型需要**小 batch + 更多雜訊**來泛化

---

## 配置快照

### 實驗 1（過擬合）

```yaml
batch_size: 2048
lr: 0.0008
weight_decay: 0.0005
dropout: 0.6
warmup_ratio: 0.05
```

### 實驗 2（仍過擬合）

```yaml
batch_size: 2048
lr: 0.0002
weight_decay: 0.001
dropout: 0.7
warmup_ratio: 0.1
```

### 實驗 3（Grad Norm 爆炸，失敗）

**日期**: 2025-10-22
**配置**:
- batch_size: 512
- lr: 0.00005
- weight_decay: 0.001
- dropout: 0.7
- warmup_ratio: 0.15
- patience: 3
- grad_clip: 1.0

**結果**:
| Epoch | Train Acc | Val Acc | Val Loss | Grad Norm | 狀態 |
|-------|-----------|---------|----------|-----------|------|
| 1 | 46.90% | 51.02% | 0.914 | 1.35 | ✅ |
| 2 | 50.88% | **51.15%** | 0.895 | 2.27 | ✅ 最佳 |
| 3 | 55.45% | 50.57% | 0.946 | 5.71 | ⚠️ Grad 開始失控 |
| 4 | 61.19% | 49.85% | 1.028 | 9.62 | 🔥 |
| 5 | 66.15% | 49.45% | 1.199 | 11.71 | 🔥🔥 |
| 6 | 70.75% | 49.19% | 1.292 | **12.76** | 🔥🔥🔥 |

**關鍵問題**:
- **Grad Norm 爆炸**: 1.35 → 12.76（即使有 grad_clip=1.0）
- **Val 持續走弱**: Epoch 2 後 Val Acc 從 51.15% 降到 49.19%
- **Val Loss 暴增**: 0.895 → 1.292 (+44%)
- **Train/Val 差距**: Epoch 6 達 21.56%

**根本原因**:
- **lr 5e-5 仍太高**（即使 batch 已降到 512）
- 小模型 (hidden 32) + 中等 batch → 需**極低學習率** (≤2e-5)
- Grad Norm 在 warmup 期間就失控，表示**更新步長過大**

**結論**: ❌ batch 512 不夠小，或 lr 5e-5 太高

---

### 實驗 3b（ChatGPT 建議分析）

**ChatGPT 建議分析**:
| 建議 | 採納 | 理由 |
|------|------|------|
| batch 512 | ✅ | 實驗 1+2 證明 2048 太大 |
| lr 5e-5 | ✅ | 配合小 batch（降 4 倍） |
| dropout 0.7 | ✅ | 實驗 2 有效，保持 |
| weight_decay 1e-3 | ✅ | 實驗 2 有效，保持 |
| patience 3 | ✅ | 快速止損（從 8 降到 3） |
| cosine_warm_restarts | ❌ | 會重置 lr 導致不穩定 |
| label_smoothing 0.05 | ❌ | 與「關閉權重」策略衝突 |

**最終配置**:

```yaml
batch_size: 512         # 降 4 倍（2048→512）
lr: 0.00005            # 降 4 倍（0.0002→0.00005）
weight_decay: 0.001    # 保持
dropout: 0.7           # 保持
warmup_ratio: 0.15     # 增加預熱（小 batch 需更多穩定）
patience: 3            # 快速止損
epochs: 50             # 補償步數（30→50）
grad_clip: 1.0         # 監控避免 >4
label_smoothing: 0.0   # 保持關閉（不混淆目標）
```

**預期目標**:

- Epoch 5: Train/Val Acc 差距 < 10%
- Val Loss 穩定或緩慢下降（不暴增）
- Grad Norm 穩定在 1.0-2.0 範圍

**停止規則**:

- 若 Epoch 3-5 差距 > 15% → 立即停止
- 若 Grad Norm 持續 > 4 → 降 lr 或增 dropout 到 0.75

---

## 實驗 4: 極低學習率（基於實驗 3 Grad Norm 爆炸）

**日期**: 2025-10-22
**配置**:
```yaml
batch_size: 512         # 保持
lr: 0.00002            # 從 5e-5 降到 2e-5（降 60%）
weight_decay: 0.001    # 保持
dropout: 0.7           # 保持
warmup_ratio: 0.20     # 增加（0.15→0.20，給更多穩定期）
patience: 5            # 增加（3→5，極低 lr 收斂慢）
epochs: 100            # 增加（50→100，補償慢收斂）
grad_clip: 1.0         # 保持
```

**調整原因**:
1. **實驗 3 Grad Norm 爆炸** (12.76) → lr 5e-5 太高
2. **Val 在 Epoch 2 達峰後走弱** → 學習率過快跳過最優解
3. **grad_clip 無效** → 問題在更新步長，非梯度本身

**預期目標**:
- Grad Norm 穩定在 < 3.0
- Val Acc 穩定或緩慢上升（不在 Epoch 2 後下降）
- Train/Val 差距 < 15%
- Val Loss 穩定或緩慢下降

**停止規則**:
- Epoch 5-10 Grad Norm 仍 > 4 → 改用方案 B (batch 256)
- Val Loss 連續 5 epochs 上升 → 早停
- Train/Val 差距 > 18% → 立即停止

**結果**: （待補充）
