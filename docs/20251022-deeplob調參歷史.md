# DeepLOB 調參歷史記錄

每次保留數據及成果,簡要說明(其他廢話不用),這是改下次調參參考用

基本目的要達到:

-  Val F1_w ≥ 基線 + 0.02（且不靠單一視窗）

-  Train–Val 差距 ≤ 10–12%

-  Grad Norm：P50≈1–2、P95<4

-  滾動視窗中 ≥70% 達標；波動係數 <0.2

- [ ]（可選）ECE<0.08 或 Brier 改善≥5%

- [ ]（若做回測）含成本收益、回撤、夏普均不劣於基線

## 實驗 1: 大 Batch + 高學習率（失敗）

**日期**: 2025-10-22
**配置**:

- batch_size: 2048
- lr: 0.0008
- weight_decay: 0.0005
- dropout: 0.6
- accumulate_steps: 1

**結果**:
| Epoch | Train Acc | Val Acc | Val Loss | 狀態 |
|-------|-----------|---------|----------|------|
| 1 | 50.14% | 51.38% | 0.89 | ✅ 正常 |
| 2 | 60.89% | 50.03% | 1.23 | ⚠️ 開始過擬合 |
| 3 | 79.19% | 49.82% | 1.76 | 🔥 嚴重過擬合 |
| 4 | 85.55% | 49.54% | 2.03 | 🔥🔥 |
| 5 | 88.21% | 49.24% | 2.28 | 🔥🔥🔥 |

**問題**:

- 學習率過高 → 快速記憶訓練集
- Train/Val Acc 差距達 39%
- Val Loss 暴增 157%

**結論**: ❌ 嚴重過擬合，需降低學習率

---

## 實驗 2: 大 Batch + 低學習率 + 強正則化（仍過擬合）

**日期**: 2025-10-22
**配置**:

- batch_size: 2048
- lr: 0.0002 ⬇️ (降低 75%)
- weight_decay: 0.001 ⬆️ (增加 2 倍)
- dropout: 0.7 ⬆️ (增加)
- warmup_ratio: 0.1 ⬆️
- accumulate_steps: 1

**結果**:
| Epoch | Train Acc | Val Acc | Val Loss | Train/Val 差距 | Grad |
|-------|-----------|---------|----------|---------------|------|
| 1 | 47.43% | 51.22% | 0.91 | +3.79% ✅ | 0.76 |
| 2 | 53.78% | 50.06% | 0.94 | -3.72% ⚠️ | 1.90 |
| 3 | 62.30% | 48.78% | 1.08 | -13.52% 🔥 | 4.48 |
| 4 | 69.58% | 48.87% | 1.30 | -20.71% 🔥🔥 | 5.94 |

**問題**:

- 降低 lr 後仍快速過擬合（Epoch 4 差距 21%）
- Val Loss 持續上升（0.91 → 1.30）
- Grad Norm 快速增長（0.76 → 5.94）

**結論**: ❌ Batch 2048 + 小模型 (hidden 32) 過擬合嚴重，需降 batch

---

## 關鍵發現

### 速度問題

- **4.8 it/s 是 LSTM + 序列 100 的正常速度**
- 嘗試啟用 AMP → 導致梯度 NaN（LSTM + 大 batch 不穩定）
- TF32 已啟用，num_workers=12，無進一步優化空間

### 大 Batch 訓練注意事項

- Batch 2048 需要 **極低學習率** (0.0002 或更低)
- 必須配合**強正則化** (dropout 0.7+, weight_decay 0.001+)
- Linear Scaling Rule (lr × 8) **不適用於小模型**

### 下次調參建議（基於實驗 1+2）

1. **必須降低 batch size** → 1024 或 512
2. **對應調整 lr** → batch 1024 用 0.0001，batch 512 用 0.00005
3. **保持強正則化** → dropout 0.7, weight_decay 0.001
4. **早期停止** → 若 Epoch 3-5 差距 >15% 立即停止

**核心教訓**:

- **Batch 2048 對 DeepLOB (hidden 32) 太大** → 無論 lr 多低都快速過擬合
- 小模型需要**小 batch + 更多雜訊**來泛化

---

## 配置快照

### 實驗 1（過擬合）

```yaml
batch_size: 2048
lr: 0.0008
weight_decay: 0.0005
dropout: 0.6
warmup_ratio: 0.05
```

### 實驗 2（仍過擬合）

```yaml
batch_size: 2048
lr: 0.0002
weight_decay: 0.001
dropout: 0.7
warmup_ratio: 0.1
```

### 實驗 3（Grad Norm 爆炸，失敗）

**日期**: 2025-10-22
**配置**:
- batch_size: 512
- lr: 0.00005
- weight_decay: 0.001
- dropout: 0.7
- warmup_ratio: 0.15
- patience: 3
- grad_clip: 1.0

**結果**:
| Epoch | Train Acc | Val Acc | Val Loss | Grad Norm | 狀態 |
|-------|-----------|---------|----------|-----------|------|
| 1 | 46.90% | 51.02% | 0.914 | 1.35 | ✅ |
| 2 | 50.88% | **51.15%** | 0.895 | 2.27 | ✅ 最佳 |
| 3 | 55.45% | 50.57% | 0.946 | 5.71 | ⚠️ Grad 開始失控 |
| 4 | 61.19% | 49.85% | 1.028 | 9.62 | 🔥 |
| 5 | 66.15% | 49.45% | 1.199 | 11.71 | 🔥🔥 |
| 6 | 70.75% | 49.19% | 1.292 | **12.76** | 🔥🔥🔥 |

**關鍵問題**:
- **Grad Norm 爆炸**: 1.35 → 12.76（即使有 grad_clip=1.0）
- **Val 持續走弱**: Epoch 2 後 Val Acc 從 51.15% 降到 49.19%
- **Val Loss 暴增**: 0.895 → 1.292 (+44%)
- **Train/Val 差距**: Epoch 6 達 21.56%

**根本原因**:
- **lr 5e-5 仍太高**（即使 batch 已降到 512）
- 小模型 (hidden 32) + 中等 batch → 需**極低學習率** (≤2e-5)
- Grad Norm 在 warmup 期間就失控，表示**更新步長過大**

**結論**: ❌ batch 512 不夠小，或 lr 5e-5 太高

---

### 實驗 3b（ChatGPT 建議分析）

**ChatGPT 建議分析**:
| 建議 | 採納 | 理由 |
|------|------|------|
| batch 512 | ✅ | 實驗 1+2 證明 2048 太大 |
| lr 5e-5 | ✅ | 配合小 batch（降 4 倍） |
| dropout 0.7 | ✅ | 實驗 2 有效，保持 |
| weight_decay 1e-3 | ✅ | 實驗 2 有效，保持 |
| patience 3 | ✅ | 快速止損（從 8 降到 3） |
| cosine_warm_restarts | ❌ | 會重置 lr 導致不穩定 |
| label_smoothing 0.05 | ❌ | 與「關閉權重」策略衝突 |

**最終配置**:

```yaml
batch_size: 512         # 降 4 倍（2048→512）
lr: 0.00005            # 降 4 倍（0.0002→0.00005）
weight_decay: 0.001    # 保持
dropout: 0.7           # 保持
warmup_ratio: 0.15     # 增加預熱（小 batch 需更多穩定）
patience: 3            # 快速止損
epochs: 50             # 補償步數（30→50）
grad_clip: 1.0         # 監控避免 >4
label_smoothing: 0.0   # 保持關閉（不混淆目標）
```

**預期目標**:

- Epoch 5: Train/Val Acc 差距 < 10%
- Val Loss 穩定或緩慢下降（不暴增）
- Grad Norm 穩定在 1.0-2.0 範圍

**停止規則**:

- 若 Epoch 3-5 差距 > 15% → 立即停止
- 若 Grad Norm 持續 > 4 → 降 lr 或增 dropout 到 0.75

---

## 實驗 4: 極低學習率（基於實驗 3 Grad Norm 爆炸）

**日期**: 2025-10-22
**配置**:
```yaml
batch_size: 512         # 保持
lr: 0.00002            # 從 5e-5 降到 2e-5（降 60%）
weight_decay: 0.001    # 保持
dropout: 0.7           # 保持
warmup_ratio: 0.20     # 增加（0.15→0.20，給更多穩定期）
patience: 5            # 增加（3→5，極低 lr 收斂慢）
epochs: 100            # 增加（50→100，補償慢收斂）
grad_clip: 1.0         # 保持
```

**調整原因**:
1. **實驗 3 Grad Norm 爆炸** (12.76) → lr 5e-5 太高
2. **Val 在 Epoch 2 達峰後走弱** → 學習率過快跳過最優解
3. **grad_clip 無效** → 問題在更新步長，非梯度本身

**預期目標**:
- Grad Norm 穩定在 < 3.0
- Val Acc 穩定或緩慢上升（不在 Epoch 2 後下降）
- Train/Val 差距 < 15%
- Val Loss 穩定或緩慢下降

**停止規則**:
- Epoch 5-10 Grad Norm 仍 > 4 → 改用方案 B (batch 256)
- Val Loss 連續 5 epochs 上升 → 早停
- Train/Val 差距 > 18% → 立即停止

**結果**:
| Epoch | Train Loss | Val Loss | Train Acc | Val Acc | Val F1 (W) | LR | Grad |
|-------|------------|----------|-----------|---------|------------|-----|------|
| 1 | 0.9850 | 0.9306 | 45.57% | 49.87% | 0.3970 | 1e-6 | 1.35 |
| 2 | 0.9358 | 0.8996 | 48.60% | 51.11% | 0.4579 | 2e-6 | 1.46 |
| 3 | 0.9071 | 0.8858 | 50.29% | 51.37% | 0.4825 | 3e-6 | 1.91 |
| 4 | 0.8782 | **0.8828** | 51.87% | **51.55%** | **0.4793** | 4e-6 | 3.02 |
| 5 | 0.8469 | 0.8917 | 53.66% | 51.24% | 0.4823 | 5e-6 | 4.90 |
| 6 | 0.8102 | 0.9205 | 56.05% | 50.81% | 0.4750 | 6e-6 | **7.42** |

**關鍵發現**:
1. **最佳點在 Epoch 4** (LR=4e-6):
   - Val Loss: **0.8828** (最低)
   - Val Acc: **51.55%** (最高)
   - Grad Norm: 3.02 (可接受，臨界點)

2. **Epoch 5 開始失控** (LR=5e-6):
   - Grad Norm: 4.90 (+62%)
   - Val Loss: 0.8917 (+1.0%)
   - Val Acc: 51.24% (-0.31%)

3. **Epoch 6 完全崩潰** (LR=6e-6):
   - Grad Norm: **7.42** (+146% from Epoch 4)
   - Val Loss: 0.9205 (+4.3%)
   - Val Acc: 50.81% (-1.4%)
   - Train/Val 差距: 5.24% (仍可接受，但趨勢不良)

**根本原因**:
- **LR 太高**：即使 warmup 到 4e-6，cosine 繼續拉高（5e-6→6e-6）導致 Grad Norm 爆炸
- **模型在低 LR 區最佳**：3-4e-6 是甜蜜點，超過 5e-6 就進入不穩定區
- **cosine scheduler 不適合**：會持續升高 LR，應改用**常數 LR** 或更保守的 cosine (eta_min 更接近 lr)

**結論**: ❌ lr 6e-6 太高，需固定在 3-4e-6

---

## 實驗 5: 固定低 LR（基於實驗 4 最佳點）

**日期**: 2025-10-23
**配置**:
```yaml
batch_size: 384         # 從 512 降到 384（抑制 Grad Norm 波動）
lr: 0.000004            # 固定 4e-6（實驗 4 Epoch 4 最佳點）
weight_decay: 0.001     # 保持
dropout: 0.7            # 保持
warmup_ratio: 0.30      # 增加（0.20→0.30，更長穩定期）
patience: 3             # 降低（5→3，快速判斷）
epochs: 30              # 降低（100→30，快速收斂）
grad_clip: 1.0          # 保持
sched: cosine           # 保持 cosine
eta_min: 3.0e-6         # 最小 LR（溫和衰減，從 4e-6 到 3e-6）

# 早停（單一指標）
early_stop:
  metric: "val.f1_macro_weighted"   # 主要指標
  patience: 3
  mode: "max"
```

**調整邏輯**:
1. **固定低 LR**: 4e-6（Epoch 4 最佳點，避免超過 5e-6）
2. **降低 batch**: 512→384（進一步抑制 Grad Norm 波動）
3. **溫和衰減**: eta_min=3e-6（cosine 從 4e-6 緩降到 3e-6，避免停滯）
4. **快速早停**: patience=3（3 epochs 無改善即停，避免過度訓練）
5. **縮短訓練**: 30 epochs（基於低 LR 快速收斂假設）

**關鍵策略**（基於實驗 4 分析）:
- **避免 LR > 5e-6**: 實驗 4 證明超過 5e-6 就 Grad Norm 爆炸
- **利用 cosine 溫和衰減**: 從 4e-6 → 3e-6（30% warmup 後開始衰減）
- **batch 384 提供更多雜訊**: 比 512 更能抑制過擬合
- **早停保護**: 3 epochs 無改善立即停止（Epoch 4 可能已是最佳）

**預期目標**:
- ✅ Grad Norm 穩定在 < 3.5（不超過 Epoch 4 的 3.02 太多）
- ✅ Val Loss 穩定或緩慢下降（不反彈到 > 0.89）
- ✅ Val Acc 維持或超過 51.55%（Epoch 4 水平）
- ✅ Val F1 (Weighted) > 0.48（穩定）
- ✅ Train/Val 差距 < 12%（健康範圍）

**成功標準**（至少滿足 3/5）:
1. Grad Norm P95 < 4.0（全程）
2. Val F1 > 0.48（最佳 epoch）
3. Val Acc > 51.5%（最佳 epoch）
4. Train/Val Acc 差距 < 12%（最終）
5. 早停發生在 Epoch 15 前（證明收斂快）

**失敗判定**（任一觸發立即停止）:
- 🔥 Grad Norm > 5.0 連續 2 epochs → lr 太高，改 3e-6
- 🔥 Val Loss 上升 > 10% from best → 過度訓練，早停失效
- 🔥 Train/Val 差距 > 18% → 嚴重過擬合，增 dropout 到 0.75

**結果**:
| Epoch | Train Loss | Val Loss | Train Acc | Val Acc | Val F1 (W) | Val F1 (U) | LR | Grad |
|-------|------------|----------|-----------|---------|------------|------------|-----|------|
| 1 | 1.0227 | 0.9673 | 43.54% | 46.75% | 0.2190 | 0.2190 | ~1e-6 | 1.73 |
| 2 | 0.9535 | 0.9113 | 47.51% | 51.02% | 0.4514 | 0.4514 | ~1e-6 | 1.52 |
| 3 | 0.9264 | 0.8960 | 49.19% | 51.22% | 0.4724 | 0.4724 | ~2e-6 | 1.78 |
| 4 | 0.9054 | 0.8858 | 50.35% | 51.43% | 0.4713 | 0.4713 | ~2e-6 | 2.20 |
| 5 | 0.8841 | 0.8828 | 51.40% | 51.47% | 0.4611 | 0.4611 | ~3e-6 | 3.06 |
| 6 | 0.8617 | 0.8854 | 52.65% | **51.59%** | 0.4695 | 0.4695 | ~3e-6 | 4.41 |

**關鍵發現**:
1. **Val 性能穩定**:
   - Val Acc 穩定在 51-52% 範圍（未崩潰）
   - Val Loss 控制在 0.88-0.89（比實驗 4 微好）
   - Train/Val 差距僅 1.06%（**健康！遠優於過去實驗**）

2. **Grad Norm 警訊**:
   - Epoch 1-4: 1.52-2.20（健康）
   - Epoch 5: 3.06（開始上升）
   - Epoch 6: **4.41**（接近臨界點，需控制）

3. **F1 微幅波動**:
   - 0.46-0.47 範圍內震盪
   - Epoch 5 下降到 0.4611，Epoch 6 回升到 0.4695
   - 需要 min_delta 避免在噪音中徘徊

**對比實驗 4**:
| 指標 | 實驗 4 Epoch 4 | 實驗 5 Epoch 6 | 評價 |
|------|----------------|----------------|------|
| Val Loss | **0.8828** | 0.8854 | 微差 (+0.3%) |
| Val Acc | 51.55% | **51.59%** | ✅ 持平 |
| Val F1 (W) | **0.4793** | 0.4695 | ⬇️ -2.0% |
| Grad Norm | 3.02 | **4.41** | 🔥 +46% |
| Train/Val Gap | ~3% | **1.06%** | ✅✅ 大幅改善 |

**核心問題診斷**:
- Grad Norm 從 Epoch 5 開始加速（3.06→4.41），但 **Val 指標仍健康**
- **不是過擬合**（Train/Val 差距極小），而是**梯度動態不穩定**
- Cosine scheduler 在 warmup 後仍讓 LR 略有變化，觸發梯度波動

**結論**: ⚠️ Val 性能良好，但 Grad Norm 需控制（4.41 接近警戒線）

---

## 實驗 6: 漸進式穩定（基於實驗 5 梯度波動）

**日期**: 2025-10-23
**配置**:
```yaml
batch_size: 384         # 保持（Train/Val gap 僅 1%，泛化良好）
lr: 0.000004            # 保持 4e-6（實驗 4/5 最佳點）
weight_decay: 0.001     # 保持
dropout: 0.7            # 保持
warmup_ratio: 0.40      # 增加（0.30→0.40，更長穩定期）
patience: 5             # 增加（3→5，配合 min_delta）
epochs: 30              # 保持
grad_clip: 0.8          # 降低（1.0→0.8，更嚴格梯度控制）
sched: cosine           # 保持
eta_min: 3.0e-6         # 保持（溫和衰減到 3e-6）

# 早停（新增 min_delta）
early_stop:
  metric: "val.f1_macro_weighted"
  patience: 5           # 增加耐心（3→5）
  mode: "max"
  min_delta: 0.0008     # 新增！F1 需至少提升 0.08% 才續命
```

**調整邏輯**（基於實驗 5 數據分析）:
1. **更嚴格梯度控制**: grad_clip 1.0→**0.8**（控制 Grad Norm，避免突破 5.0）
2. **延長 warmup**: 0.30→**0.40**（讓 LR 在前 40% 才達到 4e-6，後期更穩）
3. **加入最小改善閾值**: min_delta=**0.0008**（F1 需實質提升 ≥0.08%，避免噪音徘徊）
4. **增加耐心**: patience 3→**5**（配合 min_delta，給模型更多探索空間）
5. **保持 batch 384**: Train/Val gap 僅 1%，證明泛化良好，無需降低

**與 ChatGPT 建議對比**:
| 建議 | ChatGPT | 我的判斷 | 採納 |
|------|---------|----------|------|
| grad_clip | 0.75 | **0.8** | ✅ 採納精神，但稍寬鬆 |
| warmup_ratio | - | **0.40** | ✅ 我自己的觀察 |
| min_delta | 0.001 | **0.0008** | ✅ 稍寬鬆，給更多空間 |
| scheduler | constant | **cosine** | ❌ 保持 cosine（實驗 5 證明可行） |
| batch | 320 (Plan-B) | **384** | ❌ Train/Val gap 健康，無需降 |

**預期目標**:
- ✅ Grad Norm 控制在 < 4.0（全程）
- ✅ Val F1 穩定或超過 0.47（最佳 epoch）
- ✅ Val Acc 維持 > 51.5%
- ✅ Train/Val 差距維持 < 5%（保持實驗 5 的健康狀態）
- ✅ 早停發生在 Epoch 15-20（證明收斂穩定）

**成功標準**（至少滿足 4/5）:
1. Grad Norm P95 < 4.0（全程）
2. Val F1 > 0.47（最佳 epoch）
3. Val Acc > 51.5%（最佳 epoch）
4. Train/Val Acc 差距 < 8%（最終）
5. 早停前無 Grad Norm 爆炸（>6.0）

**失敗判定**（任一觸發立即停止）:
- 🔥 Grad Norm > 6.0 連續 2 epochs → 改用 constant scheduler 或降 batch 到 320
- 🔥 Val Loss 上升 > 15% from best → 過度訓練
- 🔥 Val F1 連續 5 epochs < 0.45 → 策略失效

**結果**: （待訓練後補充）

---

## 配置快照對比（實驗 4 vs 實驗 5 vs 實驗 6）

| 參數 | 實驗 4 | 實驗 5 | 實驗 6 | 主要變化 |
|------|--------|--------|--------|---------|
| batch_size | 512 | 384 | **384** | ➡️ 保持 |
| lr (初始) | 2e-5 (warmup) | 4e-6 | **4e-6** | ➡️ 保持 |
| warmup_ratio | 0.20 | 0.30 | **0.40** | ⬆️ +33% |
| eta_min | 2e-6 | 3e-6 | **3e-6** | ➡️ 保持 |
| epochs | 100 | 30 | **30** | ➡️ 保持 |
| patience | 5 | 3 | **5** | ⬆️ +67% |
| min_delta | - | - | **0.0008** | 🆕 新增 |
| dropout | 0.7 | 0.7 | **0.7** | ➡️ 保持 |
| weight_decay | 0.001 | 0.001 | **0.001** | ➡️ 保持 |
| grad_clip | 1.0 | 1.0 | **0.8** | ⬇️ -20% |

**核心差異**:
- **實驗 4**: warmup 到 2e-5（Epoch 6 Grad Norm 爆炸到 7.42）
- **實驗 5**: 鎖定 4e-6，batch 384（Grad Norm 4.41，Val 穩定但梯度波動）
- **實驗 6**: 延長 warmup + 更嚴 grad_clip + min_delta（控制梯度，濾除噪音）

**實驗 6 的關鍵改進**:
1. **warmup 0.40**: 前 12 epochs (40% × 30) 才達到 4e-6，後期更穩定
2. **grad_clip 0.8**: 比實驗 5 的 1.0 更嚴格，預期 Grad Norm < 4.0
3. **min_delta 0.0008**: F1 需提升 ≥0.08% 才有效，避免在 0.46-0.47 間震盪
4. **patience 5**: 配合 min_delta，給足夠探索空間但不空轉
