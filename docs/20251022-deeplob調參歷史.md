# DeepLOB 調參歷史記錄

每次保留數據及成果,簡要說明(其他廢話不用),這是改下次調參參考用

## 實驗 1: 大 Batch + 高學習率（失敗）

**日期**: 2025-10-22
**配置**:

- batch_size: 2048
- lr: 0.0008
- weight_decay: 0.0005
- dropout: 0.6
- accumulate_steps: 1

**結果**:
| Epoch | Train Acc | Val Acc | Val Loss | 狀態 |
|-------|-----------|---------|----------|------|
| 1 | 50.14% | 51.38% | 0.89 | ✅ 正常 |
| 2 | 60.89% | 50.03% | 1.23 | ⚠️ 開始過擬合 |
| 3 | 79.19% | 49.82% | 1.76 | 🔥 嚴重過擬合 |
| 4 | 85.55% | 49.54% | 2.03 | 🔥🔥 |
| 5 | 88.21% | 49.24% | 2.28 | 🔥🔥🔥 |

**問題**:

- 學習率過高 → 快速記憶訓練集
- Train/Val Acc 差距達 39%
- Val Loss 暴增 157%

**結論**: ❌ 嚴重過擬合，需降低學習率

---

## 實驗 2: 大 Batch + 低學習率 + 強正則化（仍過擬合）

**日期**: 2025-10-22
**配置**:

- batch_size: 2048
- lr: 0.0002 ⬇️ (降低 75%)
- weight_decay: 0.001 ⬆️ (增加 2 倍)
- dropout: 0.7 ⬆️ (增加)
- warmup_ratio: 0.1 ⬆️
- accumulate_steps: 1

**結果**:
| Epoch | Train Acc | Val Acc | Val Loss | Train/Val 差距 | Grad |
|-------|-----------|---------|----------|---------------|------|
| 1 | 47.43% | 51.22% | 0.91 | +3.79% ✅ | 0.76 |
| 2 | 53.78% | 50.06% | 0.94 | -3.72% ⚠️ | 1.90 |
| 3 | 62.30% | 48.78% | 1.08 | -13.52% 🔥 | 4.48 |
| 4 | 69.58% | 48.87% | 1.30 | -20.71% 🔥🔥 | 5.94 |

**問題**:
- 降低 lr 後仍快速過擬合（Epoch 4 差距 21%）
- Val Loss 持續上升（0.91 → 1.30）
- Grad Norm 快速增長（0.76 → 5.94）

**結論**: ❌ Batch 2048 + 小模型 (hidden 32) 過擬合嚴重，需降 batch

---

## 關鍵發現

### 速度問題

- **4.8 it/s 是 LSTM + 序列 100 的正常速度**
- 嘗試啟用 AMP → 導致梯度 NaN（LSTM + 大 batch 不穩定）
- TF32 已啟用，num_workers=12，無進一步優化空間

### 大 Batch 訓練注意事項

- Batch 2048 需要 **極低學習率** (0.0002 或更低)
- 必須配合**強正則化** (dropout 0.7+, weight_decay 0.001+)
- Linear Scaling Rule (lr × 8) **不適用於小模型**

### 下次調參建議（基於實驗 1+2）

1. **必須降低 batch size** → 1024 或 512
2. **對應調整 lr** → batch 1024 用 0.0001，batch 512 用 0.00005
3. **保持強正則化** → dropout 0.7, weight_decay 0.001
4. **早期停止** → 若 Epoch 3-5 差距 >15% 立即停止

**核心教訓**:
- **Batch 2048 對 DeepLOB (hidden 32) 太大** → 無論 lr 多低都快速過擬合
- 小模型需要**小 batch + 更多雜訊**來泛化

---

## 配置快照

### 實驗 1（過擬合）
```yaml
batch_size: 2048
lr: 0.0008
weight_decay: 0.0005
dropout: 0.6
warmup_ratio: 0.05
```

### 實驗 2（仍過擬合）
```yaml
batch_size: 2048
lr: 0.0002
weight_decay: 0.001
dropout: 0.7
warmup_ratio: 0.1
```

### 實驗 3（準備測試）

**ChatGPT 建議分析**:
| 建議 | 採納 | 理由 |
|------|------|------|
| batch 512 | ✅ | 實驗 1+2 證明 2048 太大 |
| lr 5e-5 | ✅ | 配合小 batch（降 4 倍） |
| dropout 0.7 | ✅ | 實驗 2 有效，保持 |
| weight_decay 1e-3 | ✅ | 實驗 2 有效，保持 |
| patience 3 | ✅ | 快速止損（從 8 降到 3） |
| cosine_warm_restarts | ❌ | 會重置 lr 導致不穩定 |
| label_smoothing 0.05 | ❌ | 與「關閉權重」策略衝突 |

**最終配置**:
```yaml
batch_size: 512         # 降 4 倍（2048→512）
lr: 0.00005            # 降 4 倍（0.0002→0.00005）
weight_decay: 0.001    # 保持
dropout: 0.7           # 保持
warmup_ratio: 0.15     # 增加預熱（小 batch 需更多穩定）
patience: 3            # 快速止損
epochs: 50             # 補償步數（30→50）
grad_clip: 1.0         # 監控避免 >4
label_smoothing: 0.0   # 保持關閉（不混淆目標）
```

**預期目標**:
- Epoch 5: Train/Val Acc 差距 < 10%
- Val Loss 穩定或緩慢下降（不暴增）
- Grad Norm 穩定在 1.0-2.0 範圍

**停止規則**:
- 若 Epoch 3-5 差距 > 15% → 立即停止
- 若 Grad Norm 持續 > 4 → 降 lr 或增 dropout 到 0.75
