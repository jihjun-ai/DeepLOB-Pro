# DeepLOB èª¿åƒæ­·å²è¨˜éŒ„

æ¯æ¬¡ä¿ç•™æ•¸æ“šåŠæˆæœ,ç°¡è¦èªªæ˜(å…¶ä»–å»¢è©±ä¸ç”¨),é€™æ˜¯æ”¹ä¸‹æ¬¡èª¿åƒåƒè€ƒç”¨

## å¯¦é©— 1: å¤§ Batch + é«˜å­¸ç¿’ç‡ï¼ˆå¤±æ•—ï¼‰

**æ—¥æœŸ**: 2025-10-22
**é…ç½®**:

- batch_size: 2048
- lr: 0.0008
- weight_decay: 0.0005
- dropout: 0.6
- accumulate_steps: 1

**çµæœ**:
| Epoch | Train Acc | Val Acc | Val Loss | ç‹€æ…‹ |
|-------|-----------|---------|----------|------|
| 1 | 50.14% | 51.38% | 0.89 | âœ… æ­£å¸¸ |
| 2 | 60.89% | 50.03% | 1.23 | âš ï¸ é–‹å§‹éæ“¬åˆ |
| 3 | 79.19% | 49.82% | 1.76 | ğŸ”¥ åš´é‡éæ“¬åˆ |
| 4 | 85.55% | 49.54% | 2.03 | ğŸ”¥ğŸ”¥ |
| 5 | 88.21% | 49.24% | 2.28 | ğŸ”¥ğŸ”¥ğŸ”¥ |

**å•é¡Œ**:

- å­¸ç¿’ç‡éé«˜ â†’ å¿«é€Ÿè¨˜æ†¶è¨“ç·´é›†
- Train/Val Acc å·®è·é” 39%
- Val Loss æš´å¢ 157%

**çµè«–**: âŒ åš´é‡éæ“¬åˆï¼Œéœ€é™ä½å­¸ç¿’ç‡

---

## å¯¦é©— 2: å¤§ Batch + ä½å­¸ç¿’ç‡ + å¼·æ­£å‰‡åŒ–ï¼ˆä»éæ“¬åˆï¼‰

**æ—¥æœŸ**: 2025-10-22
**é…ç½®**:

- batch_size: 2048
- lr: 0.0002 â¬‡ï¸ (é™ä½ 75%)
- weight_decay: 0.001 â¬†ï¸ (å¢åŠ  2 å€)
- dropout: 0.7 â¬†ï¸ (å¢åŠ )
- warmup_ratio: 0.1 â¬†ï¸
- accumulate_steps: 1

**çµæœ**:
| Epoch | Train Acc | Val Acc | Val Loss | Train/Val å·®è· | Grad |
|-------|-----------|---------|----------|---------------|------|
| 1 | 47.43% | 51.22% | 0.91 | +3.79% âœ… | 0.76 |
| 2 | 53.78% | 50.06% | 0.94 | -3.72% âš ï¸ | 1.90 |
| 3 | 62.30% | 48.78% | 1.08 | -13.52% ğŸ”¥ | 4.48 |
| 4 | 69.58% | 48.87% | 1.30 | -20.71% ğŸ”¥ğŸ”¥ | 5.94 |

**å•é¡Œ**:
- é™ä½ lr å¾Œä»å¿«é€Ÿéæ“¬åˆï¼ˆEpoch 4 å·®è· 21%ï¼‰
- Val Loss æŒçºŒä¸Šå‡ï¼ˆ0.91 â†’ 1.30ï¼‰
- Grad Norm å¿«é€Ÿå¢é•·ï¼ˆ0.76 â†’ 5.94ï¼‰

**çµè«–**: âŒ Batch 2048 + å°æ¨¡å‹ (hidden 32) éæ“¬åˆåš´é‡ï¼Œéœ€é™ batch

---

## é—œéµç™¼ç¾

### é€Ÿåº¦å•é¡Œ

- **4.8 it/s æ˜¯ LSTM + åºåˆ— 100 çš„æ­£å¸¸é€Ÿåº¦**
- å˜—è©¦å•Ÿç”¨ AMP â†’ å°è‡´æ¢¯åº¦ NaNï¼ˆLSTM + å¤§ batch ä¸ç©©å®šï¼‰
- TF32 å·²å•Ÿç”¨ï¼Œnum_workers=12ï¼Œç„¡é€²ä¸€æ­¥å„ªåŒ–ç©ºé–“

### å¤§ Batch è¨“ç·´æ³¨æ„äº‹é …

- Batch 2048 éœ€è¦ **æ¥µä½å­¸ç¿’ç‡** (0.0002 æˆ–æ›´ä½)
- å¿…é ˆé…åˆ**å¼·æ­£å‰‡åŒ–** (dropout 0.7+, weight_decay 0.001+)
- Linear Scaling Rule (lr Ã— 8) **ä¸é©ç”¨æ–¼å°æ¨¡å‹**

### ä¸‹æ¬¡èª¿åƒå»ºè­°ï¼ˆåŸºæ–¼å¯¦é©— 1+2ï¼‰

1. **å¿…é ˆé™ä½ batch size** â†’ 1024 æˆ– 512
2. **å°æ‡‰èª¿æ•´ lr** â†’ batch 1024 ç”¨ 0.0001ï¼Œbatch 512 ç”¨ 0.00005
3. **ä¿æŒå¼·æ­£å‰‡åŒ–** â†’ dropout 0.7, weight_decay 0.001
4. **æ—©æœŸåœæ­¢** â†’ è‹¥ Epoch 3-5 å·®è· >15% ç«‹å³åœæ­¢

**æ ¸å¿ƒæ•™è¨“**:
- **Batch 2048 å° DeepLOB (hidden 32) å¤ªå¤§** â†’ ç„¡è«– lr å¤šä½éƒ½å¿«é€Ÿéæ“¬åˆ
- å°æ¨¡å‹éœ€è¦**å° batch + æ›´å¤šé›œè¨Š**ä¾†æ³›åŒ–

---

## é…ç½®å¿«ç…§

### å¯¦é©— 1ï¼ˆéæ“¬åˆï¼‰
```yaml
batch_size: 2048
lr: 0.0008
weight_decay: 0.0005
dropout: 0.6
warmup_ratio: 0.05
```

### å¯¦é©— 2ï¼ˆä»éæ“¬åˆï¼‰
```yaml
batch_size: 2048
lr: 0.0002
weight_decay: 0.001
dropout: 0.7
warmup_ratio: 0.1
```

### å¯¦é©— 3ï¼ˆæº–å‚™æ¸¬è©¦ï¼‰

**ChatGPT å»ºè­°åˆ†æ**:
| å»ºè­° | æ¡ç´ | ç†ç”± |
|------|------|------|
| batch 512 | âœ… | å¯¦é©— 1+2 è­‰æ˜ 2048 å¤ªå¤§ |
| lr 5e-5 | âœ… | é…åˆå° batchï¼ˆé™ 4 å€ï¼‰ |
| dropout 0.7 | âœ… | å¯¦é©— 2 æœ‰æ•ˆï¼Œä¿æŒ |
| weight_decay 1e-3 | âœ… | å¯¦é©— 2 æœ‰æ•ˆï¼Œä¿æŒ |
| patience 3 | âœ… | å¿«é€Ÿæ­¢æï¼ˆå¾ 8 é™åˆ° 3ï¼‰ |
| cosine_warm_restarts | âŒ | æœƒé‡ç½® lr å°è‡´ä¸ç©©å®š |
| label_smoothing 0.05 | âŒ | èˆ‡ã€Œé—œé–‰æ¬Šé‡ã€ç­–ç•¥è¡çª |

**æœ€çµ‚é…ç½®**:
```yaml
batch_size: 512         # é™ 4 å€ï¼ˆ2048â†’512ï¼‰
lr: 0.00005            # é™ 4 å€ï¼ˆ0.0002â†’0.00005ï¼‰
weight_decay: 0.001    # ä¿æŒ
dropout: 0.7           # ä¿æŒ
warmup_ratio: 0.15     # å¢åŠ é ç†±ï¼ˆå° batch éœ€æ›´å¤šç©©å®šï¼‰
patience: 3            # å¿«é€Ÿæ­¢æ
epochs: 50             # è£œå„Ÿæ­¥æ•¸ï¼ˆ30â†’50ï¼‰
grad_clip: 1.0         # ç›£æ§é¿å… >4
label_smoothing: 0.0   # ä¿æŒé—œé–‰ï¼ˆä¸æ··æ·†ç›®æ¨™ï¼‰
```

**é æœŸç›®æ¨™**:
- Epoch 5: Train/Val Acc å·®è· < 10%
- Val Loss ç©©å®šæˆ–ç·©æ…¢ä¸‹é™ï¼ˆä¸æš´å¢ï¼‰
- Grad Norm ç©©å®šåœ¨ 1.0-2.0 ç¯„åœ

**åœæ­¢è¦å‰‡**:
- è‹¥ Epoch 3-5 å·®è· > 15% â†’ ç«‹å³åœæ­¢
- è‹¥ Grad Norm æŒçºŒ > 4 â†’ é™ lr æˆ–å¢ dropout åˆ° 0.75
