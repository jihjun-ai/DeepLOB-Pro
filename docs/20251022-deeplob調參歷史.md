# DeepLOB 調參歷史記錄

每次保留數據及成果,簡要說明(其他廢話不用),這是改下次調參參考用

## 實驗 1: 大 Batch + 高學習率（失敗）

**日期**: 2025-10-22
**配置**:

- batch_size: 2048
- lr: 0.0008
- weight_decay: 0.0005
- dropout: 0.6
- accumulate_steps: 1

**結果**:
| Epoch | Train Acc | Val Acc | Val Loss | 狀態 |
|-------|-----------|---------|----------|------|
| 1 | 50.14% | 51.38% | 0.89 | ✅ 正常 |
| 2 | 60.89% | 50.03% | 1.23 | ⚠️ 開始過擬合 |
| 3 | 79.19% | 49.82% | 1.76 | 🔥 嚴重過擬合 |
| 4 | 85.55% | 49.54% | 2.03 | 🔥🔥 |
| 5 | 88.21% | 49.24% | 2.28 | 🔥🔥🔥 |

**問題**:

- 學習率過高 → 快速記憶訓練集
- Train/Val Acc 差距達 39%
- Val Loss 暴增 157%

**結論**: ❌ 嚴重過擬合，需降低學習率

---

## 實驗 2: 大 Batch + 低學習率 + 強正則化（進行中）

**日期**: 2025-10-22
**配置**:

- batch_size: 2048
- lr: 0.0002 ⬇️ (降低 75%)
- weight_decay: 0.001 ⬆️ (增加 2 倍)
- dropout: 0.7 ⬆️ (增加)
- warmup_ratio: 0.1 ⬆️
- accumulate_steps: 1

**調整原因**:

1. 降低學習率 (0.0008 → 0.0002) - 避免過快收斂
2. 增加 weight_decay (0.0005 → 0.001) - 增強 L2 正則化
3. 增加 dropout (0.6 → 0.7) - 抑制過擬合
4. 增加 warmup (0.05 → 0.1) - 穩定訓練初期

**預期目標**:

- Epoch 5 Train Acc < 70%
- Train/Val Acc 差距 < 15%
- Val Loss 穩定或緩慢下降

**結果**: （待補充）

---

## 關鍵發現

### 速度問題

- **4.8 it/s 是 LSTM + 序列 100 的正常速度**
- 嘗試啟用 AMP → 導致梯度 NaN（LSTM + 大 batch 不穩定）
- TF32 已啟用，num_workers=12，無進一步優化空間

### 大 Batch 訓練注意事項

- Batch 2048 需要 **極低學習率** (0.0002 或更低)
- 必須配合**強正則化** (dropout 0.7+, weight_decay 0.001+)
- Linear Scaling Rule (lr × 8) **不適用於小模型**

### 下次調參建議

1. 如果實驗 2 仍過擬合 → 降低 batch 到 1024，lr 到 0.0001
2. 如果訓練太慢 → 考慮降低 batch 換取更快 it/s
3. 監控 Epoch 1-5 的 Train/Val 差距，早期發現過擬合

---

## 配置快照

### 實驗 1（過擬合）

```yaml
batch_size: 2048
lr: 0.0008
weight_decay: 0.0005
dropout: 0.6
warmup_ratio: 0.05
```

### 實驗 2（當前）

```yaml
batch_size: 2048
lr: 0.0002
weight_decay: 0.001
dropout: 0.7
warmup_ratio: 0.1
```
