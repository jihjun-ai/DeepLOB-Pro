# AI 分析提示詞模板

## 使用說明

1. 運行 TensorBoard 分析工具生成 JSON 報告
2. 複製下方提示詞模板
3. 將 `[JSON 報告內容]` 替換為實際的 JSON 內容
4. 提供給 AI（Claude、GPT-4 等）進行分析

---

## 提示詞模板 1：全面分析

```
我正在訓練一個強化學習模型（PPO + DeepLOB）用於台股高頻交易。
以下是訓練日誌的自動分析結果（JSON 格式）：

```json
[貼上 analysis.json 的完整內容]
```

請幫我進行全面分析：

1. **訓練狀況評估**
   - 整體訓練質量如何？（基於健康度評分、獎勵趨勢、穩定性指標）
   - 有哪些做得好的地方？
   - 有哪些需要改進的地方？

2. **問題診斷**
   - 報告中提到的問題（issues）和警告（warnings）的根本原因是什麼？
   - 為什麼會出現這些問題？
   - 這些問題的嚴重程度如何？

3. **超參數優化建議**
   - 基於當前狀況，應該調整哪些超參數？
   - 具體應該調整到什麼數值？（請給出具體的 YAML 配置）
   - 調整的優先級如何？

4. **下一步行動計劃**
   - 立即要做的事情（高優先級）
   - 中期要做的事情（中優先級）
   - 長期要做的事情（低優先級）

5. **預期改進**
   - 如果按照建議調整，預期能達到什麼效果？
   - Sharpe Ratio 能提升多少？
   - 還需要訓練多長時間？

請給出詳細、具體、可執行的建議。
```

---

## 提示詞模板 2：問題診斷

```
我的強化學習訓練遇到了問題，以下是自動分析報告：

```json
[貼上 analysis.json 的完整內容]
```

**我的困惑**:
- [描述您遇到的具體問題，例如：]
- 為什麼獎勵提升這麼慢？
- KL 散度為什麼這麼高？
- 解釋方差為什麼這麼低？

請幫我：
1. 診斷問題的根本原因
2. 解釋為什麼會出現這個現象
3. 提供具體的解決方案（包括配置修改）
4. 說明解決方案的原理
```

---

## 提示詞模板 3：實驗對比

```
我訓練了 3 個不同配置的模型，以下是對比分析結果：

```json
[貼上 experiment_comparison.json 的完整內容]
```

請幫我：
1. 分析每個實驗的優缺點
2. 解釋為什麼某個配置表現最好
3. 基於這 3 個實驗，給出下一輪實驗的建議配置
4. 預測哪些參數調整可能帶來更大提升
```

---

## 提示詞模板 4：超參數調優

```
我需要優化我的強化學習模型，當前訓練狀況如下：

```json
[貼上 analysis.json 的完整內容]
```

**當前配置**:
```yaml
env_config:
  reward:
    pnl_scale: 1.0
    cost_penalty: 1.0
    inventory_penalty: 0.01
    risk_penalty: 0.005

ppo:
  learning_rate: 0.0003
  n_steps: 2048
  batch_size: 64
  n_epochs: 10
  gamma: 0.99
  clip_range: 0.2
  ent_coef: 0.01
  lstm_hidden_size: 256
```

**目標**:
- Sharpe Ratio > 2.0
- 最大回撤 < 10%
- 勝率 > 55%

請給出：
1. 3-5 組推薦的超參數配置（YAML 格式）
2. 每組配置的預期效果
3. 實驗優先級排序
4. 每個實驗應該訓練多少步數
```

---

## 提示詞模板 5：階段評估

```
我正在進行分階段訓練，目前完成了階段二（基準測試），以下是結果：

```json
[貼上 analysis.json 的完整內容]
```

**訓練計劃**:
- 階段一：環境驗證 ✅
- 階段二：基準測試 ✅ (當前)
- 階段三：超參數優化 ⏳
- 階段四：長期訓練 ⏳
- 階段五：評估與回測 ⏳
- 階段六：部署準備 ⏳

請幫我：
1. 評估階段二的完成質量
2. 判斷是否可以進入階段三
3. 如果可以，給出階段三的詳細實驗計劃
4. 如果不可以，說明需要改進什麼
```

---

## 提示詞模板 6：性能瓶頸分析

```
我的訓練速度較慢，以下是分析報告：

```json
[貼上 analysis.json 的完整內容]
```

**系統信息**:
- GPU: RTX 5090 (32GB)
- CPU: [您的 CPU 型號]
- RAM: [您的記憶體大小]

請幫我：
1. 分析當前訓練速度（steps_per_second）是否正常
2. 找出性能瓶頸在哪裡
3. 給出提升訓練速度的建議
4. 預測優化後的訓練時間
```

---

## 示例：完整的 AI 分析對話

### 用戶輸入

```
我正在訓練一個強化學習模型（PPO + DeepLOB）用於台股高頻交易。
以下是訓練日誌的自動分析結果：

```json
{
  "metadata": {
    "total_steps": 100000,
    "duration_hours": 1.25,
    "steps_per_second": 22222
  },
  "training_progress": {
    "episode_reward": {
      "initial": -123.45,
      "final": 456.78,
      "improvement": 580.23,
      "trend": {
        "slope": 0.058,
        "direction": "increasing",
        "r_squared": 0.89
      }
    }
  },
  "performance_metrics": {
    "total_loss": {
      "initial": 45.67,
      "final": 12.34,
      "converged": true
    },
    "explained_variance": {
      "final": 0.82,
      "mean": 0.78,
      "is_good": true
    }
  },
  "stability_metrics": {
    "kl_divergence": {
      "mean": 0.015,
      "is_stable": true
    },
    "reward_stability": {
      "recent_std": 50.2,
      "coefficient_of_variation": 0.11
    }
  },
  "diagnostic": {
    "health_score": 85.0,
    "issues": [],
    "warnings": [],
    "suggestions": []
  },
  "recommendations": [
    {
      "category": "訓練時長",
      "priority": "low",
      "suggestion": "建議至少訓練 500K+ steps"
    }
  ]
}
```

請給出全面分析和優化建議。
```

### AI 預期回答（示例）

```
## 訓練狀況全面分析

### 1. 整體評估 ✅

**健康度評分: 85/100** - 訓練狀況良好

**優點**:
- ✅ 獎勵持續上升（+580.23），趨勢穩定（R²=0.89）
- ✅ 損失已收斂（45.67 → 12.34）
- ✅ 解釋方差良好（0.82），價值函數擬合優秀
- ✅ KL散度穩定（0.015 < 0.02）
- ✅ 無嚴重問題或警告

**需要改進**:
- ⚠️ 訓練步數偏少（100K），建議至少 500K
- ⚠️ 獎勵變異係數 0.11 略高，可以更穩定

### 2. 問題診斷

當前沒有嚴重問題，但有改進空間：

**獎勵變異性稍高**:
- 原因：可能是環境隨機性或策略探索
- 影響：不影響訓練，但實盤可能波動較大
- 建議：降低熵係數或增加訓練步數

### 3. 超參數優化建議

**當前配置已經相當不錯**，但可以嘗試以下微調：

#### 方案 1：延長訓練（推薦）⭐⭐⭐⭐⭐

```yaml
# 無需改配置，直接延長訓練時間
training:
  total_timesteps: 500000  # 從 100K 增加到 500K
```

**預期效果**:
- Sharpe Ratio: 1.5 → 2.0+
- 獎勵穩定性提升
- 策略更成熟

#### 方案 2：微調獎勵權重（可選）

```yaml
env_config:
  reward:
    pnl_scale: 1.2           # 從 1.0 增加（稍微激進）
    cost_penalty: 1.0        # 保持不變
    inventory_penalty: 0.015 # 從 0.01 增加（更快平倉）
    risk_penalty: 0.005      # 保持不變
```

**預期效果**:
- 可能提升 10-15% 收益
- 降低持倉時間
- 風險略增

#### 方案 3：降低探索（穩定性優先）

```yaml
ppo:
  ent_coef: 0.005  # 從 0.01 降低
  clip_range: 0.15 # 從 0.2 降低
```

**預期效果**:
- 降低獎勵波動
- 策略更保守
- 適合實盤

### 4. 下一步行動計劃

#### 🔴 高優先級（立即執行）

1. **延長訓練到 500K steps**
   ```bash
   python scripts/train_sb3_deeplob.py --timesteps 500000
   ```
   - 預計時間：6-7 小時
   - 預期 Sharpe Ratio > 2.0

2. **定期分析進度**（每 100K steps）
   ```bash
   python scripts/analyze_tensorboard.py --logdir logs/sb3_deeplob/PPO_1
   ```

#### 🟡 中優先級（本週完成）

3. **實驗獎勵權重**（如方案 2）
   - 創建新配置
   - 訓練 100K steps
   - 對比效果

4. **評估測試集性能**
   ```bash
   python scripts/evaluate_sb3.py --model checkpoints/sb3/best_model
   ```

#### 🟢 低優先級（下週）

5. **嘗試更大的網絡**
   ```yaml
   ppo.policy_kwargs.lstm_hidden_size: 384  # 從 256 增加
   ```

6. **回測系統整合**

### 5. 預期改進

**如果延長到 500K steps（方案 1）**:
- Sharpe Ratio: 1.5 → 2.0-2.5 ✅
- 最大回撤: 預計 < 12%
- 勝率: 預計 55-60%
- 訓練時間: 額外 5-6 小時

**如果結合方案 2（微調獎勵）**:
- Sharpe Ratio: 可能達到 2.5-3.0 ✅
- 收益提升 10-20%
- 但需要更多實驗驗證

### 總結

**您的訓練非常成功！** 當前配置已經很好，主要問題是訓練時間不夠。

**推薦策略**: 先延長訓練到 500K steps，然後評估是否需要微調超參數。

**預計時間線**:
- 今天：延長訓練（6-7 小時）
- 明天：評估結果，決定是否實驗
- 本週：完成超參數優化
- 下週：進入長期訓練階段（1M steps）
```

---

## 使用技巧

### 技巧 1：提供上下文

除了 JSON 報告，還可以提供：
- 當前配置文件（YAML）
- 訓練目標（Sharpe Ratio, 回撤等）
- 遇到的具體問題
- 硬體環境

### 技巧 2：分步提問

如果報告很長，可以分步提問：
1. 先問整體評估
2. 再問具體問題診斷
3. 最後問優化建議

### 技巧 3：要求具體配置

明確要求 AI 提供：
- YAML 格式的配置
- 具體的數值
- 可執行的命令

### 技巧 4：多輪對話

AI 給出建議後，可以繼續追問：
- "為什麼要這樣調整？"
- "如果我這樣改會怎樣？"
- "還有其他方案嗎？"

---

**文件版本**: v1.0
**最後更新**: 2025-10-26
**相關文檔**:
- [TENSORBOARD_ANALYSIS_GUIDE.md](TENSORBOARD_ANALYSIS_GUIDE.md) - 分析工具使用指南
- [TRAIN_SB3_DEEPLOB_GUIDE.md](TRAIN_SB3_DEEPLOB_GUIDE.md) - 訓練調教指南
