# PPO_15 訓練計劃 - 精細調整未平倉懲罰

**日期**: 2025-10-27
**版本**: v15.0
**策略**: Long Only + 中間懲罰值 + 加強價值函數

---

## 🎯 目標

解決 PPO_14 過度激進問題（Buy 比例 77.8%，解釋方差崩潰至 0.064），找到懲罰強度的最佳平衡點。

---

## 📊 PPO_14 問題診斷

### 訓練指標 (部分良好)

- ✅ **KL 散度**: 0.0169 (穩定)
- ✅ **趨勢 R²**: 0.914 (強上升)
- ❌ **解釋方差**: 0.064 (崩潰！PPO_13: 0.966 → PPO_14: 0.064)
- ⚠️ **Episode 獎勵**: -3.83 (仍為負)

### 實際交易行為 (過度激進)

- ❌ **Buy 比例**: 77.8% (目標: 15%-40%)
- ❌ **Hold 比例**: 22.2% (過少)
- ⚠️ **交易次數**: 15.2/Episode (過於頻繁)
- ❌ **Episode 獎勵**: -3.18 ± 1.69 (仍虧損)

### 根本原因

**懲罰調整過度**: `-10 → -2` (降低 80%)

**懲罰強度梯度**:
```
-10 元/倉位 → Buy 2.0%   (過度保守)
 -2 元/倉位 → Buy 77.8%  (過度激進) ← PPO_14
 -5 元/倉位 → Buy ???%   (預期 20-30%) ← PPO_15 目標
```

**解釋方差崩潰分析**:
- PPO_13: 0.966 (價值函數準確評估狀態價值)
- PPO_14: 0.064 (價值函數失去判斷能力)
- **原因**: 獎勵信號劇變 (-10 → -2)，價值函數無法適應

---

## 🔧 PPO_15 調整方案

### 核心修改 1: 中間懲罰值

**調整未平倉懲罰**: `-2 元 → -5 元/倉位` (調升 150%)

```python
# tw_lob_trading_env.py:380
unclosed_penalty = -5.0 * self.position
```

**理由**:
1. **二分法尋找平衡點**:
   - -10: 太重 → 2% Buy
   - -2: 太輕 → 77.8% Buy
   - **-5: 中間值** → 預期 20-30% Buy

2. **對稱調整**:
   - PPO_13 → PPO_14: 降低 80% (-10 → -2)
   - PPO_14 → PPO_15: 調升 150% (-2 → -5)
   - 接近中點，符合二分搜索邏輯

### 核心修改 2: 加強價值函數

**增加 vf_coef**: `1.0 → 1.5` (提升 50%)

```yaml
# configs/sb3_deeplob_config.yaml
ppo:
  vf_coef: 1.5  # PPO_15: 1.5 | PPO_14: 1.0
```

**理由**:
1. **修復解釋方差崩潰**:
   - PPO_14: vf_coef=1.0 → 解釋方差 0.064
   - 增加 vf_coef → 加強價值函數訓練
   - 目標: 解釋方差 > 0.5

2. **平衡 Policy 與 Value**:
   - vf_coef 控制價值函數在總損失中的權重
   - 提升 vf_coef → 價值函數更新更頻繁
   - 幫助適應新的獎勵分佈

---

## 🎯 訓練目標

| 指標 | PPO_14 (當前) | PPO_15 (目標) | 評估標準 |
|------|--------------|--------------|----------|
| **Buy 比例** | 77.8% ❌ | 20%-40% | 適度交易 |
| **Episode 獎勵** | -3.18 ❌ | > 0 | 盈利 |
| **解釋方差** | 0.064 ❌ | > 0.5 | 價值函數恢復 |
| **交易次數** | 15.2/Ep ⚠️ | 5-10/Ep | 合理頻率 |
| **KL 散度** | 0.0169 ✅ | < 0.02 | 保持穩定 |

---

## 📋 訓練配置

### 超參數變更

```yaml
# PPO_15 修改項
ppo:
  vf_coef: 1.5          # ↑ 提升 50% (1.0 → 1.5)

env:
  unclosed_penalty: -5  # ↑ 調升 150% (-2 → -5)

# 保持不變
ppo:
  learning_rate: 1e-4   # ✅ 穩定
  clip_range: 0.1       # ✅ 穩定
  net_arch: [512, 256]  # ✅ 容量充足

training:
  total_timesteps: 200K # 快速驗證
```

### 獎勵函數

```python
# reward_shaper.py (無變化)
總獎勵 = PnL - 交易成本

# tw_lob_trading_env.py (修改)
if Episode 結束 and 有持倉:
    總獎勵 -= 5.0 * position  # PPO_15: -5 | PPO_14: -2 | PPO_13: -10
```

---

## 🚀 執行步驟

### 1. 驗證修改

```bash
# 檢查環境代碼
type src\envs\tw_lob_trading_env.py | findstr "unclosed_penalty"
# 應顯示: unclosed_penalty = -5.0 * self.position

# 檢查配置文件
type configs\sb3_deeplob_config.yaml | findstr "vf_coef"
# 應顯示: vf_coef: 1.5

# 檢查訓練步數
type configs\sb3_deeplob_config.yaml | findstr "total_timesteps"
# 應顯示: total_timesteps: 200000
```

### 2. 開始訓練 (200K steps, ~20 分鐘)

```bash
# 激活環境
conda activate deeplob-pro

# 開始訓練
python scripts/train_sb3_deeplob.py --config configs/sb3_deeplob_config.yaml

# 監控訓練
tensorboard --logdir logs/sb3_deeplob/
```

### 3. 訓練後驗證

```bash
# 3.1 分析 TensorBoard 日誌
python scripts/analyze_tensorboard.py --log-dir logs/sb3_deeplob/PPO_15

# 3.2 檢查交易行為（最重要！）
python scripts/check_trading_behavior.py --model checkpoints/sb3/ppo_deeplob/best_model --n_episodes 5

# 3.3 評估模型性能
python scripts/evaluate_sb3.py --model checkpoints/sb3/ppo_deeplob/best_model --n_episodes 20
```

---

## 📈 預期結果

### 樂觀預期 (⭐⭐⭐⭐⭐)

- Buy 比例: **25%-35%** (理想範圍)
- Episode 獎勵: **+3~+10** (實際盈利)
- 解釋方差: **> 0.7** (價值函數恢復)
- 交易次數: **6-8/Ep** (合理頻率)

### 保守預期 (⭐⭐⭐)

- Buy 比例: **15%-50%** (接近目標)
- Episode 獎勵: **-1~+3** (接近盈虧平衡)
- 解釋方差: **> 0.4** (改善但未完全恢復)
- 需微調: -5 → -4 或 -6

### 失敗情況 (❌)

- **情況 A**: Buy 比例 < 5% (仍過度保守)
  → 懲罰仍過重，調至 -3 或 -4

- **情況 B**: Buy 比例 > 60% (仍過度激進)
  → 懲罰仍過輕，調至 -7 或 -8

- **情況 C**: 解釋方差 < 0.3 (價值函數未恢復)
  → 增加 vf_coef 至 2.0

---

## 🔄 備選方案 (如 PPO_15 失敗)

### 方案 A: 繼續二分搜索 ⭐⭐⭐⭐⭐

```
如果 PPO_15 仍過度激進 (Buy > 50%):
  PPO_16: -5 → -7 或 -8

如果 PPO_15 仍過度保守 (Buy < 10%):
  PPO_16: -5 → -3 或 -4

如果 PPO_15 接近目標 (Buy 15-40%):
  延長訓練至 1M steps + LR 衰減
```

### 方案 B: ChatGPT 動態獎勵方案 ⭐⭐⭐⭐

```python
# 持倉激勵衰減
holding_bonus: 0.02 → 0.005 (線性衰減，50K steps)

# 交易冷卻懲罰
cooldown_penalty: -0.005/步 (連續 2 步內交易)

# 未平倉懲罰（保持）
unclosed_penalty: -5/倉位
```

### 方案 C: 回到 PPO_11 + 微調 ⭐⭐⭐

```python
# 移除未平倉懲罰
unclosed_penalty = 0

# 降低持倉激勵
holding_bonus = +0.01 * position  # 降低 80% (0.05 → 0.01)
```

---

## 📊 四代演進對比

| 指標 | PPO_11 | PPO_13 | PPO_14 | **PPO_15** |
|------|--------|--------|--------|----------|
| **策略** | 持倉激勵 | 重懲罰 | 輕懲罰 | **中懲罰+強VF** |
| **懲罰值** | 0 (+0.05) | -10 | -2 | **-5** |
| **vf_coef** | 1.0 | 1.0 | 1.0 | **1.5** |
| **Buy 比例** | 99.4% 🔴 | 2.0% 🔴 | 77.8% ⚠️ | **20-30%?** 🎯 |
| **Episode 獎勵** | +24.15 ✅ | -2.91 ❌ | -3.18 ❌ | **> 0?** 🎯 |
| **解釋方差** | 0.827 ✅ | 0.966 ✅ | 0.064 ❌ | **> 0.5?** 🎯 |
| **交易次數** | 3.8/Ep | 5.8/Ep | 15.2/Ep ⚠️ | **5-10?** 🎯 |

---

## 📝 成功標準

### 必須達成 (Basic)

- [ ] Buy 比例: 10%-50% (避免極端)
- [ ] Episode 獎勵: > -2 (接近盈利)
- [ ] 解釋方差: > 0.3 (價值函數改善)
- [ ] KL 散度: < 0.02 (訓練穩定)

### 理想目標 (Target)

- [ ] Buy 比例: 20%-40% (適度交易)
- [ ] Episode 獎勵: > 0 (實際盈利)
- [ ] 解釋方差: > 0.5 (價值函數良好)
- [ ] 交易次數: 5-10/Ep (合理頻率)

### 卓越表現 (Stretch)

- [ ] Buy 比例: 25%-35% (最佳平衡)
- [ ] Episode 獎勵: > +5 (顯著盈利)
- [ ] 解釋方差: > 0.8 (價值函數優秀)
- [ ] 勝率: > 55% (多數交易盈利)

---

## 💡 核心洞察

### PPO_14 失敗的教訓

1. **80% 調整過於激進**:
   - -10 → -2 是 5 倍變化
   - 應採用二分法（25-50% 調整）

2. **解釋方差是關鍵指標**:
   - PPO_13: 0.966 (優秀) + 懲罰 -10 → Buy 2%
   - PPO_14: 0.064 (崩潰) + 懲罰 -2 → Buy 77.8%
   - **價值函數失效 → 策略失控**

3. **vf_coef 需要同步調整**:
   - 獎勵分佈劇變時，價值函數難以適應
   - 增加 vf_coef 加速價值函數收斂

### PPO_15 設計理念

1. **二分法尋找平衡**:
   - -10 和 -2 的中點是 -6
   - 選擇 -5 (略偏向激進，但安全)

2. **加強價值函數**:
   - vf_coef 1.0 → 1.5
   - 幫助價值函數適應新獎勵

3. **快速迭代驗證**:
   - 200K steps (~20 分鐘)
   - 根據結果決定 -3/-4/-6/-7

---

## 🔍 訓練中監控重點

### 關鍵指標 (TensorBoard)

```
必看指標:
1. rollout/ep_rew_mean (Episode 獎勵趨勢)
2. train/explained_variance (價值函數恢復情況)
3. train/approx_kl (穩定性)

次要指標:
4. train/policy_loss
5. train/value_loss
6. train/clip_fraction
```

### 行為監控 (check_trading_behavior.py)

```
必檢項目:
1. Buy 比例 (目標: 20-40%)
2. Episode 獎勵 (目標: > 0)
3. 交易次數 (目標: 5-10/Ep)
```

---

## 📚 參考資料

### 專案文檔

- [調參歷史](20251026-sb3調參歷史.md) - PPO_11/13/14 完整記錄
- [PPO_14 計劃](PPO_14_PLAN.md) - 前次實驗
- [ChatGPT 分析評估](CHATGPT_ANALYSIS_REVIEW.md) - 專業建議

### 訓練結果

- PPO_11: Buy 99.4%, 獎勵 +24.15 (持倉激勵過強)
- PPO_13: Buy 2.0%, 獎勵 -2.91 (懲罰過重)
- PPO_14: Buy 77.8%, 獎勵 -3.18, 解釋方差 0.064 (懲罰過輕)
- **PPO_15**: ??? (待驗證)

---

## ⏱️ 時間預估

| 步驟 | 時間 | 說明 |
|------|------|------|
| 訓練 | ~20 分鐘 | 200K steps @ 170 steps/sec |
| 分析 | ~2 分鐘 | TensorBoard + 交易行為 |
| 評估 | ~5 分鐘 | 20 Episodes 完整評估 |
| **總計** | **~27 分鐘** | 完整測試循環 |

---

## 📞 決策樹（訓練後）

```
PPO_15 訓練完成
  │
  ├─ Buy 比例 20-40% ✅
  │   ├─ 獎勵 > 0 ✅ → 🎉 成功！延長至 1M steps
  │   └─ 獎勵 < 0 ⚠️ → 延長訓練或微調 vf_coef
  │
  ├─ Buy 比例 40-60% ⚠️
  │   └─ 調整至 -6 或 -7 (PPO_16)
  │
  ├─ Buy 比例 10-20% ⚠️
  │   └─ 調整至 -3 或 -4 (PPO_16)
  │
  └─ Buy 比例 < 10% 或 > 60% ❌
      └─ 放棄固定懲罰，改用 ChatGPT 動態獎勵方案
```

---

**最後更新**: 2025-10-27
**狀態**: ⏳ 待訓練
**下一步**: 執行訓練 → 根據結果決策
