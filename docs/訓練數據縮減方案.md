# 訓練數據縮減方案

**問題**: 訓練數據過多（7,564,697 樣本 → 14,774 batches/epoch），訓練時間過長（~6 小時/40 epochs）

**目標**: 在不影響訓練成果的前提下，縮減訓練時間到 1.5-3 小時

---

## 📊 當前狀況

```
數據來源: preprocessed_v5_1hz (1Hz 高頻數據)
總樣本數: 7,564,697
Batch size: 512
Batches/epoch: 14,774
訓練速度: 28.17 it/s
時間/epoch: ~9 分鐘
總時間 (40 epochs): ~6 小時
```

---

## 🎯 縮減方案對比

| 方案 | 修改位置 | 樣本數 | 時間/epoch | 總時間 | 效果影響 | 推薦度 |
|------|---------|--------|-----------|--------|---------|--------|
| **A: 增大 batch_size** | `train_v5.yaml` | 7.5M | 4.4 分 | 2.9h | ✅ 幾乎無影響 | ⭐⭐⭐⭐⭐ |
| **B: 滑窗步長=5** | `extract_v6.py` | 1.5M | 1.8 分 | 1.2h | ⚠️ 略微降低 | ⭐⭐⭐⭐ |
| **C: tb_stride=20** | `config.yaml` | 3.8M | 4.4 分 | 2.9h | ✅ 無影響 | ⭐⭐⭐ |
| **D: 組合 A+B** | 兩處修改 | 1.5M | 0.9 分 | 0.6h | ⚠️ 略微降低 | ⭐⭐⭐⭐⭐ |

---

## 方案 A: 增大 Batch Size（最簡單）⭐⭐⭐⭐⭐

### 原理
- 更大的 batch → 更少的迭代次數 → 更快的訓練
- 對於 7.5M 樣本，batch_size 從 512 → 2048 可減少 4 倍迭代

### 修改方式

**編輯 `configs/train_v5.yaml`**:
```yaml
dataloader:
  batch_size: 2048  # 512 → 2048 (4倍加速)
```

### 效果預估
```
Batches/epoch: 14,774 → 3,693 (減少 75%)
時間/epoch: 9 分鐘 → 2.2 分鐘
總時間 (40 epochs): 6 小時 → 1.5 小時 ✅
```

### 優點
- ✅ 最簡單，無需重新生成數據
- ✅ 幾乎無副作用（RTX 5090 32GB 顯存足夠）
- ✅ 更大 batch 可能讓梯度估計更穩定

### 缺點
- ⚠️ 需要更多顯存（但 RTX 5090 32GB 綽綽有餘）
- ⚠️ 學習率可能需微調（可能需提高到 0.0001-0.0002）

### 建議
**推薦 batch_size=2048**（4 倍加速，平衡效果與速度）

---

## 方案 B: 增加滑窗步長（最有效）⭐⭐⭐⭐

### 原理
- 當前：每個時間點都創建一個樣本（stride=1）
- 優化：每 5 個點創建一個樣本（stride=5）
- 減少樣本重疊，保留多樣性

### 修改方式

**編輯 `scripts/extract_tw_stock_data_v6.py`**:

找到 Line 856:
```python
# 原始（每個點都創建樣本）
for t in range(SEQ_LEN - 1, max_t):
    window_start = t - SEQ_LEN + 1
    # ...
```

修改為（新增 `window_stride` 參數）:
```python
# 優化（每 5 個點創建一個樣本）
window_stride = config.get('window_stride', 5)  # 可配置，預設 5

for t in range(SEQ_LEN - 1, max_t, window_stride):
    window_start = t - SEQ_LEN + 1
    # ...
```

**在 `config_pro_v5_ml_optimal.yaml` 新增參數**:
```yaml
# 資料處理參數
data:
  aggregation_factor: 10
  seq_len: 100
  window_stride: 5  # ← 新增：滑窗步長（預設 1，建議 5）
```

### 重新生成數據
```bash
python scripts\extract_tw_stock_data_v6.py ^
    --preprocessed-dir .\data\preprocessed_v5_1hz ^
    --output-dir .\data\processed_v6_stride5 ^
    --config .\configs\config_pro_v5_ml_optimal.yaml
```

### 效果預估
```
樣本數: 7,564,697 → 1,512,939 (減少 80%)
Batches/epoch: 14,774 → 2,954 (減少 80%)
時間/epoch: 9 分鐘 → 1.7 分鐘
總時間 (40 epochs): 6 小時 → 1.1 小時 ✅
```

### 優點
- ✅ 大幅減少樣本數（5 倍減少）
- ✅ 保留樣本多樣性（每 5 個點取 1 個）
- ✅ 減少過度重疊

### 缺點
- ⚠️ 需要重新生成數據（8-10 分鐘）
- ⚠️ 略微損失細節（但對 1Hz 數據影響不大）

### 建議
**推薦 window_stride=5**（減少 80% 樣本，保留足夠多樣性）

---

## 方案 C: 增大 tb_stride（效果有限）⭐⭐⭐

### 原理
- `tb_stride` 只控制 Triple-Barrier 計算頻率
- 不影響最終樣本數（標籤會 ffill 填充）
- 已經設為 10，再增大效果不明顯

### 修改方式

**編輯 `config_pro_v5_ml_optimal.yaml`**:
```yaml
tb_stride: 20  # 10 → 20
```

### 效果預估
```
樣本數: 不變（7,564,697）
數據生成速度: 略快 10-20%
訓練時間: 不變
```

### 結論
- ❌ **不推薦**：對訓練速度無幫助，只加快數據生成

---

## 方案 D: 組合優化（推薦）⭐⭐⭐⭐⭐

### 策略
同時使用方案 A + B，達到最佳效果

### 修改步驟

#### 步驟 1: 修改滑窗步長（新增參數）

**編輯 `scripts/extract_tw_stock_data_v6.py`** (Line 856):
```python
window_stride = config.get('window_stride', 5)
for t in range(SEQ_LEN - 1, max_t, window_stride):
```

**編輯 `config_pro_v5_ml_optimal.yaml`**:
```yaml
data:
  window_stride: 5  # 新增
```

**重新生成數據**:
```bash
python scripts\extract_tw_stock_data_v6.py ^
    --preprocessed-dir .\data\preprocessed_v5_1hz ^
    --output-dir .\data\processed_v6_stride5 ^
    --config .\configs\config_pro_v5_ml_optimal.yaml
```

#### 步驟 2: 增大 batch size

**編輯 `configs/train_v5.yaml`**:
```yaml
dataloader:
  batch_size: 2048  # 512 → 2048
```

#### 步驟 3: 訓練

```bash
python scripts\train_deeplob_v5.py ^
    --config configs\train_v5.yaml ^
    --data-dir data\processed_v6_stride5\npz ^
    --epochs 40
```

### 效果預估
```
樣本數: 7,564,697 → 1,512,939 (減少 80%)
Batches/epoch: 14,774 → 738 (減少 95%)
時間/epoch: 9 分鐘 → 0.4 分鐘
總時間 (40 epochs): 6 小時 → 0.3 小時 (16 分鐘) ✅✅✅
```

### 優點
- ✅ 訓練時間極短（16 分鐘 vs 6 小時）
- ✅ 樣本仍然充足（150 萬樣本）
- ✅ 保留數據多樣性

### 缺點
- ⚠️ 需修改代碼 + 重新生成數據
- ⚠️ 略微損失細節（但影響很小）

---

## 🎯 推薦執行方案

### 短期快速方案（5 分鐘內）⭐⭐⭐⭐⭐

**僅修改 batch_size**，無需重新生成數據

```bash
# 1. 修改 configs/train_v5.yaml
#    batch_size: 2048

# 2. 直接訓練
python scripts\train_deeplob_v5.py ^
    --config configs\train_v5.yaml ^
    --data-dir data\processed_v6\npz ^
    --epochs 40
```

**效果**: 6 小時 → **1.5 小時** ✅

---

### 長期最佳方案（30 分鐘設置）⭐⭐⭐⭐⭐

**修改代碼 + 重新生成數據**

```bash
# 1. 修改 extract_tw_stock_data_v6.py (Line 856)
#    window_stride = config.get('window_stride', 5)
#    for t in range(SEQ_LEN - 1, max_t, window_stride):

# 2. 修改 config_pro_v5_ml_optimal.yaml
#    data:
#      window_stride: 5

# 3. 重新生成數據（8-10 分鐘）
python scripts\extract_tw_stock_data_v6.py ^
    --preprocessed-dir .\data\preprocessed_v5_1hz ^
    --output-dir .\data\processed_v6_stride5 ^
    --config .\configs\config_pro_v5_ml_optimal.yaml

# 4. 修改 train_v5.yaml
#    batch_size: 2048

# 5. 訓練（16 分鐘）
python scripts\train_deeplob_v5.py ^
    --config configs\train_v5.yaml ^
    --data-dir data\processed_v6_stride5\npz ^
    --epochs 40
```

**效果**: 6 小時 → **0.3 小時 (16 分鐘)** ✅✅✅

---

## ❓ 常見問題

### Q1: 增大 batch_size 會影響訓練效果嗎？
**A**: 幾乎不會。對於 7.5M 樣本：
- batch_size=512: 梯度估計較不穩定，但更新頻繁
- batch_size=2048: 梯度估計更準確，但更新較少
- **建議**: 同時略微提高學習率（0.00005 → 0.0001）平衡

### Q2: window_stride=5 會損失多少信息？
**A**: 對 1Hz 數據影響很小：
- 原始: 每個 tick 都是樣本（大量重疊）
- stride=5: 每 5 秒一個樣本（仍然密集）
- **結論**: 150 萬樣本足夠訓練，損失可忽略

### Q3: 需要同時調整其他參數嗎？
**A**: 建議微調：
```yaml
# 如果使用 batch_size=2048
optim:
  lr: 0.0001  # 0.00005 → 0.0001 (batch 更大，學習率可略提高)
```

### Q4: 如何驗證縮減後效果沒變差？
**A**: 對比指標：
- Val F1 (Weighted): 應相近（±0.02）
- Val Loss: 應相近
- Train/Val Acc 差距: 應相近
- 如果略有下降（< 2%），屬於正常範圍

---

## ✅ 總結

| 方案 | 時間節省 | 修改成本 | 效果影響 | 推薦場景 |
|------|---------|---------|---------|---------|
| **A: batch_size=2048** | 75% | 1 分鐘 | 幾乎無 | 快速測試 |
| **B: window_stride=5** | 81% | 20 分鐘 | 略微 | 正式訓練 |
| **D: A+B 組合** | 95% | 30 分鐘 | 略微 | 最佳選擇 |

**立即推薦**: 先用方案 A（修改 batch_size）快速驗證，如果效果可接受，再考慮方案 D 進一步優化。

---

**最後更新**: 2025-10-22
**適用專案**: DeepLOB-Pro V6 (1Hz 數據)
