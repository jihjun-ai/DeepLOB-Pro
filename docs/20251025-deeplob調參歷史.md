# DeepLOB 調參歷史記錄

每次保留數據及成果,簡要說明(其他廢話不用),這是改下次調參參考用

## 格式:

**日期**:
**配置**:
**結果**:
**問題**:
**結論**:

---

## V7 實驗 1 (重新開始,V7 數據)

**日期**: 2025-10-25

**數據**: V7 (processed_v7, 更乾淨、無重複計算、標籤一致性高)

**配置**:

```yaml
# 模型容量 (降低)
conv_filters: 32 (48→32, -33%)
lstm_hidden: 48 (64→48, -25%)
dropout: 0.75 (0.70→0.75, +7%)

# 優化器 (慢學習+強正則)
lr: 1e-6 (2.5e-6→1e-6, -60%) ⭐⭐⭐
weight_decay: 0.002 (0.001→0.002, +100%) ⭐⭐⭐
batch_size: 256 (512→256, -50%)

# 學習率調度
warmup_ratio: 0.30 (0.20→0.30)
eta_min: 3e-7 (1e-6→3e-7)

# 訓練策略
epochs: 15 (20→15)
patience: 1 (2→1) ⭐⭐⭐
label_smoothing: 0.02 (0.015→0.02)
```

**先前結果** (實驗 6b 配置在 V7 數據):

```
Epoch 1: Train 45.03% | Val 49.33% | F1 0.4748 | Grad 1.24
Epoch 2: Train 48.40% | Val 49.78% | F1 0.4850 | Grad 1.32
Epoch 3: Train 50.54% | Val 50.09% | F1 0.4871 | Grad 1.67 ⭐ 最佳
Epoch 4: Train 52.82% | Val 49.98% | F1 0.4883 | Grad 2.25
Epoch 5: Train 55.76% | Val 49.20% | F1 0.4777 | Grad 3.34
Epoch 6: Train 59.23% | Val 47.93% | F1 0.4742 | Grad 5.12 ❌
```

**問題**:

1. 嚴重過擬合: Epoch 3 後 Train-Val Gap 從 0.45% → 11.30%
2. Val 性能惡化: Val Acc 50.09% → 47.93% (-2.16%)
3. 梯度持續增長: 1.24 → 5.12 (4.1倍)
4. 最佳點過早: Epoch 3 就達峰值

**根本原因**:

- V7 數據更"乾淨"(無重複計算、標籤一致性高)
- V5/V6 的配置是針對"髒數據"優化的
- 乾淨數據 + 大容量模型 + 高學習率 = 快速過擬合

**改進策略**:

1. 降低模型容量 (32 filters, 48 hidden)
2. 大幅降低學習率 (-60%)
3. 大幅提高正則化 (dropout +7%, weight_decay +100%)
4. 極激進早停 (patience=1)

**預期效果**:
- Epoch 3-5 達最佳
- Val Acc 52-55%
- Train-Val Gap < 5%
- 梯度 < 3.0

**結果** (實際訓練):
```
Epoch 1: Train 40.58% | Val 48.60% | F1 0.4520 | Grad 1.75
Epoch 5: Train 48.86% | Val 49.54% | F1 0.4890 | Grad 2.10
Epoch 6: Train 50.13% | Val 49.74% | F1 0.4928 | Grad 2.62
Epoch 7: Train 51.41% | Val 50.18% | F1 0.4932 | Grad 3.18 ⭐ Val Acc 最佳
Epoch 8: Train 52.73% | Val 49.95% | F1 0.4949 | Grad 3.81 ⭐ Val F1 最佳
Epoch 9: Train 53.97% | Val 49.65% | F1 0.4887 | Grad 4.46 ❌ 過擬合
```

**成果**:
- ✅ Val Acc 提升: 50.09% → 50.18% (+0.09%)
- ✅ Val F1 提升: 0.4871 → 0.4949 (+0.0078)
- ✅ 最佳點延後: Epoch 3 → Epoch 7-8 (延後 4-5 epochs)
- ✅ 過擬合改善: Gap 11.30% → 4.32% (-6.98%)
- ✅ 梯度控制: 5.12 → 4.46 (-0.66)

**仍存在問題**:
- ⚠️ Epoch 7-9: Gap 從 1.23% 快速惡化到 4.32%
- ⚠️ 梯度持續增長: 1.75 → 4.46 (2.5倍)
- ⚠️ Val F1 在 Epoch 8 達峰後下降

**結論**:
- 降低模型容量 + 降低學習率 **有效延緩過擬合**
- 但學習率 1e-6 仍偏高, 需進一步降低到 7e-7
- 正則化需加強 (dropout 0.80, weight_decay 0.003)

---

## V7 實驗 2

**日期**: 2025-10-25

**數據**: V7 (同實驗 1)

**配置** (基於實驗 1 改進):
```yaml
# 正則化 (極限)
batch_size: 128 (256→128, -50%) ⭐
dropout: 0.80 (0.75→0.80, +7%)
label_smoothing: 0.03 (0.02→0.03, +50%)

# 優化器 (超慢學習)
lr: 7e-7 (1e-6→7e-7, -30%) ⭐⭐⭐
weight_decay: 0.003 (0.002→0.003, +50%) ⭐⭐⭐
grad_clip: 1.5 (2.0→1.5, -25%)

# 學習率調度
warmup_ratio: 0.40 (0.30→0.40, 前 8/20 epochs)
warmup_start_factor: 0.15 (0.2→0.15, 起點 1e-7)
eta_min: 2e-7 (3e-7→2e-7)

# 訓練策略
epochs: 20 (15→20, 超慢需更長)
patience: 2 (1→2)
```

**改進重點**:
- 學習率降到接近極限 (7e-7)
- 正則化接近極限 (dropout 0.80, weight_decay 0.003)
- Batch size 128 (梯度噪音更大防過擬合)

**預期效果**:
- Epoch 8-12 達最佳
- Val Acc > 50.5%
- Train-Val Gap < 3%
- 梯度 < 3.5

**結果** (實際訓練):
```
Epoch 1:  Train 39.10% | Val 48.49% | F1 0.4428 | Grad 2.67
Epoch 5:  Train 46.89% | Val 49.63% | F1 0.4838 | Grad 2.44
Epoch 9:  Train 50.38% | Val 50.25% | F1 0.4929 | Grad 4.13 ⭐ Val Acc 最佳
Epoch 12: Train 53.88% | Val 49.75% | F1 0.4899 | Grad 6.65 ❌ 過擬合
```

**成果**:
- ⚠️ Val Acc 微提升: 50.18% → 50.25% (+0.07%, 改善不明顯)
- ⚠️ Val F1 微下降: 0.4949 → 0.4929 (-0.002)
- ✅ 最佳點略延後: Epoch 7-8 → Epoch 9
- ✅ 過擬合控制良好: Gap < 4% (Epoch 1-9)
- ⚠️ 梯度仍偏高: 2.67 → 6.65 (Epoch 12)

**問題診斷**:
1. **學習率過低** ❌: 7e-7 過於保守，Epoch 1-9 學習極慢
2. **Batch Size 過小** ❌: 128 梯度噪音大，不穩定
3. **正則化過強** ⚠️: dropout 0.80 + weight_decay 0.003 抑制學習
4. **改善不明顯**: 相比實驗 1 僅微幅提升 0.07%

**結論**:
- 實驗 2 **未能明顯超越實驗 1**
- 學習率 7e-7 過低 + Batch 128 過小 = 學習過慢
- **建議**: 回調學習率到 8.5e-7, Batch Size 192, dropout 0.77

---

## V7 實驗 3 (平衡策略) ❌ 失敗

**日期**: 2025-10-25

**數據**: V7 (同實驗 1/2)

**配置** (基於實驗 1/2 經驗):
```yaml
# 正則化 (適中, 避免過強)
batch_size: 192  # 實驗2: 128 → 192 (介於 128-256, 降噪同時保留穩定)
dropout: 0.77    # 實驗2: 0.80 → 0.77 (稍微放寬)
label_smoothing: 0.025  # 實驗2: 0.03 → 0.025 (回調)

# 優化器 (平衡學習速度與穩定性) ⭐⭐⭐
lr: 0.00000085   # 實驗2: 7e-7 → 8.5e-7 (+21%, 稍快學習)
weight_decay: 0.0025  # 實驗2: 0.003 → 0.0025 (-17%, 稍微放寬)
grad_clip: 1.7   # 實驗2: 1.5 → 1.7 (稍微放寬)

# 學習率調度
warmup_ratio: 0.35     # 實驗2: 0.40 → 0.35 (介於實驗1/2)
warmup_start_factor: 0.17  # 實驗2: 0.15 → 0.17 (起點 1.4e-7)
eta_min: 0.00000025    # 實驗2: 2e-7 → 2.5e-7

# 訓練策略
epochs: 18      # 實驗2: 20 → 18 (預期 Epoch 8-10 達最佳)
patience: 2     # 保持
```

**改進假設** (事後證明錯誤):
- 實驗 1: 學習率 1e-6 過高 → 過擬合快
- 實驗 2: 學習率 7e-7 過低 → 學習太慢
- 實驗 3: 學習率 8.5e-7 **(介於兩者, 平衡點)** ❌

**結果** (實際訓練):
```
Epoch 1:  Train 39.87% | Val 48.48% | F1 0.4466 | Grad 2.08
Epoch 5:  Train 47.74% | Val 49.51% | F1 0.4847 | Grad 2.09
Epoch 7:  Train 49.85% | Val 49.92% | F1 0.4912 | Grad 2.96
Epoch 8:  Train 51.06% | Val 49.92% | F1 0.4949 | Grad 3.57 ⭐ Val Acc 最佳
Epoch 10: Train 53.72% | Val 49.67% | F1 0.4916 | Grad 5.06 ❌ 過擬合
```

**實驗對比**:
| 指標 | 實驗 1 | 實驗 2 | 實驗 3 | 最佳 |
|------|--------|--------|--------|------|
| Val Acc | 50.18% (Epoch 7) | **50.25%** (Epoch 9) | 49.92% (Epoch 8) | 實驗 2 ⭐ |
| Val F1 | **0.4949** (Epoch 8) | 0.4929 (Epoch 9) | **0.4949** (Epoch 8) | 實驗 1/3 並列 |
| 學習率 | 1e-6 (過高) | **7e-7** (合適) | 8.5e-7 (偏高) | 實驗 2 ⭐ |
| 梯度範數 | 4.46 | **4.13** | 5.06 | 實驗 2 ⭐ |
| 過擬合控制 | 中 | **最佳** | 偏差 | 實驗 2 ⭐ |

**問題診斷**:
1. ❌ **實驗 3 未達預期**: Val Acc 49.92%，比實驗 2 退步 -0.33%
2. ❌ **學習率回調反而更差**: 8.5e-7 仍偏高，過擬合快於實驗 2
3. ❌ **Batch 192 不夠大**: 梯度噪音仍不足以防過擬合
4. ⚠️ **梯度範數過高**: 5.06 > 實驗 2 的 4.13

**結論**:
- 實驗 2 配置 (學習率 7e-7) **實際最優**
- 實驗 3 的"平衡假設"錯誤，導致反向優化
- 三實驗最佳 Val Acc 差距極小: 50.18% vs **50.25%** vs 49.92% (僅 0.33%)
- **當前最佳配置**: 實驗 2 (Val Acc 50.25%, F1 0.4929)

---

## V7 實驗 4 建議 (基於實驗 2 微調)

**日期**: 待執行

**數據**: V7

**配置** (基於實驗 2 最優配置):
```yaml
# 正則化 (微調實驗 2)
batch_size: 160      # 128 → 160 (+25%, 稍大降噪)
dropout: 0.78        # 0.80 → 0.78 (-2%, 微放寬)
label_smoothing: 0.028  # 0.03 → 0.028 (-7%)

# 優化器 (極微幅調整) ⭐⭐⭐
lr: 0.00000075       # 7e-7 → 7.5e-7 (+7%, 微調)
weight_decay: 0.0028 # 0.003 → 0.0028 (-7%)
grad_clip: 1.6       # 1.5 → 1.6

# 學習率調度
warmup_ratio: 0.38   # 0.40 → 0.38 (約 8.4/22 epochs)
warmup_start_factor: 0.16  # 0.15 → 0.16
eta_min: 0.00000022  # 2e-7 → 2.2e-7

# 訓練策略
epochs: 22           # 20 → 22 (慢學習需更長)
patience: 2          # 保持
```

**改進邏輯**:
- 實驗 2 已證明 7e-7 是合適學習率
- 實驗 4 目標: 微幅提升 (+7% LR) 尋找極限
- Batch 160: 介於 128-192, 平衡穩定與正則化
- 所有參數微調 (-7% ~ +7%)

**預期效果**:
- Epoch 10-12 達最佳 (vs 實驗 2 Epoch 9)
- Val Acc > 50.3% (vs 實驗 2 的 50.25%)
- Val F1 > 0.495 (vs 實驗 2 的 0.4929)
- Train-Val Gap < 3%
- 梯度 < 4.0

---

##
