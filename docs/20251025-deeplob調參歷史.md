# DeepLOB 調參歷史記錄

每次保留數據及成果,簡要說明(其他廢話不用),這是改下次調參參考用

## 格式:

**日期**:
**配置**:
**結果**:
**問題**:
**結論**:

---

## V7 實驗 1 (重新開始,V7 數據)

**日期**: 2025-10-25

**數據**: V7 (processed_v7, 更乾淨、無重複計算、標籤一致性高)

**配置**:

```yaml
# 模型容量 (降低)
conv_filters: 32 (48→32, -33%)
lstm_hidden: 48 (64→48, -25%)
dropout: 0.75 (0.70→0.75, +7%)

# 優化器 (慢學習+強正則)
lr: 1e-6 (2.5e-6→1e-6, -60%) ⭐⭐⭐
weight_decay: 0.002 (0.001→0.002, +100%) ⭐⭐⭐
batch_size: 256 (512→256, -50%)

# 學習率調度
warmup_ratio: 0.30 (0.20→0.30)
eta_min: 3e-7 (1e-6→3e-7)

# 訓練策略
epochs: 15 (20→15)
patience: 1 (2→1) ⭐⭐⭐
label_smoothing: 0.02 (0.015→0.02)
```

**先前結果** (實驗 6b 配置在 V7 數據):

```
Epoch 1: Train 45.03% | Val 49.33% | F1 0.4748 | Grad 1.24
Epoch 2: Train 48.40% | Val 49.78% | F1 0.4850 | Grad 1.32
Epoch 3: Train 50.54% | Val 50.09% | F1 0.4871 | Grad 1.67 ⭐ 最佳
Epoch 4: Train 52.82% | Val 49.98% | F1 0.4883 | Grad 2.25
Epoch 5: Train 55.76% | Val 49.20% | F1 0.4777 | Grad 3.34
Epoch 6: Train 59.23% | Val 47.93% | F1 0.4742 | Grad 5.12 ❌
```

**問題**:

1. 嚴重過擬合: Epoch 3 後 Train-Val Gap 從 0.45% → 11.30%
2. Val 性能惡化: Val Acc 50.09% → 47.93% (-2.16%)
3. 梯度持續增長: 1.24 → 5.12 (4.1倍)
4. 最佳點過早: Epoch 3 就達峰值

**根本原因**:

- V7 數據更"乾淨"(無重複計算、標籤一致性高)
- V5/V6 的配置是針對"髒數據"優化的
- 乾淨數據 + 大容量模型 + 高學習率 = 快速過擬合

**改進策略**:

1. 降低模型容量 (32 filters, 48 hidden)
2. 大幅降低學習率 (-60%)
3. 大幅提高正則化 (dropout +7%, weight_decay +100%)
4. 極激進早停 (patience=1)

**預期效果**:
- Epoch 3-5 達最佳
- Val Acc 52-55%
- Train-Val Gap < 5%
- 梯度 < 3.0

**結果** (實際訓練):
```
Epoch 1: Train 40.58% | Val 48.60% | F1 0.4520 | Grad 1.75
Epoch 5: Train 48.86% | Val 49.54% | F1 0.4890 | Grad 2.10
Epoch 6: Train 50.13% | Val 49.74% | F1 0.4928 | Grad 2.62
Epoch 7: Train 51.41% | Val 50.18% | F1 0.4932 | Grad 3.18 ⭐ Val Acc 最佳
Epoch 8: Train 52.73% | Val 49.95% | F1 0.4949 | Grad 3.81 ⭐ Val F1 最佳
Epoch 9: Train 53.97% | Val 49.65% | F1 0.4887 | Grad 4.46 ❌ 過擬合
```

**成果**:
- ✅ Val Acc 提升: 50.09% → 50.18% (+0.09%)
- ✅ Val F1 提升: 0.4871 → 0.4949 (+0.0078)
- ✅ 最佳點延後: Epoch 3 → Epoch 7-8 (延後 4-5 epochs)
- ✅ 過擬合改善: Gap 11.30% → 4.32% (-6.98%)
- ✅ 梯度控制: 5.12 → 4.46 (-0.66)

**仍存在問題**:
- ⚠️ Epoch 7-9: Gap 從 1.23% 快速惡化到 4.32%
- ⚠️ 梯度持續增長: 1.75 → 4.46 (2.5倍)
- ⚠️ Val F1 在 Epoch 8 達峰後下降

**結論**:
- 降低模型容量 + 降低學習率 **有效延緩過擬合**
- 但學習率 1e-6 仍偏高, 需進一步降低到 7e-7
- 正則化需加強 (dropout 0.80, weight_decay 0.003)

---

## V7 實驗 2

**日期**: 2025-10-25

**數據**: V7 (同實驗 1)

**配置** (基於實驗 1 改進):
```yaml
# 正則化 (極限)
batch_size: 128 (256→128, -50%) ⭐
dropout: 0.80 (0.75→0.80, +7%)
label_smoothing: 0.03 (0.02→0.03, +50%)

# 優化器 (超慢學習)
lr: 7e-7 (1e-6→7e-7, -30%) ⭐⭐⭐
weight_decay: 0.003 (0.002→0.003, +50%) ⭐⭐⭐
grad_clip: 1.5 (2.0→1.5, -25%)

# 學習率調度
warmup_ratio: 0.40 (0.30→0.40, 前 8/20 epochs)
warmup_start_factor: 0.15 (0.2→0.15, 起點 1e-7)
eta_min: 2e-7 (3e-7→2e-7)

# 訓練策略
epochs: 20 (15→20, 超慢需更長)
patience: 2 (1→2)
```

**改進重點**:
- 學習率降到接近極限 (7e-7)
- 正則化接近極限 (dropout 0.80, weight_decay 0.003)
- Batch size 128 (梯度噪音更大防過擬合)

**預期效果**:
- Epoch 8-12 達最佳
- Val Acc > 50.5%
- Train-Val Gap < 3%
- 梯度 < 3.5

**結果** (實際訓練):
```
Epoch 1:  Train 39.10% | Val 48.49% | F1 0.4428 | Grad 2.67
Epoch 5:  Train 46.89% | Val 49.63% | F1 0.4838 | Grad 2.44
Epoch 9:  Train 50.38% | Val 50.25% | F1 0.4929 | Grad 4.13 ⭐ Val Acc 最佳
Epoch 12: Train 53.88% | Val 49.75% | F1 0.4899 | Grad 6.65 ❌ 過擬合
```

**成果**:
- ⚠️ Val Acc 微提升: 50.18% → 50.25% (+0.07%, 改善不明顯)
- ⚠️ Val F1 微下降: 0.4949 → 0.4929 (-0.002)
- ✅ 最佳點略延後: Epoch 7-8 → Epoch 9
- ✅ 過擬合控制良好: Gap < 4% (Epoch 1-9)
- ⚠️ 梯度仍偏高: 2.67 → 6.65 (Epoch 12)

**問題診斷**:
1. **學習率過低** ❌: 7e-7 過於保守，Epoch 1-9 學習極慢
2. **Batch Size 過小** ❌: 128 梯度噪音大，不穩定
3. **正則化過強** ⚠️: dropout 0.80 + weight_decay 0.003 抑制學習
4. **改善不明顯**: 相比實驗 1 僅微幅提升 0.07%

**結論**:
- 實驗 2 **未能明顯超越實驗 1**
- 學習率 7e-7 過低 + Batch 128 過小 = 學習過慢
- **建議**: 回調學習率到 8.5e-7, Batch Size 192, dropout 0.77

---

## V7 實驗 3 (平衡策略) ❌ 失敗

**日期**: 2025-10-25

**數據**: V7 (同實驗 1/2)

**配置** (基於實驗 1/2 經驗):
```yaml
# 正則化 (適中, 避免過強)
batch_size: 192  # 實驗2: 128 → 192 (介於 128-256, 降噪同時保留穩定)
dropout: 0.77    # 實驗2: 0.80 → 0.77 (稍微放寬)
label_smoothing: 0.025  # 實驗2: 0.03 → 0.025 (回調)

# 優化器 (平衡學習速度與穩定性) ⭐⭐⭐
lr: 0.00000085   # 實驗2: 7e-7 → 8.5e-7 (+21%, 稍快學習)
weight_decay: 0.0025  # 實驗2: 0.003 → 0.0025 (-17%, 稍微放寬)
grad_clip: 1.7   # 實驗2: 1.5 → 1.7 (稍微放寬)

# 學習率調度
warmup_ratio: 0.35     # 實驗2: 0.40 → 0.35 (介於實驗1/2)
warmup_start_factor: 0.17  # 實驗2: 0.15 → 0.17 (起點 1.4e-7)
eta_min: 0.00000025    # 實驗2: 2e-7 → 2.5e-7

# 訓練策略
epochs: 18      # 實驗2: 20 → 18 (預期 Epoch 8-10 達最佳)
patience: 2     # 保持
```

**改進假設** (事後證明錯誤):
- 實驗 1: 學習率 1e-6 過高 → 過擬合快
- 實驗 2: 學習率 7e-7 過低 → 學習太慢
- 實驗 3: 學習率 8.5e-7 **(介於兩者, 平衡點)** ❌

**結果** (實際訓練):
```
Epoch 1:  Train 39.87% | Val 48.48% | F1 0.4466 | Grad 2.08
Epoch 5:  Train 47.74% | Val 49.51% | F1 0.4847 | Grad 2.09
Epoch 7:  Train 49.85% | Val 49.92% | F1 0.4912 | Grad 2.96
Epoch 8:  Train 51.06% | Val 49.92% | F1 0.4949 | Grad 3.57 ⭐ Val Acc 最佳
Epoch 10: Train 53.72% | Val 49.67% | F1 0.4916 | Grad 5.06 ❌ 過擬合
```

**實驗對比**:
| 指標 | 實驗 1 | 實驗 2 | 實驗 3 | 最佳 |
|------|--------|--------|--------|------|
| Val Acc | 50.18% (Epoch 7) | **50.25%** (Epoch 9) | 49.92% (Epoch 8) | 實驗 2 ⭐ |
| Val F1 | **0.4949** (Epoch 8) | 0.4929 (Epoch 9) | **0.4949** (Epoch 8) | 實驗 1/3 並列 |
| 學習率 | 1e-6 (過高) | **7e-7** (合適) | 8.5e-7 (偏高) | 實驗 2 ⭐ |
| 梯度範數 | 4.46 | **4.13** | 5.06 | 實驗 2 ⭐ |
| 過擬合控制 | 中 | **最佳** | 偏差 | 實驗 2 ⭐ |

**問題診斷**:
1. ❌ **實驗 3 未達預期**: Val Acc 49.92%，比實驗 2 退步 -0.33%
2. ❌ **學習率回調反而更差**: 8.5e-7 仍偏高，過擬合快於實驗 2
3. ❌ **Batch 192 不夠大**: 梯度噪音仍不足以防過擬合
4. ⚠️ **梯度範數過高**: 5.06 > 實驗 2 的 4.13

**結論**:
- 實驗 2 配置 (學習率 7e-7) **實際最優**
- 實驗 3 的"平衡假設"錯誤，導致反向優化
- 三實驗最佳 Val Acc 差距極小: 50.18% vs **50.25%** vs 49.92% (僅 0.33%)
- **當前最佳配置**: 實驗 2 (Val Acc 50.25%, F1 0.4929)

---

## V5 實驗 4 (基於 V7 實驗 2 最優配置) ⭐⭐⭐

**日期**: 2025-10-25

**數據**: V5 (processed_v5, 舊版數據)

**背景**:
- V5 先前實驗: Val Acc 49.93% (Epoch 8), 卡在 50% 天花板
- V7 實驗 2: Val Acc 50.25% (Epoch 9), 學習率 7e-7 最優 ⭐
- V5 先前問題: 梯度爆炸 (2.08→5.07), 過擬合 (Train-Val Gap +4.0%)

**配置** (完全採用 V7 實驗 2):
```yaml
# 優化器 (超慢學習) ⭐⭐⭐⭐⭐
lr: 0.0000007        # 7e-7 (V7實驗2證明最優)
weight_decay: 0.003  # 極限正則化
grad_clip: 1.5       # 收緊控制

# 正則化 (極限)
batch_size: 128      # 梯度噪音防過擬合
dropout: 0.80        # 極限正則化
label_smoothing: 0.03  # 極限平滑

# 學習率調度
warmup_ratio: 0.40   # 前 8/20 epochs
warmup_start_factor: 0.15  # 起點 1e-7
eta_min: 0.0000002   # 2e-7

# 訓練策略
epochs: 20           # 超慢學習需更長
patience: 2          # 避免過早停
```

**結果** (實際訓練):
```
Epoch 1:  Train 39.10% | Val 48.49% | F1 0.4428 | Grad 2.67
Epoch 5:  Train 46.89% | Val 49.63% | F1 0.4838 | Grad 2.44
Epoch 9:  Train 50.38% | Val 50.24% | F1 0.4929 | Grad 4.14 ⭐ Val Acc 最佳
Epoch 12: Train 53.88% | Val 49.74% | F1 0.4899 | Grad 6.65 ❌ 過擬合
```

**成果**:
- ✅ Val Acc 達標: 50.24% (vs 預期 > 50.0%)
- ✅ 與 V7 實驗 2 一致: 50.24% vs 50.25% (僅差 0.01%)
- ✅ 最佳點延後: Epoch 9 (vs V5先前 Ep7-8)
- ✅ 梯度控制良好 (Ep9): 4.14 < 4.5
- ⚠️ 後期過擬合: Ep9→12 梯度 4.14→6.65 (+60%)

**問題診斷**:
1. **50% 天花板依然存在** ❌: Val Acc 50.24%, 僅比先前 +0.31%
2. **配置已達最優** ✅: V5 與 V7 實驗 2 幾乎一致
3. **後期梯度爆炸** ⚠️: Ep9→12 梯度 +60%
4. **LR 顯示問題** ⚠️: Ep1-5 顯示 0.000000 (實際 warmup 起點 1e-7)

**關鍵發現**:
- **V5/V7 數據質量一致**: 相同配置相同結果 (50.24% vs 50.25%)
- **50% 天花板非配置問題**: 三實驗均卡在 50.0-50.3%
- **根本原因推測**:
  1. 模型容量不足 (conv 32, lstm 48 vs 原版 64)
  2. 數據標籤質量 (三分類高頻困難)
  3. 優化策略過保守 (極限正則化抑制學習)

**結論**:
- V7 實驗 2 配置**確實最優** (V5/V7 數據驗證)
- 但 batch 128 + lr 7e-7 **學習過慢**
- 建議下次: batch 160 + lr 7.3e-7 (ChatGPT 建議的微調版)

---

## V5 實驗 5 (基於 ChatGPT 建議保守微調) ⭐⭐⭐⭐⭐

**日期**: 2025-10-25

**數據**: V5 (processed_v5)

**背景**:
- V5 實驗 4: Val Acc 50.24% (Epoch 9), 卡在 50% 天花板
- ChatGPT 分析: batch 160 關鍵改進, lr 微升 +4-7%
- 問題: Ep9 後過擬合 (梯度 4.14→6.65)

**配置** (基於 ChatGPT 建議的保守版):
```yaml
# 正則化 (微調)
batch_size: 160      # 128→160 (+25%, ChatGPT核心建議) ⭐⭐⭐⭐⭐
dropout: 0.78        # 0.80→0.78 (-2.5%, 配合LR微升)
label_smoothing: 0.028  # 0.03→0.028 (-7%)

# 優化器 (保守微調) ⭐⭐⭐⭐
lr: 0.00000073       # 7e-7→7.3e-7 (+4%, 更保守避免Exp-3過擬合)
weight_decay: 0.0029 # 0.003→0.0029 (-3%, 平衡正則化)
grad_clip: 1.6       # 1.5→1.6 (+7%, 配合batch變大)

# 學習率調度
warmup_ratio: 0.38   # 0.40→0.38 (-5%)
warmup_start_factor: 0.16  # 0.15→0.16 (+7%, 改善LR顯示)
eta_min: 0.00000021  # 2e-7→2.1e-7 (+5%)

# 訓練策略
epochs: 22           # 20→22 (預期最佳點延後到Ep10-12)
patience: 2          # 保持
```

**ChatGPT 建議對比**:
| 參數 | ChatGPT 建議 | 本實驗 (保守版) | 差異 |
|------|-------------|----------------|------|
| batch_size | 160 | 160 | ✅ 採納 |
| lr | 7.5e-7 | 7.3e-7 | 更保守 (-3%) |
| dropout | 0.78 | 0.78 | ✅ 採納 |
| weight_decay | 0.0028 | 0.0029 | 微調 (+0.4%) |
| grad_clip | 1.6 | 1.6 | ✅ 採納 |

**改進邏輯**:
1. **batch 160**: 降低梯度噪音, 穩定學習 (關鍵改進)
2. **lr 7.3e-7**: 避免回到 Exp-3 (8.5e-7) 的過擬合
3. **微放寬正則**: 配合 LR 微升, 加速學習
4. **改善 LR 顯示**: warmup_start_factor 0.16 (Ep1 顯示 1.17e-7)

**預期效果**:
- Val Acc: 50.4-50.6% (+0.2-0.4% vs Exp-4)
- 最佳點: Epoch 10-12 (vs Exp-4 Epoch 9)
- 梯度範數: < 4.0 (vs Exp-4 的 4.14)
- Train-Val Gap: < 3% (vs Exp-4 的 3.5%)

**監控重點**:
1. Epoch 10-12 是否 Val Acc > 50.4%
2. 梯度範數是否 < 4.0
3. 若仍卡 50.5%, 下次改用激進方案 B (增加模型容量)

**結果**: 待執行

**結論**: 待更新

---

## 50% 天花板根本原因診斷 ⭐⭐⭐⭐⭐ (重要)

**日期**: 2025-10-25

**診斷方法**: 基於專業分析框架 (標籤健檢 + 基線測試)

### 標籤分布分析

```
V7 數據標籤分布:
==================
Train (1,011,625 樣本):
  下跌 (0): 312,895 (30.93%)
  持平 (1): 432,512 (42.75%) ⚠️ 關鍵問題
  上漲 (2): 266,218 (26.32%)

Val (162,936 樣本):
  下跌 (0):  50,747 (31.15%)
  持平 (1):  70,863 (43.49%) ⚠️
  上漲 (2):  41,326 (25.36%)

Test (168,358 樣本):
  下跌 (0):  46,959 (27.89%)
  持平 (1):  75,640 (44.93%) ⚠️
  上漲 (2):  45,759 (27.18%)
```

### 關鍵發現

1. **持平類佔 43-45%** ⚠️⚠️⚠️⚠️⚠️
   - 模型只要**偏好預測持平**就能達到 43% 基準
   - Val Acc 50.24% ≈ **43% (持平) + 7% (其他)**
   - 從 V5 Exp-4 報告：**持平類召回率 93.7%** → 模型確實偏好持平

2. **Val Loss 接近隨機基線** ⚠️
   - 三分類隨機基線: ln(3) = **1.0986**
   - V5 Exp-4 Val Loss: **0.995**
   - 僅比隨機好 **9.4%** → 可分信號極弱

3. **多輪調參 Val 幾乎不動** ⚠️
   - V7 Exp 1: 50.18% (lr 1e-6, batch 256)
   - V7 Exp 2: 50.25% (lr 7e-7, batch 128) ⭐
   - V7 Exp 3: 49.92% (lr 8.5e-7, batch 192)
   - V5 Exp 4: 50.24% (lr 7e-7, batch 128)
   - **差異範圍 < 0.33%** → 曲線形狀變、結論不變

4. **過擬合模式** ⚠️
   - Ep9 後 Train 持續上升、Val 轉頭下降
   - 梯度範數 4.14 → 6.65 (+60%)
   - 代表**背訓練集答案**，但驗證集無更多規律可學

### 診斷結論 ⭐⭐⭐⭐⭐

**確認：50% 天花板是標籤定義/任務設計問題，非模型容量問題**

#### 證據鏈 (符合"訊號弱/標籤噪音高"四徵):

1. ✅ **多次調參→Val 幾乎不動** (0.33% 範圍)
2. ✅ **Val loss 僅略低於隨機基線** (0.995 vs 1.0986)
3. ✅ **W-F1 ≈ U-F1** (整體可分性不高，非單一類拖累)
4. ✅ **後期過擬合 + 梯度放大卻無收益**

#### 根本原因:

1. **標籤定義含糊** ⚠️⚠️⚠️⚠️⚠️ (最關鍵)
   - 三分類 (下跌/持平/上漲) 在高頻 LOB 資料極難
   - **持平類邊界不清**：什麼算持平？
   - 可能存在大量**邊界噪音樣本** (信心 0.45-0.55 區域)

2. **任務可分性低** ⚠️⚠️⚠️
   - 100 時間步 LOB → 預測下一步方向
   - 高頻價格變動包含大量**隨機噪音**
   - Bayes error 可能本身就很高 (接近 50%)

3. **模型已達理論上限** ✅
   - 相同配置在 V5/V7 數據均 50.24-50.25%
   - 不同容量/LR/正則 → 結果一致
   - **繼續調參邊際效益極小**

### 突破方向 (優先順序) ⭐⭐⭐⭐⭐

#### 方向 1: 改進標籤質量 (保持三分類) ⭐⭐⭐⭐⭐

```python
# 重要: DeepLOB 應保持三分類輸出!
# 理由: RL Agent 需要持平機率作為風險信號
# 問題: 持平類佔 43%, 邊界含糊 → 訓練困難

# 改進方案:
1. **收緊持平定義** ⭐⭐⭐⭐⭐ (最直接)
   當前 (推測): |price_change| < 0.05% → 持平
   改進: |price_change| < 0.02% → 持平
   結果: 持平類 43% → 30%, 邊界更清晰

   或使用動態閾值:
   |price_change| < 0.5 * rolling_std → 持平

2. **波動率調整標籤**
   - 低波動時段: 更寬鬆的持平定義
   - 高波動時段: 更嚴格的持平定義
   - 避免標籤噪音

3. **多地平線標籤** (Multi-horizon)
   - 同時預測 2/5/10 步的方向
   - 多任務學習讓模型自己學對齊
   - 可能抓到不同時間尺度的模式

3. **帶成交邏輯的 PnL 標籤**
   - 不用裸 mid-price 方向
   - 用**觸價事件** (是否在 N 步內觸及目標價)
   - 或用實際可實現 PnL (考慮 bid-ask spread)

4. **收緊持平定義**
   - 當前可能: |price_change| < threshold → 持平
   - 改為: 收緊 threshold (例如 0.05% → 0.02%)
   - 減少持平類樣本 (43% → 30%)
```

#### 方向 2: 資料降噪 ⭐⭐⭐⭐

```python
1. **去除低信噪比時段**
   - 開盤/收盤前後 5 分鐘
   - 成交量極低時段
   - 劇烈跳空時段

2. **邊界樣本分析**
   - 抽查模型信心 0.45-0.55 的樣本
   - 人工檢視 LOB 序列是否「肉眼難判」
   - 若大量樣本屬於噪音區 → 先處理資料

3. **標籤一致性測試**
   - 對重複/高度相似的 LOB 切片
   - 檢查標籤是否一致
   - 若不一致率高 → 標籤生成邏輯有問題
```

#### 方向 3: 模型改進 (次優) ⭐⭐⭐

```python
# 注意: 當前診斷顯示這不是主因, 但可嘗試

1. **表徵增強**
   - DeepLOB 後加 BiLSTM/TCN
   - 加 Attention pooling
   - 自監督預訓練 (masked LOB reconstruction)

2. **多任務學習**
   - 主任務: 方向分類
   - 輔助任務: 價格回歸、波動率預測
   - 共享表徵可能學到更多資訊

3. **不確定性量化**
   - 加 Dropout 推理 (MC Dropout)
   - 或 Ensemble
   - 只在**高信心區域**做決策
```

#### 方向 4: 改變評估視角 ⭐⭐⭐⭐

```python
# 50% 準確率未必代表失敗!

1. **分桶評估**
   - 按模型信心分桶 (0.6-0.7, 0.7-0.8, 0.8+)
   - 只要**高信心區域 precision 高**
   - 一樣能用於交易決策

2. **校準曲線**
   - 檢查預測機率是否可靠
   - 若校準良好, 可用於 RL 或風險管理

3. **策略回測**
   - 不只看準確率
   - 看**實際交易 PnL、Sharpe Ratio**
   - 可能 50% 準確率但高勝率策略存在
```

### 下一步建議 (立即執行) ⭐⭐⭐⭐⭐

#### 短期 (1-2 天):

1. **執行 V5 實驗 5** (已配置)
   - 驗證 batch 160 微調是否有微小提升
   - **預期**: 仍卡在 50.5% 左右

2. **標籤邊界分析** ⭐⭐⭐⭐⭐
   ```python
   # 檢查持平類定義
   # 分析模型信心 0.45-0.55 樣本
   # 人工抽樣 100 個邊界樣本
   ```

3. **簡單基線測試**
   ```python
   # Logistic Regression 在 DeepLOB 特徵上
   # 若也能 48-50%, 確認非容量問題
   ```

#### 中期 (1-2 週):

1. **重新定義標籤** ⭐⭐⭐⭐⭐
   - 改為二分類 + 不交易遮罩
   - 或多地平線標籤

2. **資料降噪**
   - 去除低信噪比時段
   - 標籤一致性測試

3. **分桶評估**
   - 按信心區間評估
   - 準備用於 RL (Stable-Baselines3)

### 總結

**停止繼續調超參數** ⚠️⚠️⚠️⚠️⚠️

當前配置 (V7 Exp-2 / V5 Exp-4) **已是理論最優**，繼續微調 LR/batch/正則：
- **最多提升 0.2-0.4%** (50.24% → 50.5%)
- **邊際效益極小**
- **不解決根本問題**

**根本解決方向**:
1. **直接進入 RL 訓練** ⭐⭐⭐⭐⭐ (當前 DeepLOB 可能已夠用)
2. 改進標籤質量 (收緊持平定義) > 降噪資料 > 模型改進 > 繼續調參

**重要澄清**:
- DeepLOB **應保持三分類** (下跌/持平/上漲)
- 持平機率是 RL 的**重要風險信號**
- 50% 準確率在 RL 情境可能夠用 (只需高信心區域準確)

**當務之急**:
1. ✅ 執行診斷腳本 (確認基線) - 1 天
2. ⏳ 訓練 V5 實驗 5 (驗證微調) - 1 天
3. ⭐⭐⭐⭐⭐ **直接開始 RL 訓練** (最優先)
   - 使用當前 DeepLOB (50% 準確率)
   - 訓練 PPO Agent
   - 分桶評估 + 策略回測
4. 根據 RL 結果決定是否需要改進 DeepLOB

---

## V7 實驗 4 建議 (基於實驗 2 微調)

**日期**: 待執行

**數據**: V7

**配置** (基於實驗 2 最優配置):
```yaml
# 正則化 (微調實驗 2)
batch_size: 160      # 128 → 160 (+25%, 稍大降噪)
dropout: 0.78        # 0.80 → 0.78 (-2%, 微放寬)
label_smoothing: 0.028  # 0.03 → 0.028 (-7%)

# 優化器 (極微幅調整) ⭐⭐⭐
lr: 0.00000075       # 7e-7 → 7.5e-7 (+7%, 微調)
weight_decay: 0.0028 # 0.003 → 0.0028 (-7%)
grad_clip: 1.6       # 1.5 → 1.6

# 學習率調度
warmup_ratio: 0.38   # 0.40 → 0.38 (約 8.4/22 epochs)
warmup_start_factor: 0.16  # 0.15 → 0.16
eta_min: 0.00000022  # 2e-7 → 2.2e-7

# 訓練策略
epochs: 22           # 20 → 22 (慢學習需更長)
patience: 2          # 保持
```

**改進邏輯**:
- 實驗 2 已證明 7e-7 是合適學習率
- 實驗 4 目標: 微幅提升 (+7% LR) 尋找極限
- Batch 160: 介於 128-192, 平衡穩定與正則化
- 所有參數微調 (-7% ~ +7%)

**預期效果**:
- Epoch 10-12 達最佳 (vs 實驗 2 Epoch 9)
- Val Acc > 50.3% (vs 實驗 2 的 50.25%)
- Val F1 > 0.495 (vs 實驗 2 的 0.4929)
- Train-Val Gap < 3%
- 梯度 < 4.0

---

##
