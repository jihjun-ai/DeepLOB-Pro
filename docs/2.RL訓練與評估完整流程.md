# RL 訓練與評估完整流程

**日期**: 2025-10-25
**當前階段**: RL 訓練中 (PPO + DeepLOB)
**目標**: 實現高頻交易策略 (Sharpe Ratio > 2.0)

---

## 📋 目錄

1. [當前狀態](#當前狀態)
2. [正在執行：RL 完整訓練](#正在執行rl-完整訓練)
3. [訓練完成後：評估流程](#訓練完成後評估流程)
4. [根據評估結果的決策樹](#根據評估結果的決策樹)
5. [備選方案：DeepLOB Exp-6](#備選方案deeplob-exp-6)
6. [常見問題排解](#常見問題排解)
7. [完整指令速查](#完整指令速查)

---

## 當前狀態

### ✅ 已完成

1. **DeepLOB 訓練** (V5 Exp-5)
   - Val Acc: 50.24%
   - 檢查點: `checkpoints/v5/deeplob_v5_best.pth`
   - 問題: 模型過度保守，不給出高信心預測 (≥0.8)

2. **診斷與校準**
   - 標籤分布確認: 持平類 43%
   - 簡單基線: Logistic Regression ≈ 48-50%
   - Temperature Scaling: 1.48 (無法產生高信心預測)
   - 結論: 過度正則化問題

3. **RL 快速測試** (10K steps) ✅
   - 初期 Reward: -183
   - 最新 Reward: -39.5 (+79% 改善!)
   - 結論: **訓練有效，繼續完整訓練**

### ⏳ 正在進行

**RL 完整訓練** (1M steps, 4-8 小時)

```bash
# 當前執行中
python scripts/train_sb3_deeplob.py --timesteps 1000000
```

---

## 正在執行：RL 完整訓練

### 訓練配置

```yaml
算法: PPO (Proximal Policy Optimization)
環境: TaiwanLOBTradingEnv
特徵提取器: DeepLOB (凍結權重)
總步數: 1,000,000
預計時間: 4-8 小時 (RTX 5090)
```

### 監控方式

#### 方法 1: TensorBoard (推薦)

```bash
# 另開終端
conda activate deeplob-pro
tensorboard --logdir logs/sb3_deeplob/

# 瀏覽器打開
http://localhost:6006
```

**重點監控指標**:
- `rollout/ep_rew_mean`: Episode 平均獎勵 (目標: 持續上升)
- `train/explained_variance`: 價值函數品質 (目標: > 0.5)
- `train/policy_gradient_loss`: 策略梯度損失
- `train/value_loss`: 價值函數損失

#### 方法 2: 查看日誌文件

```bash
# 查看最新訓練日誌
tail -f logs/sb3_deeplob/最新運行目錄/progress.txt
```

### 預期訓練軌跡

```
訓練階段            Steps        預期 Reward    說明
===========================================================
探索期             0-10K        -180 → -40     ✅ 已完成
學習基礎策略       10-50K       -40 → 0        快速改善
開始獲利           50-200K      0 → +50        策略成型
穩定獲利           200K-1M      +50 → +100+    精煉策略
```

### 訓練進度檢查點

每隔一段時間檢查：

```bash
# 檢查最新 Reward
grep "ep_rew_mean" logs/sb3_deeplob/最新運行/progress.txt | tail -5

# 預期輸出類似:
# | rollout/ep_rew_mean     | -39.5    |  ← 10K steps
# | rollout/ep_rew_mean     | -10.2    |  ← 50K steps
# | rollout/ep_rew_mean     | 15.8     |  ← 100K steps
# | rollout/ep_rew_mean     | 45.3     |  ← 500K steps
# | rollout/ep_rew_mean     | 78.6     |  ← 1M steps
```

### 何時停止訓練？

**正常情況**: 等待 1M steps 自動完成

**提前停止** (如果出現):
- ⚠️ Reward 連續 100K steps 沒有改善
- ⚠️ Policy Loss 爆炸 (>10.0)
- ⚠️ Value Loss 發散

**處理方式**: 見 [常見問題排解](#常見問題排解)

---

## 訓練完成後：評估流程

### 步驟 1: 找到最佳模型

```bash
# 檢查保存的模型
ls -lh checkpoints/sb3/ppo_deeplob/

# 應該看到:
# - best_model.zip         ← 最佳模型 (根據評估 Reward)
# - ppo_deeplob_final.zip  ← 最終模型 (1M steps 時)
```

### 步驟 2: 運行評估腳本

```bash
# 評估最佳模型 (20 個 episodes)
python scripts/evaluate_sb3.py \
    --model checkpoints/sb3/ppo_deeplob/best_model \
    --n_episodes 20 \
    --save_report

# 預計時間: 5-10 分鐘
```

### 步驟 3: 查看評估報告

```bash
# 查看 JSON 報告
cat results/rl_evaluation_report.json

# 或用 Python 美化輸出
python -c "import json; r=json.load(open('results/rl_evaluation_report.json')); print(json.dumps(r, indent=2, ensure_ascii=False))"
```

### 步驟 4: 解讀評估指標

#### 核心指標

```json
{
  "收益指標": {
    "total_return": 總收益,
    "sharpe_ratio": Sharpe Ratio,        ← ⭐⭐⭐⭐⭐ 最重要!
    "max_drawdown": 最大回撤,
    "win_rate": 勝率
  },
  "交易統計": {
    "num_trades": 交易次數,
    "avg_trade_duration": 平均持倉時間,
    "transaction_costs": 交易成本
  }
}
```

#### 成功標準

| 指標 | 目標 | 優秀 | 說明 |
|------|------|------|------|
| **Sharpe Ratio** | > 2.0 | > 3.0 | 風險調整後收益 |
| **勝率** | > 55% | > 60% | 獲利交易比例 |
| **最大回撤** | < 10% | < 5% | 最大虧損幅度 |
| **交易次數** | 合理 | 10-50/天 | 高頻但不過度 |

---

## 根據評估結果的決策樹

### 情境 A: 優秀表現 (Sharpe > 2.5) ✅✅✅

**恭喜！策略成功！**

**下一步**:
1. 超參數優化 (學習率、Gamma、熵係數)
2. 增加訓練時間 (2M-5M steps)
3. 回測系統整合
4. 準備實盤測試

**執行**:
```bash
# 1. 超參數優化 (使用 Optuna)
python scripts/optimize_sb3_hyperparams.py

# 2. 更長時間訓練
python scripts/train_sb3_deeplob.py --timesteps 5000000

# 3. 回測
python scripts/backtest_sb3_strategy.py
```

---

### 情境 B: 良好表現 (Sharpe 2.0-2.5) ✅✅

**不錯！可以進一步優化**

**下一步**:
1. 微調獎勵函數權重
2. 調整學習率/Gamma
3. 嘗試不同 RL 算法 (A2C, SAC)

**執行**:
```bash
# 調整獎勵函數
# 編輯 src/envs/tw_lob_trading_env.py
# 修改 reward_shaper.py 中的權重

# 重新訓練
python scripts/train_sb3_deeplob.py \
    --timesteps 1000000 \
    --learning-rate 1e-4
```

---

### 情境 C: 中等表現 (Sharpe 1.5-2.0) ⚠️

**有改善空間**

**可能原因**:
1. DeepLOB 預測質量不足
2. 獎勵函數設計不當
3. RL 超參數未調優

**診斷步驟**:

```bash
# 1. 分析失敗案例
python scripts/analyze_failed_trades.py

# 2. 檢查 DeepLOB 預測分布
python scripts/analyze_deeplob_predictions.py

# 3. 可視化策略行為
python scripts/visualize_strategy.py
```

**改進方向**:
- 選項 1: 訓練 DeepLOB Exp-6 (降低正則化)
- 選項 2: 調整環境設計 (獎勵函數、狀態空間)
- 選項 3: 嘗試其他 RL 算法

---

### 情境 D: 表現不佳 (Sharpe < 1.5) ❌

**需要重大改進**

**診斷清單**:

1. **檢查 DeepLOB 預測**
   ```bash
   # 分桶評估
   python scripts/evaluate_deeplob_by_confidence.py \
       --checkpoint checkpoints/v5/deeplob_v5_best.pth
   ```

2. **檢查環境設計**
   ```bash
   # 驗證環境
   python scripts/verify_env.py

   # 檢查獎勵函數
   python scripts/test_reward_function.py
   ```

3. **檢查 RL 學習**
   ```bash
   # 查看學習曲線
   tensorboard --logdir logs/sb3_deeplob/
   ```

**改進方案** (按優先順序):

#### 方案 1: 訓練 DeepLOB Exp-6 ⭐⭐⭐⭐⭐

**問題**: 當前 DeepLOB 過度保守，不給高信心預測

**解決**: 降低正則化重新訓練

```bash
# 訓練 Exp-6 (2-3 小時)
python scripts/train_deeplob_v5.py \
    --config configs/train_v5_exp6.yaml

# 評估 Exp-6
python scripts/evaluate_deeplob_by_confidence.py \
    --checkpoint logs/deeplob_v5_exp6/最新運行/best_model.pth

# 預期: 高信心區 (≥0.8) 有 10-20% 樣本, 準確率 70%+

# 使用 Exp-6 重新訓練 RL
python scripts/train_sb3_deeplob.py \
    --timesteps 1000000 \
    --deeplob-checkpoint logs/deeplob_v5_exp6/最新運行/best_model.pth
```

#### 方案 2: 重新設計獎勵函數

**編輯**: `src/envs/reward_shaper.py`

```python
# 當前獎勵組成:
# 1. PnL 獎勵
# 2. 交易成本懲罰
# 3. 庫存懲罰
# 4. 風險調整項

# 可調整:
# - 增加 PnL 權重
# - 減少庫存懲罰
# - 加入趨勢跟隨獎勵
```

#### 方案 3: 簡化環境

```bash
# 使用更簡單的狀態空間
# 移除複雜特徵
# 編輯 src/envs/tw_lob_trading_env.py
```

---

## 備選方案：DeepLOB Exp-6

### 何時需要？

- RL Sharpe < 1.5
- 或想要更好的 DeepLOB 預測

### 配置變更

```yaml
V5 Exp-5 (當前):
  dropout: 0.78
  label_smoothing: 0.028
  weight_decay: 0.0029
  問題: 過度正則化，無高信心預測

V5 Exp-6 (新):
  dropout: 0.65          ← -17%
  label_smoothing: 0.01  ← -64%
  weight_decay: 0.002    ← -31%
  目標: 產生 10-20% 高信心預測
```

### 訓練步驟

```bash
# 1. 訓練 Exp-6
python scripts/train_deeplob_v5.py \
    --config configs/train_v5_exp6.yaml

# 2. 評估信心分布
python scripts/evaluate_deeplob_by_confidence.py \
    --checkpoint logs/deeplob_v5_exp6/最新運行/best_model.pth \
    --data-dir data/processed_v7/npz

# 3. 檢查結果
cat results/deeplob_confidence_eval/confidence_evaluation.json

# 預期:
# {
#   "high_confidence": {
#     "threshold": 0.8,
#     "n_samples": 16000-32000,  ← 10-20%
#     "accuracy": 0.70-0.80      ← 70-80%
#   }
# }
```

### 使用 Exp-6 重新訓練 RL

```bash
# 使用新的 DeepLOB 檢查點
python scripts/train_sb3_deeplob.py \
    --timesteps 1000000 \
    --deeplob-checkpoint logs/deeplob_v5_exp6/最新運行/best_model.pth \
    --output-dir checkpoints/sb3/ppo_deeplob_exp6
```

---

## 常見問題排解

### Q1: RL 訓練 Reward 不上升

**症狀**: Reward 停滯在負值，100K+ steps 無改善

**診斷**:
```bash
# 檢查學習曲線
tensorboard --logdir logs/sb3_deeplob/

# 查看 explained_variance
# 若 < 0: 價值函數學習失敗
```

**解決**:
```bash
# 降低學習率
python scripts/train_sb3_deeplob.py \
    --timesteps 1000000 \
    --learning-rate 1e-4  # 預設 3e-4

# 或增加 n_steps (更多經驗)
# 編輯 scripts/train_sb3_deeplob.py
# n_steps=2048 → 4096
```

### Q2: Policy Loss 爆炸

**症狀**: `train/loss` > 10.0, 訓練崩潰

**解決**:
```bash
# 降低學習率
--learning-rate 1e-5

# 減小 clip_range
# 編輯配置: clip_range=0.2 → 0.1
```

### Q3: 訓練中斷

**恢復訓練**:
```bash
# 從檢查點繼續
# 注意: 需要修改 train_sb3_deeplob.py 支援續訓
# 或直接重新開始 (PPO 訓練相對快)
```

### Q4: GPU 記憶體不足

**症狀**: CUDA out of memory

**解決**:
```bash
# 減小 batch_size
# 編輯 scripts/train_sb3_deeplob.py
# batch_size=64 → 32

# 或減少平行環境數
# n_envs=4 → 2
```

### Q5: 評估腳本報錯

**症狀**: `evaluate_sb3.py` 執行失敗

**檢查**:
```bash
# 確認模型檔案存在
ls -lh checkpoints/sb3/ppo_deeplob/best_model.zip

# 確認環境正確
python scripts/verify_env.py

# 測試載入模型
python -c "from stable_baselines3 import PPO; model = PPO.load('checkpoints/sb3/ppo_deeplob/best_model'); print('載入成功')"
```

---

## 完整指令速查

### 當前階段：等待訓練完成

```bash
# 檢查訓練進度
tail -f logs/sb3_deeplob/最新運行/progress.txt

# TensorBoard 監控
tensorboard --logdir logs/sb3_deeplob/
```

### 訓練完成後

```bash
# 1. 評估策略
python scripts/evaluate_sb3.py \
    --model checkpoints/sb3/ppo_deeplob/best_model \
    --n_episodes 20 \
    --save_report

# 2. 查看報告
cat results/rl_evaluation_report.json

# 3. 根據 Sharpe Ratio 決定下一步
```

### 如果需要改進 DeepLOB

```bash
# 訓練 Exp-6
python scripts/train_deeplob_v5.py --config configs/train_v5_exp6.yaml

# 評估 Exp-6
python scripts/evaluate_deeplob_by_confidence.py \
    --checkpoint logs/deeplob_v5_exp6/最新運行/best_model.pth

# 使用 Exp-6 重新訓練 RL
python scripts/train_sb3_deeplob.py \
    --timesteps 1000000 \
    --deeplob-checkpoint logs/deeplob_v5_exp6/最新運行/best_model.pth
```

---

## 時間規劃

### 今晚 (自動執行)
- ✅ RL 訓練 (1M steps, 4-8 小時)

### 明天早上 (10 分鐘)
1. 檢查訓練是否完成
2. 運行評估腳本
3. 查看 Sharpe Ratio

### 明天上午 (根據結果)

**如果 Sharpe > 2.0** ✅:
- 繼續優化 RL
- 準備回測

**如果 Sharpe < 1.5** ❌:
- 訓練 DeepLOB Exp-6 (2-3 小時)
- 重新訓練 RL (4-8 小時)

---

## 成功標準總結

### 階段性目標

| 階段 | 目標 | 標準 | 狀態 |
|------|------|------|------|
| DeepLOB 訓練 | 價格預測 | Val Acc > 50% | ✅ 50.24% |
| RL 快速測試 | 驗證可行性 | Reward 上升 | ✅ -183→-39.5 |
| RL 完整訓練 | 策略學習 | Reward > 0 | ⏳ 訓練中 |
| 策略評估 | 實戰可用 | Sharpe > 2.0 | ⏳ 待評估 |

### 最終目標

```
✅ Sharpe Ratio > 2.0
✅ 勝率 > 55%
✅ 最大回撤 < 10%
✅ 交易次數合理 (10-50/天)
✅ 回測穩定獲利
```

---

## 相關文檔

- [DeepLOB 訓練報告](1.DeepLOB 台股模型訓練最終報告.md)
- [調參歷史](20251025-deeplob調參歷史.md)
- [SB3 實作報告](SB3_IMPLEMENTATION_REPORT.md)
- [下一步執行計劃](../NEXT_STEPS.md)

---

**最後更新**: 2025-10-25
**當前版本**: v1.0
**作者**: Claude Code + User

**祝訓練順利！明天見！** 🚀
