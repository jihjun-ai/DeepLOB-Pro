# DeepLOB 調參歷史

### 每次保留數據及成果,簡要說明(其他廢話不用),這是改下次調參參考用

#### 數據產生:python scripts/train_deeplob_v5.py --config configs/train_v5_recovery.yaml --data-dir ./data/processed_v5_fixed/npz --epochs 50

#### 環境: conda activate deeplob-pro

---

## 實驗 #1 - 2025-10-20 19:33

**配置**: `configs/train_v5_recovery.yaml`
**數據**: `data/processed_v5_fixed/npz`

### 超參數
- LR: 0.00008, Cosine (warmup 0.15)
- Batch: 512, Dropout: 0.6
- Model: Conv 32x3, LSTM 64, FC 64
- Weight decay: 0.0005, Grad clip: 0.5
- Label smoothing: 0.15

### 結果 (9 epochs)
| Epoch | Train Loss | Val Loss | Train Acc | Val Acc | Val F1 | Grad |
|-------|------------|----------|-----------|---------|--------|------|
| 1     | 1.0470     | 1.0408   | 41.86%    | 40.46%  | 0.4025 | 1.50 |
| 2     | 1.0243     | 1.0354   | 44.04%    | **42.48%** | 0.4191 | 1.11 |
| 9     | 0.9230     | 1.1219   | 56.03%    | 38.57%  | 0.3868 | 2.52 |

### 問題
- ❌ **Epoch 2 後嚴重過擬合**
- ❌ Val Loss 爆炸: 1.0354 → 1.1219
- ❌ Train-Val Gap 持續擴大: +2% → +17%
- ❌ 梯度不穩定: 1.11 → 2.52

### 結論
**失敗** - Warmup 階段就過擬合，學習率過高

---

## 實驗 #2 - 待執行
