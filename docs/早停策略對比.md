# 早停策略對比分析

**問題**：實驗 4b 只訓練 3 個 epoch 就停止，可能過早。

**原因**：早停參數過於嚴格
- `Patience = 1`：只容忍 1 個 epoch 不改善
- `min_delta = 0.0005`：0.05% 的波動就觸發

---

## 📊 三種早停策略對比

### 策略 1：實驗 4b（過於嚴格）❌

```yaml
early_stop:
  patience: 1
  min_delta: 0.0005
```

**問題**：
```
Epoch 2: Val F1 = 0.4844 (最佳)
Epoch 3: Val F1 = 0.4810 (下降 0.0034 = 0.34%)
→ 觸發早停！（但可能只是正常波動）
```

**結果**：只訓練 3 個 epoch（太短）

---

### 策略 2：大幅放寬（推薦）⭐⭐⭐⭐⭐

```yaml
early_stop:
  patience: 5          # 1 → 5
  min_delta: 0.0001    # 0.0005 → 0.0001
```

**理念**：只在"嚴重惡化"時才停止

**觸發條件**：
- 連續 5 個 epoch 無改善（>0.01%）
- 或者驗證集嚴重惡化（<-1%）

**預期行為**：
```
Epoch 5: Val F1 = 0.50
Epoch 6: Val F1 = 0.49 (下降 1%, counter=1)
Epoch 7: Val F1 = 0.49 (持平, counter=2)
Epoch 8: Val F1 = 0.50 (回升, counter=0) ← 重置
Epoch 9: Val F1 = 0.48 (下降, counter=1)
...
Epoch 14: (counter=5) → 觸發早停
```

**優勢**：
- ✅ 允許正常波動（±1-2%）
- ✅ 避免過早停止
- ✅ 仍然防止嚴重惡化

**使用配置**：
- `train_v5.yaml`（激進版）
- `train_v5_conservative.yaml`（保守版）

---

### 策略 3：完全關閉早停（最激進）⭐⭐⭐

```yaml
early_stop:
  patience: 999        # 極大值，實際不觸發
  min_delta: 0.0       # 任何改善都算
```

**理念**：固定訓練 N 個 epoch，讓模型充分學習

**人工監控**（需要）：
1. **Train-Val Gap**：
   - <5%：✅ 健康
   - 5-10%：⚠️ 警告
   - >10%：🔥 手動停止

2. **Val Loss 趨勢**：
   - 持續下降：✅ 繼續
   - 連續 5 epoch 上升：⚠️ 考慮停止

3. **Grad Norm**：
   - <5.0：✅ 穩定
   - >5.0：🔥 梯度爆炸，立即停止

**優勢**：
- ✅ 完全不受早停限制
- ✅ 可觀察完整學習曲線
- ✅ 找到真正最佳點

**劣勢**：
- ⚠️ 需要人工監控
- ⚠️ 可能訓練過久（浪費時間）
- ⚠️ 可能過擬合

**使用配置**：
- `train_v5_no_earlystop.yaml`

---

## 🎯 推薦使用順序

### 第一輪：保守版 + 放寬早停

```bash
python scripts/train_deeplob_v5.py \
  --config configs/train_v5_conservative.yaml \
  --data-dir data/processed_v7/npz
```

**配置**：
- 容量：32-32-32（不變）
- Patience：5（寬容）
- Epochs：15
- 預期時間：5-7 分鐘

**目標**：證明是否為"訓練不足"問題
- 成功（>55%）：不是容量問題 ✅
- 失敗（<53%）：考慮增大容量

---

### 第二輪：激進版 + 放寬早停

```bash
python scripts/train_deeplob_v5.py \
  --config configs/train_v5.yaml \
  --data-dir data/processed_v7/npz
```

**配置**：
- 容量：48-64-64（增大）
- Patience：5（寬容）
- Epochs：20
- 預期時間：10-15 分鐘

**目標**：突破 60%
- 成功（>60%）：目標達成 ✅
- 失敗（<58%）：考慮其他策略

---

### 第三輪：完全關閉早停（如果需要）

```bash
python scripts/train_deeplob_v5.py \
  --config configs/train_v5_no_earlystop.yaml \
  --data-dir data/processed_v7/npz
```

**配置**：
- 容量：48-64-64（增大）
- Patience：999（關閉）
- Epochs：15（固定）
- 預期時間：10 分鐘

**目標**：觀察完整學習曲線
- 找到真正的最佳 epoch
- 分析過擬合時間點

---

## 📋 早停參數對比表

| 配置 | Patience | min_delta | Epochs | 容量 | 預期時間 | 預期 Test Acc |
|------|----------|-----------|--------|------|----------|---------------|
| **實驗 4b** | 1 | 0.0005 | 10 | 32-32-32 | 2 分鐘 | 51.14% |
| **保守版** | 5 | 0.0001 | 15 | 32-32-32 | 5-7 分鐘 | 55-58% |
| **激進版** | 5 | 0.0001 | 20 | 48-64-64 | 10-15 分鐘 | >60% |
| **關閉早停** | 999 | 0.0 | 15 | 48-64-64 | 10 分鐘 | >60% |

---

## 💡 早停的哲學

### ❌ 錯誤觀念
> "驗證集性能一下降就停止"

**問題**：
- 正常訓練會有波動（±1-2%）
- 可能只是隨機噪音，不是真正惡化
- 過早停止會錯失最佳點

### ✅ 正確觀念
> "只在嚴重且持續惡化時才停止"

**理由**：
- 允許正常波動
- 給模型更多學習時間
- 找到真正的收斂點

---

## 📊 實驗 4b 的早停分析

### 訓練過程

```
Epoch 1: Val F1 = 0.4778 (baseline)
Epoch 2: Val F1 = 0.4844 (改善 0.66%, 最佳點)
Epoch 3: Val F1 = 0.4810 (下降 0.34%, counter=1)
→ Patience=1 觸發早停
```

### 問題分析

**下降幅度**：0.34%（0.0034）
- 這是**正常波動**，不是惡化！
- 驗證集本身就有隨機性（±1-2%）

**過早停止**：
- 只訓練 3 個 epoch
- Train Acc 才 50.08%（訓練集都沒學好）
- 可能 Epoch 5-10 會更好

### 如果用放寬早停

```
Epoch 1: Val F1 = 0.4778
Epoch 2: Val F1 = 0.4844 (最佳)
Epoch 3: Val F1 = 0.4810 (counter=1)
Epoch 4: Val F1 = 0.4850 (回升, counter=0) ← 重置，找到更好的點！
Epoch 5: Val F1 = 0.4920 (繼續改善)
...
Epoch 10: (可能達到 52-55%)
```

---

## 🎯 建議

### 對於調參實驗

**使用策略 2（放寬早停）**：
- `Patience = 5`
- `min_delta = 0.0001`

**理由**：
- ✅ 平衡效率和充分訓練
- ✅ 避免過早停止
- ✅ 仍然防止嚴重惡化

---

### 對於最終訓練

**使用策略 3（關閉早停）**：
- `Patience = 999`
- 固定訓練 15-20 個 epoch
- 人工監控，手動選擇最佳檢查點

**理由**：
- ✅ 確保找到最佳點
- ✅ 可分析完整學習曲線
- ✅ 避免錯失性能

---

## 📝 總結

**早停的目的**：
- ❌ 不是"一下降就停止"
- ✅ 是"防止嚴重且持續的惡化"

**實驗 4b 的教訓**：
- Patience=1 太嚴格（只容忍 1 個 epoch）
- min_delta=0.0005 太敏感（0.05% 就觸發）
- 結果：3 個 epoch 就停止（太早）

**修正策略**：
- Patience=5（容忍 5 個 epoch 波動）
- min_delta=0.0001（幾乎不限制）
- 結果：預計訓練 8-15 個 epoch（更充分）

---

**最後更新**: 2025-10-24
**狀態**: 已更新所有配置文件
