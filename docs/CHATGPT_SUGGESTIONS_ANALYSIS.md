# ChatGPT å»ºè­°åˆ†æå ±å‘Š

**æ—¥æœŸ**: 2025-10-21
**åˆ†æè€…**: Claude (Sonnet 4.5)
**ç›®çš„**: è©•ä¼° 7 é …å»ºè­°çš„å¿…è¦æ€§ã€å¯è¡Œæ€§å’Œå„ªå…ˆç´š

---

## ç¸½è¦½

| å»ºè­° | ChatGPT å„ªå…ˆç´š | æˆ‘çš„è©•ä¼° | æ¡ç´å»ºè­° | ç†ç”± |
|------|---------------|----------|---------|------|
| A. ffill ä¸Šé™ç®¡æ§ | ä¸­åº¦ | **é«˜å„ªå…ˆ** | âœ… **æ¡ç´** | å·²æœ‰éƒ¨åˆ†å¯¦ç¾ï¼Œéœ€è£œå¼·ç›£æ§ |
| B. å‹•æ…‹éæ¿¾é€€åŒ–ä¿è­· | ä¸­åº¦ | **ä¸­å„ªå…ˆ** | âš ï¸ **éƒ¨åˆ†æ¡ç´** | æ¥µç«¯æƒ…æ³ç½•è¦‹ï¼Œå¯è§€å¯Ÿå¾Œæ±ºå®š |
| C. åˆ‡åˆ†ç©©å®šæ€§ | é«˜å„ªå…ˆ | **é«˜å„ªå…ˆ** | âœ… **ç«‹å³æ¡ç´** | é—œéµå•é¡Œï¼Œå·²æœ‰ seed ä½†éœ€é©—è­‰ |
| D. é›¶åƒ¹çµ±è¨ˆä¸€è‡´æ€§ | ä¸­å„ªå…ˆ | **ä¸­å„ªå…ˆ** | âœ… **æ¡ç´** | å¯©è¨ˆéœ€æ±‚ï¼Œæ˜“å¯¦ç¾ |
| E. æ¨™ç±¤é‚Šç•Œæª¢æŸ¥ | ä¸­å„ªå…ˆ | **é«˜å„ªå…ˆ** | âœ… **ç«‹å³æ¡ç´** | æ•¸æ“šå®Œæ•´æ€§é—œéµ |
| F. è¨ˆç®—è³‡æºå„ªåŒ– | ä½å„ªå…ˆ | **ä½å„ªå…ˆ** | â³ **å»¶å¾Œ** | ç•¶å‰è¦æ¨¡ç„¡ç“¶é ¸ |
| G. æ‰¹æ¬¡å™¨å¥å£¯æ€§ | ä½å„ªå…ˆ | **ä¸­å„ªå…ˆ** | âœ… **æ¡ç´** | Windows ç’°å¢ƒé‡è¦ |

---

## A. ffill ä¸Šé™èˆ‡é•·ç¼ºå£ç®¡æ§

### ChatGPT å»ºè­°
- **å•é¡Œ**: ç›¤ä¸­é•·æ™‚é–“ç„¡å ±åƒ¹ï¼Œffill æŠŠèˆŠç‹€æ…‹å»¶ä¼¸åˆ°ç¾åœ¨ â†’ å‡è¨Šè™Ÿ
- **å»ºè­°**:
  1. `ffill_max_gap` è¨­å®šä¸Šé™ï¼ˆ30-60 ç§’ï¼‰
  2. è¶…éä¸Šé™æ¨™è¨˜ç‚ºç¼ºå¤±
  3. metadata è¨˜éŒ„ `max_gap_sec`, `ffill_ratio`
  4. æ»‘çª—æ™‚æ’é™¤ã€Œç¼ºå¤±å æ¯”éé«˜ã€çš„çª—å£

### ç¾ç‹€æª¢æŸ¥ âœ… å·²æœ‰åŸºç¤å¯¦ç¾

**preprocess_single_day.py:233**:
```python
def aggregate_to_1hz(seq, reducer='last', ffill_limit=120):
    """
    ffill_limit: å‰å€¼å¡«è£œæœ€å¤§é–“éš”ï¼ˆç§’ï¼‰
    """
```

**å·²å¯¦ç¾**:
- âœ… `ffill_limit=120` ç§’ï¼ˆ2 åˆ†é˜ï¼‰
- âœ… `bucket_mask` æ¨™è¨˜ {0: å–®äº‹ä»¶, 1: ffill, 2: ç¼ºå¤±, 3: å¤šäº‹ä»¶}
- âœ… metadata è¨˜éŒ„ `ffill_ratio`, `missing_ratio`, `max_gap_sec`

**ç•¶å‰å€¼**ï¼ˆä¾†è‡ª load_npz.py è¼¸å‡ºï¼‰:
```
mask=1 (ffill): 27.4%
mask=2 (missing): 1.1%
max_gap_sec: 181 ç§’
```

### è©•ä¼°

#### âœ… å„ªé»ï¼ˆå·²å¯¦ç¾éƒ¨åˆ†ï¼‰
1. **ffill_limit å·²è¨­å®š**: 120 ç§’æ˜¯åˆç†å€¼ï¼ˆå°è‚¡ç›¤ä¸­å†·é–€è‚¡å¯èƒ½æ•¸åˆ†é˜ç„¡å ±åƒ¹ï¼‰
2. **mask æ©Ÿåˆ¶å¥å…¨**: æ¸…æ¥šå€åˆ†åŸå§‹/ffill/ç¼ºå¤±/èšåˆ
3. **çµ±è¨ˆå·²è¨˜éŒ„**: metadata åŒ…å«æ‰€éœ€è³‡è¨Š

#### âš ï¸ ä¸è¶³ä¹‹è™•
1. **ç¼ºå°‘æ»‘çª—éšæ®µçš„å“è³ªéæ¿¾**:
   - ç•¶å‰ï¼šç”Ÿæˆæ‰€æœ‰æ»‘çª—ï¼Œä¸ç®¡ç¼ºå¤±å æ¯”
   - å»ºè­°ï¼šè‹¥çª—å£å…§ `mask=1` (ffill) å æ¯” > 50%ï¼Œæ‡‰æ¨™è¨˜ä½è³ªé‡

2. **ffill_limit åå¤§**:
   - 120 ç§’ = 2 åˆ†é˜ï¼Œå°é«˜é »äº¤æ˜“åé•·
   - å»ºè­°èª¿æ•´ç‚º **60 ç§’**ï¼ˆ1 åˆ†é˜ï¼‰

3. **ç¼ºå°‘ç•°å¸¸ç›£æ§**:
   - max_gap_sec=181 ç§’ï¼ˆ3 åˆ†é˜ï¼‰è¶…é ffill_limit=120 ç§’
   - æ‡‰åœ¨ summary.json ä¸­æ¨™è¨»ã€Œé•·ç¼ºå£è‚¡ç¥¨ã€

### å»ºè­°è¡Œå‹• âœ… **æ¡ç´**

#### å„ªå…ˆç´šï¼š**é«˜**

#### å¯¦ç¾æ–¹æ¡ˆ

**éšæ®µ 1: é è™•ç†éšæ®µ**ï¼ˆå·²å®Œæˆ âœ…ï¼‰
```python
# preprocess_single_day.py
# ç•¶å‰å·²è¨˜éŒ„ï¼š
vol_stats['ffill_ratio'] = float((bucket_mask == 1).sum() / len(bucket_mask))
vol_stats['max_gap_sec'] = int(np.max(np.diff(...)))
```

**éšæ®µ 2: è¨“ç·´æ•¸æ“šç”Ÿæˆéšæ®µ**ï¼ˆéœ€è£œå……ï¼‰
```python
# extract_tw_stock_data_v6.py - æ»‘çª—ç”Ÿæˆæ™‚
for t in range(SEQ_LEN - 1, max_t):
    window_mask = bucket_mask[window_start:t+1]

    # æ–°å¢ï¼šå“è³ªæª¢æŸ¥
    ffill_ratio = (window_mask == 1).sum() / len(window_mask)
    if ffill_ratio > 0.5:  # è¶…é 50% æ˜¯ ffill
        # é¸é … 1: è·³éï¼ˆæ¨è–¦ï¼‰
        continue
        # é¸é … 2: é™æ¬Š
        # weight *= 0.5
```

**éšæ®µ 3: æ‘˜è¦å ±å‘Š**ï¼ˆéœ€è£œå……ï¼‰
```python
# preprocess_single_day.py - summary.json
"data_quality": {
    "long_gap_symbols": [  # æ–°å¢
        {"symbol": "3048", "max_gap_sec": 181},
        ...
    ],
    "high_ffill_symbols": [  # æ–°å¢
        {"symbol": "2330", "ffill_ratio": 0.45},
        ...
    ]
}
```

#### æ™‚ç¨‹
- **ç«‹å³**: é™ä½ ffill_limit å¾ 120 â†’ 60 ç§’
- **æœ¬é€±**: æ·»åŠ æ»‘çª—å“è³ªéæ¿¾
- **ä¸‹é€±**: å¢å¼·æ‘˜è¦å ±å‘Š

---

## B. å‹•æ…‹éæ¿¾çš„ã€Œé€€åŒ–ä¿è­·ã€

### ChatGPT å»ºè­°
- **å•é¡Œ**: æ¥µç«¯è¡Œæƒ…æ—¥ï¼ˆå¤§å¤šæ•¸æ¨™çš„åœæ»¯ï¼‰ï¼Œåˆ†ä½æ•¸é›†ä¸­ â†’ éæ¿¾å¤±æ•ˆ
- **å»ºè­°**:
  1. è‹¥ `IQR < 1e-6`ï¼Œå›é€€åˆ°å¯¬é¬†éæ¿¾ï¼ˆP10 æˆ– noneï¼‰
  2. summary.json è¨˜éŒ„ `filter_confidence`, `range_iqr`

### ç¾ç‹€æª¢æŸ¥

**preprocess_single_day.py:515-564**:
```python
def determine_adaptive_threshold(daily_stats, config):
    range_values = [s['range_pct'] for s in daily_stats]

    candidates = {
        'P10': np.percentile(range_values, 10),
        'P25': np.percentile(range_values, 25),
        'P50': np.percentile(range_values, 50),
        'none': 0.0
    }

    # é¸æ“‡æœ€æ¥è¿‘ç›®æ¨™åˆ†å¸ƒçš„é–¾å€¼
```

### è©•ä¼°

#### âœ… å„ªé»ï¼ˆç¾æœ‰è¨­è¨ˆï¼‰
1. **å¤šå€™é¸é–¾å€¼**: P10/P25/P50/noneï¼Œæœ‰é€€è·¯
2. **ç›®æ¨™åˆ†å¸ƒé©…å‹•**: é¸æ“‡æœ€æ¥è¿‘ 30/40/30 çš„é–¾å€¼
3. **å·²æœ‰ 'none' é¸é …**: æ¥µç«¯æƒ…æ³å¯ä»¥ä¸éæ¿¾

#### âš ï¸ æ½›åœ¨å•é¡Œ
1. **æ¥µç«¯è¡Œæƒ…æ¸¬è©¦ä¸è¶³**:
   - ä¾‹å¦‚ï¼šæ¼²è·Œåœæ—¥ã€ç†”æ–·æ—¥
   - æ‰€æœ‰è‚¡ç¥¨ range_pct â‰ˆ 0.1ï¼ˆæ¼²åœï¼‰æˆ– â‰ˆ 0ï¼ˆåœç‰Œï¼‰

2. **IQR æª¢æŸ¥ç¼ºå¤±**:
   - ç•¶å‰æ²’æœ‰æª¢æ¸¬åˆ†ä½æ•¸é‡ç–Šçš„é‚è¼¯

#### ğŸ¤” å¯¦éš›ç™¼ç”Ÿæ¦‚ç‡
- **å°è‚¡å¯¦æ³**:
  - å…¨é¢æ¼²è·Œåœï¼šæ¥µç½•è¦‹ï¼ˆ1997 äºæ´²é‡‘èé¢¨æš´ã€2020 ç–«æƒ…æš´è·Œï¼‰
  - ç•¶å‰æœ‰ç†”æ–·æ©Ÿåˆ¶ï¼Œé›£ä»¥å‡ºç¾æ¥µç«¯æƒ…æ³
  - å³ä½¿æ¥µç«¯æ—¥ï¼Œä»æœ‰éƒ¨åˆ†è‚¡ç¥¨æ³¢å‹•ï¼ˆæ¬Šå€¼è‚¡ã€æ¦‚å¿µè‚¡ï¼‰

- **æ•¸æ“šæœŸé–“**:
  - ç•¶å‰æ•¸æ“šï¼š2025-09-01 ~ 2025-09-10ï¼ˆ10 å¤©ï¼‰
  - æ¥µç«¯æƒ…æ³å‡ºç¾æ¦‚ç‡ï¼š< 0.1%

### å»ºè­°è¡Œå‹• âš ï¸ **éƒ¨åˆ†æ¡ç´**

#### å„ªå…ˆç´šï¼š**ä¸­ä½**ï¼ˆè§€å¯Ÿå¾Œæ±ºå®šï¼‰

#### å¯¦ç¾æ–¹æ¡ˆï¼ˆè¼•é‡ç´šï¼‰

**æ–¹æ¡ˆ 1: ä¿å®ˆç›£æ§ï¼ˆæ¨è–¦ï¼‰**
```python
# preprocess_single_day.py - determine_adaptive_threshold()
range_values = [s['range_pct'] for s in daily_stats]

# è¨ˆç®— IQR
q1, q3 = np.percentile(range_values, [25, 75])
iqr = q3 - q1

# è¨˜éŒ„ä½†ä¸å¹²é ï¼ˆå…ˆè§€å¯Ÿï¼‰
if iqr < 0.001:  # 0.1% æ³¢å‹•
    logging.warning(
        f"âš ï¸ ç•¶æ—¥æ³¢å‹•æ¥µä½: IQR={iqr:.6f}\n"
        f"   é€™å¯èƒ½æ˜¯æ¥µç«¯è¡Œæƒ…æ—¥ï¼ˆåœç‰Œ/æ¼²è·Œåœï¼‰\n"
        f"   å»ºè­°æ‰‹å‹•æª¢æŸ¥éæ¿¾çµæœ"
    )

# å¯«å…¥ summary.json
summary['filter_confidence'] = {
    'iqr': float(iqr),
    'warning': 'low_volatility' if iqr < 0.001 else 'normal'
}
```

**æ–¹æ¡ˆ 2: ä¸»å‹•å›é€€ï¼ˆè‹¥ç¢ºèªéœ€è¦ï¼‰**
```python
if iqr < 0.0005:  # æ¥µç«¯é–¾å€¼
    logging.warning(f"âš ï¸ IQR={iqr:.6f} éä½ï¼Œå¼·åˆ¶ä½¿ç”¨ 'none' éæ¿¾")
    return 0.0, 'none_forced', {'down': 0.33, 'neutral': 0.34, 'up': 0.33}
```

#### æ™‚ç¨‹
- **æœ¬é€±**: æ·»åŠ  IQR ç›£æ§å’Œæ—¥èªŒ
- **è§€å¯Ÿ 1-2 å€‹æœˆ**: æ˜¯å¦å‡ºç¾æ¥µç«¯æƒ…æ³
- **è‹¥é »ç¹å‡ºç¾**: å¯¦ç¾ä¸»å‹•å›é€€é‚è¼¯

#### çµè«–
**æš«ä¸å¯¦ç¾ä¸»å‹•å›é€€**ï¼ŒåŸå› ï¼š
1. æ¥µç«¯æƒ…æ³ç½•è¦‹ï¼ˆ< 0.1%ï¼‰
2. ç¾æœ‰ 'none' é¸é …å·²æä¾›é€€è·¯
3. éæ—©å„ªåŒ–å¯èƒ½å¼•å…¥è¤‡é›œåº¦
4. å»ºè­°å…ˆç›£æ§ï¼Œç´¯ç©æ•¸æ“šå¾Œæ±ºå®š

---

## C. åˆ‡åˆ†ç©©å®šæ€§èˆ‡éš¨æ©Ÿç¨®å­

### ChatGPT å»ºè­°
- **å•é¡Œ**: æŒ‰è‚¡ç¥¨æ•¸åˆ‡åˆ† 70/15/15 æ²’å•é¡Œï¼Œä½†è‹¥æ²’å›ºå®š seedï¼Œé‡è·‘æœƒæŠ–å‹•
- **å»ºè­°**:
  1. æ˜ç¢ºè¨­å®š `random.seed(seed)` æˆ– `np.random.default_rng(seed)`
  2. å°‡ seed å¯«å…¥ `normalization_meta.json`
  3. æ¸¬è©¦ï¼šå…©æ¬¡é‡è·‘ stock_ids çš„ split ä¸€è‡´

### ç¾ç‹€æª¢æŸ¥ âœ… å·²æœ‰å¯¦ç¾ä½†éœ€é©—è­‰

**extract_tw_stock_data_v6.py:413-420**:
```python
# æ­¥é©Ÿ 3: æŒ‰è‚¡ç¥¨åˆ‡åˆ† 70/15/15
import random
SPLIT_SEED = config.get('split', {}).get('seed', 42)
random.Random(SPLIT_SEED).shuffle(valid_stocks)

n_train = max(1, int(n_stocks * config['split']['train_ratio']))
n_val = max(1, int(n_stocks * config['split']['val_ratio']))
```

**config_pro_v5_ml_optimal.yaml:94**:
```yaml
split:
  train_ratio: 0.7
  val_ratio: 0.15
  test_ratio: 0.15
  seed: 42  # âœ… å·²è¨­å®š
```

**normalization_meta.json** (éœ€æª¢æŸ¥æ˜¯å¦å·²å¯«å…¥):
```json
{
  "data_split": {
    "method": "by_stock_count",
    "total_stocks": 250,
    // â“ ç¼ºå°‘ seed è¨˜éŒ„
  }
}
```

### è©•ä¼°

#### âœ… å„ªé»
1. **seed å·²è¨­å®š**: `seed: 42`
2. **ä½¿ç”¨ random.Random(seed)**: æ­£ç¢ºçš„éš”é›¢ç”¨æ³•
3. **ç¢ºå®šæ€§åˆ‡åˆ†**: ç†è«–ä¸Šå¯å¾©ç¾

#### âš ï¸ æ½›åœ¨å•é¡Œ
1. **seed æœªå¯«å…¥ metadata**:
   - ç„¡æ³•å¾è¼¸å‡ºåæ¨ä½¿ç”¨çš„ seed
   - è‹¥é…ç½®æ–‡ä»¶ä¸Ÿå¤±ï¼Œç„¡æ³•å¾©ç¾

2. **æœªæ¸¬è©¦å¾©ç¾æ€§**:
   - ç¼ºå°‘å“ˆå¸Œæ ¡é©—æˆ–å–®å…ƒæ¸¬è©¦
   - ç„¡æ³•ç¢ºèªå…©æ¬¡é‹è¡Œ split å®Œå…¨ä¸€è‡´

3. **å¤šè™•éš¨æ©Ÿæ€§**:
   - `random.shuffle()` - è‚¡ç¥¨åˆ‡åˆ† âœ…ï¼ˆå·²å›ºå®šï¼‰
   - NumPy éš¨æ©Ÿæ“ä½œï¼ˆè‹¥æœ‰ï¼‰- â“ï¼ˆéœ€æª¢æŸ¥ï¼‰

### å»ºè­°è¡Œå‹• âœ… **ç«‹å³æ¡ç´**

#### å„ªå…ˆç´šï¼š**é«˜**ï¼ˆé—œéµå¯å¾©ç¾æ€§ï¼‰

#### å¯¦ç¾æ–¹æ¡ˆ

**Step 1: è¨˜éŒ„ seed åˆ° metadata**ï¼ˆç°¡å–®ï¼‰
```python
# extract_tw_stock_data_v6.py - meta å­—å…¸
meta = {
    ...
    "data_split": {
        "method": "by_stock_count",
        "seed": SPLIT_SEED,  # æ–°å¢
        "train_stocks": len(train_stocks),
        ...
    }
}
```

**Step 2: æ·»åŠ å¾©ç¾æ€§æ¸¬è©¦**ï¼ˆæ¨è–¦ï¼‰
```python
# tests/test_reproducibility.py
def test_split_reproducibility():
    """æ¸¬è©¦å…©æ¬¡é‹è¡Œ split ä¸€è‡´"""
    # é‹è¡Œå…©æ¬¡
    split1 = run_pipeline(seed=42)
    split2 = run_pipeline(seed=42)

    # é©—è­‰ stock_ids ä¸€è‡´
    assert split1['train_symbols'] == split2['train_symbols']
    assert split1['val_symbols'] == split2['val_symbols']

    # å“ˆå¸Œæ ¡é©—
    hash1 = hashlib.md5(str(split1['train_symbols']).encode()).hexdigest()
    hash2 = hashlib.md5(str(split2['train_symbols']).encode()).hexdigest()
    assert hash1 == hash2
```

**Step 3: æ–‡æª”åŒ– seed ä½¿ç”¨**
```markdown
# å¯å¾©ç¾æ€§ä¿è­‰

æœ¬å°ˆæ¡ˆä½¿ç”¨å›ºå®šéš¨æ©Ÿç¨®å­ç¢ºä¿çµæœå¯å¾©ç¾ï¼š

- **é è™•ç†éšæ®µ**: ç„¡éš¨æ©Ÿæ€§
- **è¨“ç·´æ•¸æ“šç”Ÿæˆéšæ®µ**: seed=42ï¼ˆè‚¡ç¥¨åˆ‡åˆ†ï¼‰
- **æ¨¡å‹è¨“ç·´éšæ®µ**: seed=42ï¼ˆPyTorch, NumPyï¼‰

é‡æ–°é‹è¡Œæ™‚ï¼Œç¢ºä¿ï¼š
1. ä½¿ç”¨ç›¸åŒé…ç½®æ–‡ä»¶ï¼ˆseed=42ï¼‰
2. ä½¿ç”¨ç›¸åŒé è™•ç†æ•¸æ“š
3. Python/NumPy/PyTorch ç‰ˆæœ¬ä¸€è‡´
```

#### æ™‚ç¨‹
- **ç«‹å³**: è¨˜éŒ„ seed åˆ° metadataï¼ˆ5 åˆ†é˜ï¼‰
- **æœ¬é€±**: æ·»åŠ å¾©ç¾æ€§æ¸¬è©¦
- **æœ¬é€±**: é©—è­‰å…©æ¬¡é‹è¡Œ split ä¸€è‡´

---

## D. é›¶åƒ¹/ç•°å¸¸åƒ¹è™•ç½®ä¸€è‡´æ€§

### ChatGPT å»ºè­°
- **å•é¡Œ**: ç•°å¸¸åƒ¹è™•ç†è¦å‰‡å­˜åœ¨ï¼Œä½†çµ±è¨ˆä¸å®Œæ•´
- **å»ºè­°**:
  1. å¢åŠ ç•°å¸¸çµ±è¨ˆå­—æ®µï¼š`zero_price_rows`, `outlier_price_rows`
  2. å¯«å…¥æ¯æ—¥ summary.json
  3. æä¾›ã€Œè¢«å‰”é™¤/ä¿®æ­£çš„ç•°å¸¸æ¯”ä¾‹ã€
  4. è‹¥ç™¼ç¾ç•°å¸¸é›†ä¸­ï¼ŒåŠ ç™½åå–®/é»‘åå–®ï¼ˆå¯é¸ï¼‰

### ç¾ç‹€æª¢æŸ¥

**preprocess_single_day.py:179-181**ï¼ˆç•°å¸¸æª¢æ¸¬ï¼‰:
```python
# é›¶å€¼è™•ç†
for p, q in zip(bids_p + asks_p, bids_q + asks_q):
    if p == 0.0 and q != 0.0:
        return (sym, t, None)  # ç›´æ¥ä¸Ÿæ£„
```

**preprocess_single_day.py:189-192**ï¼ˆåƒ¹æ ¼é™åˆ¶ï¼‰:
```python
# åƒ¹æ ¼é™åˆ¶æª¢æŸ¥
prices_to_check = [p for p in bids_p + asks_p if p > 0]
if not all(within_limits(p, lower, upper) for p in prices_to_check):
    return (sym, t, None)  # ç›´æ¥ä¸Ÿæ£„
```

**ç•¶å‰çµ±è¨ˆ**ï¼ˆå…¨åŸŸï¼‰:
```python
stats = {
    "total_raw_events": 0,
    "cleaned_events": 0,  # åªæœ‰ç¸½æ•¸ï¼Œæ²’æœ‰åˆ†é¡
    ...
}
```

### è©•ä¼°

#### âœ… å„ªé»
1. **ç•°å¸¸æª¢æ¸¬å®Œæ•´**: é›¶åƒ¹ã€åƒ¹æ ¼é™åˆ¶ã€åƒ¹å·®æª¢æŸ¥
2. **è™•ç†ä¸€è‡´**: å…¨éƒ¨ç›´æ¥ä¸Ÿæ£„ï¼ˆä¸ä¿®è£œï¼‰
3. **å…¨åŸŸçµ±è¨ˆ**: æœ‰ç¸½æ¸…æ´—æ•¸

#### âš ï¸ ä¸è¶³
1. **ç¼ºå°‘åˆ†é¡çµ±è¨ˆ**:
   - ä¸çŸ¥é“ä¸Ÿäº†å¤šå°‘ã€Œé›¶åƒ¹ã€
   - ä¸çŸ¥é“ä¸Ÿäº†å¤šå°‘ã€Œè¶…é™åƒ¹ã€
   - ä¸çŸ¥é“ä¸Ÿäº†å¤šå°‘ã€Œåƒ¹å·®ç•°å¸¸ã€

2. **ç„¡æŒ‰è‚¡ç¥¨çµ±è¨ˆ**:
   - ç„¡æ³•è­˜åˆ¥ã€Œç‰¹å®šè‚¡ç¥¨ç•°å¸¸å¤šã€
   - ç„¡æ³•å¯©è¨ˆç•°å¸¸é›†ä¸­ç¾è±¡

3. **summary.json ä¸å®Œæ•´**:
   - ç•¶å‰åªæœ‰é€šé/éæ¿¾è‚¡ç¥¨æ•¸
   - æ²’æœ‰ç•°å¸¸ç´°ç¯€

### å»ºè­°è¡Œå‹• âœ… **æ¡ç´**

#### å„ªå…ˆç´šï¼š**ä¸­**ï¼ˆå¯©è¨ˆéœ€æ±‚ï¼Œæ˜“å¯¦ç¾ï¼‰

#### å¯¦ç¾æ–¹æ¡ˆ

**Step 1: æ“´å±•å…¨åŸŸçµ±è¨ˆ**
```python
# preprocess_single_day.py
stats = {
    "total_raw_events": 0,
    "cleaned_events": 0,

    # æ–°å¢ï¼šç•°å¸¸åˆ†é¡
    "rejected_zero_price": 0,
    "rejected_out_of_limit": 0,
    "rejected_bad_spread": 0,
    "rejected_trial": 0,
    "rejected_out_of_time": 0,
}
```

**Step 2: ä¿®æ”¹ parse_line()**
```python
def parse_line(raw: str):
    ...
    # è©¦æ’®ç§»é™¤
    if parts[IDX_TRIAL].strip() == "1":
        stats["rejected_trial"] += 1  # æ–°å¢
        return (sym, t, None)

    # æ™‚é–“çª—æª¢æŸ¥
    if not is_in_trading_window(t):
        stats["rejected_out_of_time"] += 1  # æ–°å¢
        return (sym, t, None)

    # åƒ¹å·®æª¢æŸ¥
    if not spread_ok(bid1, ask1):
        stats["rejected_bad_spread"] += 1  # æ–°å¢
        return (sym, t, None)

    # é›¶å€¼æª¢æŸ¥
    for p, q in zip(...):
        if p == 0.0 and q != 0.0:
            stats["rejected_zero_price"] += 1  # æ–°å¢
            return (sym, t, None)

    # åƒ¹æ ¼é™åˆ¶
    if not all(within_limits(...)):
        stats["rejected_out_of_limit"] += 1  # æ–°å¢
        return (sym, t, None)
```

**Step 3: æ“´å±• summary.json**
```python
# preprocess_single_day.py - generate_daily_summary()
summary = {
    ...
    "data_quality": {
        "total_raw_events": stats['total_raw_events'],
        "cleaned_events": stats['cleaned_events'],
        "rejection_breakdown": {
            "trial": stats['rejected_trial'],
            "out_of_time": stats['rejected_out_of_time'],
            "bad_spread": stats['rejected_bad_spread'],
            "zero_price": stats['rejected_zero_price'],
            "out_of_limit": stats['rejected_out_of_limit'],
        },
        "rejection_rate": {
            "total": 1 - stats['cleaned_events'] / stats['total_raw_events'],
            "zero_price_pct": stats['rejected_zero_price'] / stats['total_raw_events'],
            ...
        }
    }
}
```

**Step 4: æŒ‰è‚¡ç¥¨ç•°å¸¸çµ±è¨ˆ**ï¼ˆå¯é¸ï¼Œç¬¬äºŒéšæ®µï¼‰
```python
# è‹¥ç™¼ç¾ç•°å¸¸é›†ä¸­
"abnormal_symbols": [
    {
        "symbol": "3048",
        "zero_price_ratio": 0.25,  # 25% çš„äº‹ä»¶æ˜¯é›¶åƒ¹
        "warning": "high_zero_price_rate"
    }
]
```

#### æ™‚ç¨‹
- **æœ¬é€±**: å¯¦ç¾ Step 1-3ï¼ˆåˆ†é¡çµ±è¨ˆå’Œ summary.jsonï¼‰
- **ä¸‹é€±**: è§€å¯Ÿ summary.jsonï¼Œæ±ºå®šæ˜¯å¦éœ€è¦ Step 4

---

## E. æ¨™ç±¤èˆ‡æ¬Šé‡çš„é‚Šç•Œæª¢æŸ¥

### ChatGPT å»ºè­°
- **å•é¡Œ**: æ¨™ç±¤æ˜ å°„å’Œæ¬Šé‡è£å‰ªå·²å¯¦ç¾ï¼Œä½†ç¼ºå°‘ç¡¬æ€§æª¢æŸ¥
- **å»ºè­°**:
  1. Hard assertionï¼š`y âˆˆ {0, 1, 2}`
  2. Hard assertionï¼š`weights` å…¨éƒ¨æœ‰é™ï¼Œå‡å€¼ â‰ˆ 1
  3. Neutral ç›£æ§ï¼š`Neutral < 15%` å ±è­¦
  4. å¯«å…¥ metadata æ¯å€‹ split çš„æ¨™ç±¤åˆ†å¸ƒ

### ç¾ç‹€æª¢æŸ¥

**extract_tw_stock_data_v6.py:543-544**ï¼ˆæ¨™ç±¤æ˜ å°„ï¼‰:
```python
# 4. è½‰æ›æ¨™ç±¤
y_tb = tb_df["y"].map({-1: 0, 0: 1, 1: 2})
```

**extract_tw_stock_data_v6.py:547-558**ï¼ˆæ¬Šé‡è¨ˆç®—ï¼‰:
```python
if config['sample_weights']['enabled']:
    w = make_sample_weight(...)
else:
    w = pd.Series(np.ones(len(y_tb)), index=y_tb.index)
```

**make_sample_weight() - æ¬Šé‡è£å‰ª**:
```python
w = np.clip(w, 0.1, 5.0)  # âœ… å·²è£å‰ª
```

**ç•¶å‰è¼¸å‡ºçµ±è¨ˆ**:
```python
# åªæœ‰ç¸½é«”åˆ†å¸ƒï¼Œæ²’æœ‰é©—è­‰
logging.info(f"  æ¨™ç±¤åˆ†å¸ƒ: Down={...}, Neutral={...}, Up={...}")
```

### è©•ä¼°

#### âœ… å„ªé»
1. **æ¨™ç±¤æ˜ å°„æ˜ç¢º**: {-1,0,1} â†’ {0,1,2}
2. **æ¬Šé‡å·²è£å‰ª**: [0.1, 5.0]
3. **æœ‰æ—¥èªŒè¼¸å‡º**: é¡¯ç¤ºæ¨™ç±¤åˆ†å¸ƒ

#### âš ï¸ ä¸è¶³
1. **ç„¡ assertion æª¢æŸ¥**:
   - è‹¥æ˜ å°„å¤±æ•—ï¼ˆNaN æ¨™ç±¤ï¼‰ï¼Œä¸æœƒå ±éŒ¯
   - è‹¥æ¬Šé‡ç•°å¸¸ï¼ˆinfï¼‰ï¼Œä¸æœƒå ±éŒ¯

2. **Neutral < 15% ç„¡è‡ªå‹•å ±è­¦**:
   - é›–ç„¶æ–‡æª”è¦æ±‚ï¼Œä½†ä»£ç¢¼æœªå¯¦ç¾
   - éœ€è¦æ‰‹å‹•æª¢æŸ¥æ—¥èªŒ

3. **metadata ç¼ºå°‘è©³ç´°åˆ†å¸ƒ**:
   - ç•¶å‰åªè¨˜éŒ„ç¸½æ•¸ï¼Œæ²’æœ‰ç™¾åˆ†æ¯”

### å»ºè­°è¡Œå‹• âœ… **ç«‹å³æ¡ç´**

#### å„ªå…ˆç´šï¼š**é«˜**ï¼ˆæ•¸æ“šå®Œæ•´æ€§é—œéµï¼‰

#### å¯¦ç¾æ–¹æ¡ˆ

**Step 1: æ¨™ç±¤é‚Šç•Œæª¢æŸ¥**
```python
# extract_tw_stock_data_v6.py - build_split_v6()

# æ˜ å°„å¾Œç«‹å³æª¢æŸ¥
y_tb = tb_df["y"].map({-1: 0, 0: 1, 1: 2})

# Hard assertion
if y_tb.isna().any():
    raise ValueError(
        f"âŒ æ¨™ç±¤æ˜ å°„å¤±æ•—ï¼šç™¼ç¾ {y_tb.isna().sum()} å€‹ NaN\n"
        f"   åŸå§‹æ¨™ç±¤å¯èƒ½ä¸åœ¨ {{-1, 0, 1}} ç¯„åœå…§"
    )

if not y_tb.isin([0, 1, 2]).all():
    invalid = y_tb[~y_tb.isin([0, 1, 2])]
    raise ValueError(
        f"âŒ æ¨™ç±¤å€¼ç•°å¸¸ï¼šç™¼ç¾é {{0,1,2}} çš„å€¼\n"
        f"   ç•°å¸¸å€¼: {invalid.unique()}"
    )
```

**Step 2: æ¬Šé‡é‚Šç•Œæª¢æŸ¥**
```python
# åœ¨ç”Ÿæˆæ¨£æœ¬æ¬Šé‡å¾Œ
w = make_sample_weight(...)

# Hard assertion
if not np.isfinite(w).all():
    raise ValueError(
        f"âŒ æ¬Šé‡åŒ…å« NaN æˆ– infï¼š{np.sum(~np.isfinite(w))} å€‹"
    )

# æª¢æŸ¥å‡å€¼ï¼ˆæ‡‰æ¥è¿‘ 1ï¼‰
w_mean = w.mean()
if not (0.5 < w_mean < 2.0):
    logging.warning(
        f"âš ï¸ æ¬Šé‡å‡å€¼ç•°å¸¸: {w_mean:.3f} (é æœŸæ¥è¿‘ 1.0)\n"
        f"   é€™å¯èƒ½å°è‡´è¨“ç·´ä¸ç©©å®š"
    )
```

**Step 3: Neutral æ¯”ä¾‹æª¢æŸ¥**
```python
# åœ¨æ¯å€‹ split ç”Ÿæˆå¾Œ
label_dist = np.bincount(y_array, minlength=3)
label_pct = label_dist / label_dist.sum() * 100

neutral_pct = label_pct[1]
if neutral_pct < 15.0:
    logging.warning(
        f"âš ï¸ {split_name.upper()} é›† Neutral æ¯”ä¾‹éä½: {neutral_pct:.1f}%\n"
        f"   ç›®æ¨™: 20-45%ï¼Œç•¶å‰: {neutral_pct:.1f}%\n"
        f"   â†’ å»ºè­°èª¿æ•´ Triple-Barrier åƒæ•¸æˆ–éæ¿¾é–¾å€¼"
    )

if neutral_pct > 60.0:
    logging.warning(
        f"âš ï¸ {split_name.upper()} é›† Neutral æ¯”ä¾‹éé«˜: {neutral_pct:.1f}%\n"
        f"   ç›®æ¨™: 20-45%ï¼Œç•¶å‰: {neutral_pct:.1f}%\n"
        f"   â†’ å»ºè­°èª¿æ•´ min_return é–¾å€¼"
    )
```

**Step 4: æ“´å±• metadata**
```python
# normalization_meta.json
"data_split": {
    "results": {
        "train": {
            "samples": 5584553,
            "label_dist": [1234567, 2345678, 2004308],
            "label_pct": [22.1, 42.0, 35.9],  # æ–°å¢
            "neutral_warning": false,  # æ–°å¢
            "weight_stats": {
                "mean": 1.02,
                "std": 0.85,
                "min": 0.1,
                "max": 5.0,
                "is_finite": true  # æ–°å¢
            }
        },
        ...
    }
}
```

#### æ™‚ç¨‹
- **ç«‹å³**: å¯¦ç¾ Step 1-3ï¼ˆé‚Šç•Œæª¢æŸ¥å’Œå ±è­¦ï¼‰
- **æœ¬é€±**: å¯¦ç¾ Step 4ï¼ˆmetadata æ“´å±•ï¼‰

---

## F. è¨ˆç®—è³‡æºèˆ‡å¯æ“´å±•æ€§

### ChatGPT å»ºè­°
- **å•é¡Œ**: å¤§é‡ symbol-day ä¸²èµ·ä¾†æ»‘çª—ï¼Œå³°å€¼è¨˜æ†¶é«”å¯èƒ½é£†é«˜
- **å»ºè­°**:
  1. æŒ‰ symbol æµå¼ç”Ÿæˆã€åˆ†å¡Š `np.savez_compressed`
  2. ä½¿ç”¨ `float32` é™ä½é«”ç©

### ç¾ç‹€æª¢æŸ¥

**è¨˜æ†¶é«”ä½¿ç”¨**ï¼ˆç•¶å‰æ•¸æ“šè¦æ¨¡ï¼‰:
```python
# è¨“ç·´é›†: 5,584,553 æ¨£æœ¬
# æ¯æ¨£æœ¬: (100, 20) float32 + label + weight
# è¨˜æ†¶é«”: 5.58M Ã— 100 Ã— 20 Ã— 4 bytes â‰ˆ 44.6 GBï¼ˆç†è«–å³°å€¼ï¼‰
```

**ç•¶å‰å¯¦ç¾**:
```python
# extract_tw_stock_data_v6.py - build_split_v6()
X_windows = []  # ç´¯ç©æ‰€æœ‰æ¨£æœ¬
y_labels = []
...
X_array = np.stack(X_windows, axis=0)  # ä¸€æ¬¡æ€§è½‰æ›
np.savez_compressed(npz_path, X=X_array, ...)
```

**æ•¸æ“šé¡å‹**:
```python
# preprocess_single_day.py:629
np.savez_compressed(...,
    features=features.astype(np.float32),  # âœ… å·²æ˜¯ float32
    ...
)
```

### è©•ä¼°

#### âœ… å„ªé»
1. **å·²ä½¿ç”¨ float32**: é è™•ç†éšæ®µå·²è½‰å‹
2. **å·²ä½¿ç”¨å£“ç¸®**: `savez_compressed`
3. **ç•¶å‰è¦æ¨¡å¯æ‰¿å—**: 44.6 GB å³°å€¼ï¼ŒRTX 5090 æœ‰ 32GB VRAM

#### âš ï¸ æ½›åœ¨å•é¡Œ
1. **æœªä¾†æ“´å±•æ€§**:
   - è‹¥æ•¸æ“šå¢åŠ  10 å€ï¼ˆ500 å¤© Ã— 500 è‚¡ç¥¨ï¼‰
   - å³°å€¼è¨˜æ†¶é«” > 400 GB â†’ è¶…å‡º RAM é™åˆ¶

2. **å…¨éƒ¨è¼‰å…¥è¨˜æ†¶é«”**:
   - `X_windows.append()` ç´¯ç©æ‰€æœ‰æ¨£æœ¬
   - å°æ¥µå¤§æ•¸æ“šé›†ä¸é©ç”¨

#### ğŸ¤” å¯¦éš›éœ€æ±‚
- **ç•¶å‰æ•¸æ“š**: 10 å¤© Ã— 250 è‚¡ç¥¨ = 2500 symbol-days
- **é æœŸè¦æ¨¡**: 60 å¤© Ã— 500 è‚¡ç¥¨ = 30000 symbol-days (12x)
- **è¨˜æ†¶é«”ä¼°ç®—**: 44.6 GB Ã— 12 â‰ˆ **535 GB** å³°å€¼

**çµè«–**: æœªä¾†å¯èƒ½éœ€è¦å„ªåŒ–

### å»ºè­°è¡Œå‹• â³ **å»¶å¾Œ**ï¼ˆä½†æº–å‚™æ–¹æ¡ˆï¼‰

#### å„ªå…ˆç´šï¼š**ä½**ï¼ˆç•¶å‰ç„¡ç“¶é ¸ï¼‰

#### å¯¦ç¾æ–¹æ¡ˆï¼ˆç•¶éœ€è¦æ™‚ï¼‰

**æ–¹æ¡ˆ 1: åˆ†å¡Šä¿å­˜**ï¼ˆæ¨è–¦ï¼‰
```python
# extract_tw_stock_data_v6.py
def build_split_v6_chunked(split_name, chunk_size=1000000):
    """æŒ‰ 100 è¬æ¨£æœ¬åˆ†å¡Šä¿å­˜"""
    X_windows = []
    y_labels = []
    chunk_id = 0

    for sym, n_points, day_data_sorted in stock_list:
        for date, features, mids in day_data_sorted:
            # ... ç”Ÿæˆæ¨£æœ¬ ...

            # é”åˆ° chunk_size æ™‚ä¿å­˜
            if len(X_windows) >= chunk_size:
                save_chunk(split_name, chunk_id, X_windows, y_labels, ...)
                X_windows = []
                y_labels = []
                chunk_id += 1

    # ä¿å­˜æœ€å¾Œä¸€å¡Š
    save_chunk(split_name, chunk_id, X_windows, y_labels, ...)

    # åˆä½µæ‰€æœ‰ chunksï¼ˆå¯é¸ï¼‰
    merge_chunks(split_name)
```

**æ–¹æ¡ˆ 2: æµå¼å¯«å…¥**ï¼ˆæ›´é«˜ç´šï¼‰
```python
# ä½¿ç”¨ zarr æˆ– h5py æµå¼å¯«å…¥
import zarr

z = zarr.open('train.zarr', mode='w', shape=(0, 100, 20),
              chunks=(10000, 100, 20), dtype='float32')

for sample in generate_samples():
    z.append(sample)
```

**æ–¹æ¡ˆ 3: memmap**ï¼ˆå…§å»ºï¼‰
```python
# é å…ˆåˆ†é…æ–‡ä»¶æ˜ å°„
X_mmap = np.memmap('train_X.dat', dtype='float32',
                   mode='w+', shape=(estimated_size, 100, 20))

idx = 0
for sample in generate_samples():
    X_mmap[idx] = sample
    idx += 1
```

#### æ™‚ç¨‹
- **ç•¶å‰**: ä¸å¯¦ç¾ï¼ˆç„¡ç“¶é ¸ï¼‰
- **ç›£æ§**: è¨˜æ†¶é«”ä½¿ç”¨ç‡
- **è§¸ç™¼æ¢ä»¶**: å³°å€¼è¨˜æ†¶é«” > 80% ç³»çµ± RAM
- **å¯¦ç¾æ™‚æ©Ÿ**: æ•¸æ“šè¦æ¨¡æ“´å¤§ 10 å€æ™‚

---

## G. æ‰¹æ¬¡å™¨èˆ‡ç›®éŒ„å¥å£¯æ€§

### ChatGPT å»ºè­°
- **å•é¡Œ 1**: Windows batch èˆ‡ Python è·¯å¾‘åˆ†éš”ç¬¦å·®ç•°ï¼ˆ\ vs /ï¼‰
- **å•é¡Œ 2**: ç•¶å¤©ç„¡é€šééæ¿¾çš„ symbol æ™‚ï¼Œæ‡‰å„ªé›…è·³é

### ç¾ç‹€æª¢æŸ¥

**å•é¡Œ 1: è·¯å¾‘è™•ç†**

**batch_preprocess.bat**:
```bat
python scripts\preprocess_single_day.py ^
    --input data\temp\%filename% ^
    --output-dir data\preprocessed_v5_1hz ^
    --config configs\config_pro_v5_ml_optimal.yaml
```

**Python ä»£ç¢¼**:
```python
# extract_tw_stock_data_v6.py:332
for npz_file in sorted(glob.glob(os.path.join(daily_dir, "*", "*.npz"))):
    # âœ… ä½¿ç”¨ os.path.join
```

**å•é¡Œ 2: ç©ºæ•¸æ“šè™•ç†**

**extract_tw_stock_data_v6.py:832-834**:
```python
if not preprocessed_data:
    logging.error("æ²’æœ‰å¯ç”¨çš„é è™•ç†æ•¸æ“šï¼")
    return 1  # âœ… å„ªé›…é€€å‡º
```

**preprocess_single_day.py:812-814**:
```python
if not daily_stats:
    logging.warning("ç•¶å¤©ç„¡æœ‰æ•ˆæ•¸æ“šï¼")
    return stats  # âœ… è¿”å›ç©ºçµ±è¨ˆ
```

### è©•ä¼°

#### âœ… å„ªé»
1. **Python ç«¯è·¯å¾‘è™•ç†æ­£ç¢º**: å…¨éƒ¨ä½¿ç”¨ `os.path.join`
2. **ç©ºæ•¸æ“šå·²è™•ç†**: å…©éšæ®µéƒ½æœ‰æª¢æŸ¥
3. **å„ªé›…é€€å‡º**: ä¸æœƒå´©æ½°

#### âš ï¸ æ½›åœ¨å•é¡Œ
1. **batch è…³æœ¬ç¡¬ç·¨ç¢¼è·¯å¾‘**:
   - `data\temp\` ä½¿ç”¨åæ–œç·š
   - è·¨å¹³å°å…¼å®¹æ€§å·®ï¼ˆä½†åƒ… Windows ä½¿ç”¨ï¼‰

2. **ç©ºæ•¸æ“šè¿”å›ç¢¼ä¸ä¸€è‡´**:
   - é è™•ç†éšæ®µï¼šè¿”å› `stats`ï¼ˆæˆåŠŸï¼‰
   - è¨“ç·´æ•¸æ“šç”Ÿæˆéšæ®µï¼šè¿”å› `1`ï¼ˆéŒ¯èª¤ï¼‰
   - æ‡‰è©²çµ±ä¸€ç‚ºã€Œè­¦å‘Šã€ç‹€æ…‹ç¢¼ 2

3. **summary.json ç”Ÿæˆæ™‚æ©Ÿ**:
   - è‹¥æ‰€æœ‰ symbol è¢«éæ¿¾ï¼Œæ˜¯å¦ä»ç”Ÿæˆ summaryï¼Ÿ
   - ç•¶å‰ï¼šæœƒç”Ÿæˆç©º summary

### å»ºè­°è¡Œå‹• âœ… **æ¡ç´**

#### å„ªå…ˆç´šï¼š**ä¸­**ï¼ˆWindows ç’°å¢ƒé‡è¦ï¼‰

#### å¯¦ç¾æ–¹æ¡ˆ

**Step 1: çµ±ä¸€è·¯å¾‘è®Šæ•¸**ï¼ˆbatch è…³æœ¬ï¼‰
```bat
REM batch_preprocess.bat
SET INPUT_DIR=data\temp
SET OUTPUT_DIR=data\preprocessed_v5_1hz
SET CONFIG=configs\config_pro_v5_ml_optimal.yaml

for %%f in (%INPUT_DIR%\*.txt) do (
    python scripts\preprocess_single_day.py ^
        --input "%%f" ^
        --output-dir "%OUTPUT_DIR%" ^
        --config "%CONFIG%"
)
```

**Step 2: ç©ºæ•¸æ“šè¿”å›ç¢¼çµ±ä¸€**
```python
# preprocess_single_day.py - process_single_day()
if not daily_stats:
    logging.warning("âš ï¸ ç•¶å¤©ç„¡æœ‰æ•ˆæ•¸æ“šï¼")
    # ä»ç„¶ç”Ÿæˆç©º summary
    generate_daily_summary(
        output_dir, date, [], 0.0, 'none',
        {'down': 0.33, 'neutral': 0.34, 'up': 0.33},
        0, 0
    )
    return stats  # è¿”å›çµ±è¨ˆï¼ˆè¦–ç‚ºæˆåŠŸï¼‰
```

**Step 3: ç©ºæ•¸æ“šå ±å‘Šå¢å¼·**
```python
# extract_tw_stock_data_v6.py - load_all_preprocessed_data()
if not all_data:
    logging.error("âŒ æ²’æœ‰å¯ç”¨çš„é è™•ç†æ•¸æ“šï¼")
    logging.info("å¯èƒ½åŸå› ï¼š")
    logging.info("  1. preprocessed_dir è·¯å¾‘éŒ¯èª¤")
    logging.info("  2. æ‰€æœ‰è‚¡ç¥¨è¢«éæ¿¾ï¼ˆpass_filter=falseï¼‰")
    logging.info("  3. é è™•ç†å°šæœªåŸ·è¡Œ")
    return []
```

**Step 4: è·¨å¹³å°è·¯å¾‘å…¼å®¹æ€§**ï¼ˆå¯é¸ï¼‰
```python
# utils/path_helper.py (æ–°å¢)
import os
from pathlib import Path

def normalize_path(path_str):
    """çµ±ä¸€è·¯å¾‘åˆ†éš”ç¬¦ï¼ˆè·¨å¹³å°ï¼‰"""
    return str(Path(path_str))

# ä½¿ç”¨ç¯„ä¾‹
input_path = normalize_path("data/temp/20250901.txt")  # Windows: data\temp\20250901.txt
```

#### æ™‚ç¨‹
- **æœ¬é€±**: å¯¦ç¾ Step 2-3ï¼ˆç©ºæ•¸æ“šè™•ç†å¢å¼·ï¼‰
- **ä¸‹é€±**: å¯¦ç¾ Step 1ï¼ˆbatch è…³æœ¬æ”¹é€²ï¼‰
- **å¯é¸**: Step 4ï¼ˆè‹¥éœ€è¦è·¨å¹³å°ï¼‰

---

## ç¸½çµèˆ‡å„ªå…ˆç´šæ’åº

### ç«‹å³å¯¦æ–½ï¼ˆæœ¬é€±å®Œæˆï¼‰âœ…

| å»ºè­° | å·¥ä½œé‡ | å½±éŸ¿ | è¡Œå‹• |
|------|--------|------|------|
| **C. åˆ‡åˆ†ç©©å®šæ€§** | å°ï¼ˆ30 åˆ†é˜ï¼‰ | é«˜ | è¨˜éŒ„ seed åˆ° metadata + æ¸¬è©¦ |
| **E. æ¨™ç±¤é‚Šç•Œæª¢æŸ¥** | ä¸­ï¼ˆ2 å°æ™‚ï¼‰ | é«˜ | æ·»åŠ  assertion + Neutral å ±è­¦ |
| **A. ffill å“è³ªéæ¿¾** | ä¸­ï¼ˆ3 å°æ™‚ï¼‰ | é«˜ | æ»‘çª—å“è³ªæª¢æŸ¥ + é™ä½ ffill_limit |
| **G. æ‰¹æ¬¡å™¨å¥å£¯æ€§** | å°ï¼ˆ1 å°æ™‚ï¼‰ | ä¸­ | ç©ºæ•¸æ“šè™•ç†å¢å¼· |

### çŸ­æœŸå¯¦æ–½ï¼ˆä¸‹é€±ï¼‰â³

| å»ºè­° | å·¥ä½œé‡ | å½±éŸ¿ | è¡Œå‹• |
|------|--------|------|------|
| **D. ç•°å¸¸çµ±è¨ˆ** | ä¸­ï¼ˆ3 å°æ™‚ï¼‰ | ä¸­ | åˆ†é¡çµ±è¨ˆ + summary.json |
| **A. ffill æ‘˜è¦** | å°ï¼ˆ1 å°æ™‚ï¼‰ | ä¸­ | é•·ç¼ºå£/é«˜ ffill å ±å‘Š |
| **C. å¾©ç¾æ€§æ¸¬è©¦** | ä¸­ï¼ˆ2 å°æ™‚ï¼‰ | é«˜ | å–®å…ƒæ¸¬è©¦ + æ–‡æª” |

### è§€å¯Ÿå¾Œæ±ºå®šï¼ˆ1-2 å€‹æœˆï¼‰ğŸ”

| å»ºè­° | å·¥ä½œé‡ | è§¸ç™¼æ¢ä»¶ | è¡Œå‹• |
|------|--------|---------|------|
| **B. å‹•æ…‹éæ¿¾é€€åŒ–** | å°ï¼ˆ1 å°æ™‚ï¼‰ | å‡ºç¾æ¥µç«¯è¡Œæƒ…æ—¥ | æ·»åŠ  IQR æª¢æŸ¥ |
| **F. è¨˜æ†¶é«”å„ªåŒ–** | å¤§ï¼ˆ1-2 å¤©ï¼‰ | æ•¸æ“šè¦æ¨¡ > 10x | åˆ†å¡Šä¿å­˜æˆ–æµå¼å¯«å…¥ |

### ä¸æ¡ç´ âŒ

ç„¡ï¼ˆæ‰€æœ‰å»ºè­°éƒ½æœ‰åƒ¹å€¼ï¼Œåªæ˜¯å„ªå…ˆç´šä¸åŒï¼‰

---

## å¯¦æ–½æª¢æŸ¥æ¸…å–®

### æœ¬é€±ä»»å‹™ï¼ˆé ä¼° 8 å°æ™‚ï¼‰

- [ ] **C.1**: è¨˜éŒ„ seed åˆ° normalization_meta.jsonï¼ˆ30 åˆ†é˜ï¼‰
- [ ] **E.1**: æ¨™ç±¤æ˜ å°„ assertionï¼ˆ30 åˆ†é˜ï¼‰
- [ ] **E.2**: æ¬Šé‡é‚Šç•Œæª¢æŸ¥ï¼ˆ30 åˆ†é˜ï¼‰
- [ ] **E.3**: Neutral æ¯”ä¾‹å ±è­¦ï¼ˆ30 åˆ†é˜ï¼‰
- [ ] **E.4**: metadata æ“´å±•ï¼ˆ1 å°æ™‚ï¼‰
- [ ] **A.1**: é™ä½ ffill_limit 120â†’60 ç§’ï¼ˆ15 åˆ†é˜ï¼‰
- [ ] **A.2**: æ»‘çª— ffill å“è³ªéæ¿¾ï¼ˆ2 å°æ™‚ï¼‰
- [ ] **G.1**: ç©ºæ•¸æ“šè¿”å›ç¢¼çµ±ä¸€ï¼ˆ30 åˆ†é˜ï¼‰
- [ ] **G.2**: ç©ºæ•¸æ“šå ±å‘Šå¢å¼·ï¼ˆ30 åˆ†é˜ï¼‰
- [ ] **æ¸¬è©¦**: é‹è¡Œå®Œæ•´æµæ°´ç·šé©—è­‰ï¼ˆ1 å°æ™‚ï¼‰

### ä¸‹é€±ä»»å‹™ï¼ˆé ä¼° 6 å°æ™‚ï¼‰

- [ ] **D.1**: æ“´å±•å…¨åŸŸç•°å¸¸çµ±è¨ˆï¼ˆ1 å°æ™‚ï¼‰
- [ ] **D.2**: ä¿®æ”¹ parse_line() åˆ†é¡è¨ˆæ•¸ï¼ˆ1 å°æ™‚ï¼‰
- [ ] **D.3**: summary.json ç•°å¸¸ç´°ç¯€ï¼ˆ1 å°æ™‚ï¼‰
- [ ] **A.3**: é•·ç¼ºå£/é«˜ ffill æ‘˜è¦å ±å‘Šï¼ˆ1 å°æ™‚ï¼‰
- [ ] **C.2**: å¾©ç¾æ€§å–®å…ƒæ¸¬è©¦ï¼ˆ2 å°æ™‚ï¼‰

### è§€å¯ŸæŒ‡æ¨™

- [ ] **B**: ç›£æ§æ¯æ—¥ IQR å€¼ï¼ˆæ¯é€±æª¢æŸ¥ summary.jsonï¼‰
- [ ] **F**: ç›£æ§è¨˜æ†¶é«”å³°å€¼ä½¿ç”¨ç‡ï¼ˆæ¯æ¬¡è¨“ç·´æª¢æŸ¥ï¼‰

---

**åˆ†æå®Œæˆæ—¥æœŸ**: 2025-10-21
**åˆ†æè€…**: Claude (Sonnet 4.5)
**ä¸‹ä¸€æ­¥**: ç­‰å¾…ç”¨æˆ¶ç¢ºèªæ¡ç´é …ç›®å¾Œé–‹å§‹å¯¦æ–½
