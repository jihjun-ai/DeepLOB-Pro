# æ¬Šé‡è¨ˆç®—ç­–ç•¥åˆ†æ

**å•é¡Œ**: æ‡‰è©²åœ¨é è™•ç†éšæ®µä¿å­˜æ¬Šé‡ï¼Œé‚„æ˜¯åœ¨è¨“ç·´æ™‚å‹•æ…‹è¨ˆç®—ï¼Ÿ

**æ—¥æœŸ**: 2025-10-23

---

## æ–¹æ¡ˆå°æ¯”

### æ–¹æ¡ˆ Aï¼šé è™•ç†æ™‚è¨ˆç®—ä¸¦ä¿å­˜æ¬Šé‡ âš–ï¸

#### å¯¦ä½œæ–¹å¼

```python
# åœ¨ preprocess_single_day.py çš„ compute_label_preview() ä¸­

def compute_sample_weights(labels: np.ndarray) -> np.ndarray:
    """
    è¨ˆç®—æ¨£æœ¬æ¬Šé‡ï¼ˆåŸºæ–¼æ¨™ç±¤åˆ†å¸ƒï¼‰

    Args:
        labels: æ¨™ç±¤é™£åˆ— (-1, 0, 1, NaN)

    Returns:
        sample_weights: æ¨£æœ¬æ¬Šé‡é™£åˆ—ï¼ˆèˆ‡ labels ç­‰é•·ï¼‰
    """
    # éæ¿¾ NaN
    valid_labels = labels[~np.isnan(labels)]

    # è¨ˆç®—é¡åˆ¥è¨ˆæ•¸
    unique, counts = np.unique(valid_labels, return_counts=True)
    total = len(valid_labels)
    n_classes = len(unique)

    # è¨ˆç®— class weights (balanced)
    class_weights = {}
    for label, count in zip(unique, counts):
        class_weights[int(label)] = total / (n_classes * count)

    # åˆ†é…çµ¦æ¯å€‹æ¨£æœ¬
    sample_weights = np.full_like(labels, np.nan, dtype=np.float32)
    for label, weight in class_weights.items():
        mask = (labels == label)
        sample_weights[mask] = weight

    return sample_weights


# åœ¨ save_preprocessed_npz() ä¸­ä¿å­˜
def save_preprocessed_npz(..., labels=None):
    # å¦‚æœæœ‰æ¨™ç±¤ï¼Œè¨ˆç®—æ¬Šé‡
    sample_weights = None
    if labels is not None:
        sample_weights = compute_sample_weights(labels)

    # ä¿å­˜
    save_data = {
        'features': features,
        'mids': mids,
        'labels': labels,
        'sample_weights': sample_weights,  # ğŸ†• æ–°å¢
        ...
    }
    np.savez_compressed(npz_path, **save_data)
```

#### NPZ æ–‡ä»¶çµæ§‹ï¼ˆæ–°å¢å¾Œï¼‰

```python
data = np.load('2330.npz')
data.keys()
# ['features', 'mids', 'labels', 'sample_weights', 'metadata', ...]

# ä½¿ç”¨ç¯„ä¾‹
labels = data['labels']          # (15957,) [-1, 0, 1, NaN]
sample_weights = data['sample_weights']  # (15957,) [1.11, 0.83, 1.11, NaN]
```

---

### âœ… æ–¹æ¡ˆ A çš„å„ªé»

#### 1. ä¸€æ¬¡è¨ˆç®—ï¼Œå¤šæ¬¡ä½¿ç”¨
```python
# é è™•ç†æ™‚è¨ˆç®—ä¸€æ¬¡
sample_weights = compute_sample_weights(labels)  # åªè¨ˆç®—ä¸€æ¬¡

# è¨“ç·´æ™‚ç›´æ¥ä½¿ç”¨ï¼ˆä¸åŒå¯¦é©—éƒ½ç”¨åŒä¸€ä»½ï¼‰
experiment_1: loss = criterion(pred, target, weight=sample_weights)
experiment_2: loss = criterion(pred, target, weight=sample_weights)
experiment_3: loss = criterion(pred, target, weight=sample_weights)
```
**å„ªå‹¢**: é¿å…é‡è¤‡è¨ˆç®—ï¼Œç¯€çœæ™‚é–“ï¼ˆé›–ç„¶æ¬Šé‡è¨ˆç®—å¾ˆå¿«ï¼‰

---

#### 2. ç¢ºä¿å¯¦é©—ä¸€è‡´æ€§
```python
# æ–¹æ¡ˆ A: æ‰€æœ‰å¯¦é©—ä½¿ç”¨ç›¸åŒæ¬Šé‡ï¼ˆå¾ NPZ è®€å–ï¼‰
weights_exp1 = data['sample_weights']  # [1.11, 0.83, 1.11, ...]
weights_exp2 = data['sample_weights']  # [1.11, 0.83, 1.11, ...] âœ… å®Œå…¨ç›¸åŒ

# æ–¹æ¡ˆ B: æ¯æ¬¡å¯¦é©—å¯èƒ½ç•¥æœ‰ä¸åŒï¼ˆå¦‚æœè¨“ç·´é›†æœ‰è®ŠåŒ–ï¼‰
weights_exp1 = compute_class_weight(train_set_1)  # [1.10, 0.84, 1.12]
weights_exp2 = compute_class_weight(train_set_2)  # [1.12, 0.82, 1.11] âš ï¸ ç•¥æœ‰å·®ç•°
```
**å„ªå‹¢**: å¯¦é©—å¯é‡ç¾æ€§æ›´é«˜

---

#### 3. ä¾¿æ–¼è¦–è¦ºåŒ–å’Œæª¢æŸ¥
```python
# åœ¨ Label Viewer ä¸­å¯ä»¥é¡¯ç¤ºæ¬Šé‡åˆ†å¸ƒ
import plotly.graph_objects as go

fig = go.Figure()
fig.add_trace(go.Histogram(
    x=sample_weights[~np.isnan(sample_weights)],
    name='Sample Weights'
))
fig.update_layout(title='æ¨£æœ¬æ¬Šé‡åˆ†å¸ƒ')
```
**å„ªå‹¢**: å¯ä»¥åœ¨è¨“ç·´å‰æª¢æŸ¥æ¬Šé‡æ˜¯å¦åˆç†

---

#### 4. æ”¯æ´è¤‡é›œæ¬Šé‡ç­–ç•¥
```python
# å¯ä»¥ä¿å­˜å¤šç¨®æ¬Šé‡ç­–ç•¥
def compute_multiple_weight_strategies(labels):
    return {
        'balanced': compute_balanced_weights(labels),
        'focal': compute_focal_weights(labels, gamma=2.0),
        'effective_num': compute_effective_num_weights(labels, beta=0.999),
        'custom': compute_custom_weights(labels)
    }

# ä¿å­˜åˆ° metadata
metadata['weight_strategies'] = {
    'balanced': {...},
    'focal': {...},
    'effective_num': {...}
}
```
**å„ªå‹¢**: å¯ä»¥é å…ˆè¨ˆç®—å¤šç¨®ç­–ç•¥ï¼Œè¨“ç·´æ™‚é¸æ“‡

---

#### 5. æ¸›å°‘è¨“ç·´è…³æœ¬è¤‡é›œåº¦
```python
# æ–¹æ¡ˆ A: è¨“ç·´è…³æœ¬ç°¡å–®
weights = torch.FloatTensor(data['sample_weights'])
loss = criterion(pred, target, weight=weights)

# æ–¹æ¡ˆ B: è¨“ç·´è…³æœ¬éœ€è¦è¨ˆç®—é‚è¼¯
from sklearn.utils.class_weight import compute_class_weight
class_weights = compute_class_weight('balanced', classes=[-1,0,1], y=train_labels)
sample_weights = np.array([class_weights[label+1] for label in train_labels])
weights = torch.FloatTensor(sample_weights)
```
**å„ªå‹¢**: è¨“ç·´ä»£ç¢¼æ›´ç°¡æ½”

---

### âŒ æ–¹æ¡ˆ A çš„ç¼ºé»

#### 1. å¢åŠ æ–‡ä»¶å¤§å°
```python
# æª”æ¡ˆå¤§å°æ¯”è¼ƒï¼ˆå–®ä¸€è‚¡ç¥¨å–®æ—¥ï¼‰
ç„¡æ¬Šé‡ NPZ:  250 KB
æœ‰æ¬Šé‡ NPZ:  300 KB  (+20%)

# å…¨éƒ¨ 195 æª”è‚¡ç¥¨å–®æ—¥
ç„¡æ¬Šé‡:  50 MB
æœ‰æ¬Šé‡:  60 MB  (+10 MB)

# å…¨å¹´æ•¸æ“šï¼ˆ250 äº¤æ˜“æ—¥ï¼‰
ç„¡æ¬Šé‡:  12.5 GB
æœ‰æ¬Šé‡:  15.0 GB  (+2.5 GB)
```
**å½±éŸ¿**: ç£ç¢Ÿç©ºé–“å¢åŠ ç´„ 20%

---

#### 2. éˆæ´»æ€§é™ä½
```python
# å¦‚æœæƒ³å˜—è©¦ä¸åŒæ¬Šé‡ç­–ç•¥ï¼Œéœ€è¦ï¼š

# æ–¹æ¡ˆ A: é‡æ–°é è™•ç†ï¼ˆè€—æ™‚ï¼‰
python preprocess_single_day.py --weight-strategy focal  # 30 åˆ†é˜
python preprocess_single_day.py --weight-strategy effective_num  # 30 åˆ†é˜

# æ–¹æ¡ˆ B: ä¿®æ”¹è¨“ç·´è…³æœ¬ï¼ˆå³æ™‚ï¼‰
train.py --weight-strategy focal  # 0 ç§’
train.py --weight-strategy effective_num  # 0 ç§’
```
**å½±éŸ¿**: èª¿åƒéˆæ´»æ€§é™ä½

---

#### 3. å¯èƒ½éæ™‚æˆ–ä¸ä¸€è‡´
```python
# æƒ…å¢ƒï¼šå¾ŒçºŒèª¿æ•´äº†æ¨™ç±¤ç”Ÿæˆé‚è¼¯

# æ–¹æ¡ˆ A: æ¬Šé‡å¯èƒ½èˆ‡æ¨™ç±¤ä¸åŒ¹é…
labels = regenerate_labels(mids, new_config)  # æ¨™ç±¤æ”¹è®Š
weights = data['sample_weights']  # âš ï¸ é‚„æ˜¯èˆŠçš„æ¬Šé‡ï¼

# æ–¹æ¡ˆ B: å‹•æ…‹è¨ˆç®—ï¼Œè‡ªå‹•é©æ‡‰
labels = regenerate_labels(mids, new_config)
weights = compute_weights(labels)  # âœ… è‡ªå‹•æ›´æ–°
```
**å½±éŸ¿**: ç¶­è­·æˆæœ¬å¢åŠ 

---

#### 4. æ¬Šé‡è¨ˆç®—æœ¬èº«å¾ˆå¿«
```python
import time
import numpy as np

labels = np.random.choice([-1, 0, 1], size=1_000_000)

# è¨ˆç®—æ¬Šé‡
start = time.time()
weights = compute_sample_weights(labels)
elapsed = time.time() - start

print(f"è¨ˆç®— 100 è¬æ¨£æœ¬æ¬Šé‡: {elapsed:.3f} ç§’")
# è¼¸å‡º: è¨ˆç®— 100 è¬æ¨£æœ¬æ¬Šé‡: 0.002 ç§’
```
**å½±éŸ¿**: é å…ˆè¨ˆç®—çš„æ™‚é–“å„ªå‹¢å¾®ä¹å…¶å¾®

---

#### 5. ä¸ç¬¦åˆä¸»æµåšæ³•
```python
# PyTorch æ¨™æº–åšæ³•ï¼ˆè¨“ç·´æ™‚è¨ˆç®—ï¼‰
from torch.utils.data import WeightedRandomSampler

class_weights = compute_class_weight('balanced', ...)
sample_weights = [class_weights[label] for label in labels]
sampler = WeightedRandomSampler(sample_weights, len(sample_weights))

# TensorFlow æ¨™æº–åšæ³•ï¼ˆè¨“ç·´æ™‚è¨ˆç®—ï¼‰
class_weight = {0: 1.5, 1: 0.5, 2: 3.0}
model.fit(X, y, class_weight=class_weight)
```
**å½±éŸ¿**: èˆ‡ä¸»æµæ¡†æ¶æ…£ä¾‹ä¸ä¸€è‡´

---

## æ–¹æ¡ˆ Bï¼šè¨“ç·´æ™‚å‹•æ…‹è¨ˆç®—æ¬Šé‡ï¼ˆç•¶å‰åšæ³•ï¼‰

### å¯¦ä½œæ–¹å¼

```python
# åœ¨ train_deeplob_generic.py ä¸­

from sklearn.utils.class_weight import compute_class_weight
import torch

# è¨“ç·´å‰ä¸€æ¬¡æ€§è¨ˆç®—
train_labels = train_dataset.labels  # å¾è¨“ç·´é›†ç²å–æ¨™ç±¤

# è¨ˆç®— class weights
class_weights = compute_class_weight(
    'balanced',
    classes=np.array([-1, 0, 1]),
    y=train_labels
)

# è½‰ç‚º sample weights
sample_weights = np.array([class_weights[label + 1] for label in train_labels])

# ç”¨æ–¼ Loss Function
criterion = nn.CrossEntropyLoss(
    weight=torch.FloatTensor(class_weights)
)

# æˆ–ç”¨æ–¼ Sampler
from torch.utils.data import WeightedRandomSampler
sampler = WeightedRandomSampler(sample_weights, len(sample_weights))
train_loader = DataLoader(train_dataset, sampler=sampler)
```

---

### âœ… æ–¹æ¡ˆ B çš„å„ªé»

#### 1. éˆæ´»æ€§æ¥µé«˜
```python
# å¯ä»¥è¼•é¬†åˆ‡æ›ä¸åŒæ¬Šé‡ç­–ç•¥

# Balanced weights
weights = compute_class_weight('balanced', classes=[-1,0,1], y=labels)

# Focal lossï¼ˆå‹•æ…‹èª¿æ•´ gammaï¼‰
focal_loss = FocalLoss(gamma=2.0)

# Effective number of samples
weights = compute_effective_num_weights(labels, beta=0.999)

# è‡ªå®šç¾©æ¬Šé‡
weights = {-1: 2.0, 0: 1.0, 1: 1.5}
```

---

#### 2. æ–‡ä»¶å¤§å°ä¸å¢åŠ 
```
NPZ å¤§å°: 250 KB (ä¸è®Š)
ç£ç¢Ÿç©ºé–“ç¯€çœ: 2.5 GB (å…¨å¹´æ•¸æ“š)
```

---

#### 3. è¨ˆç®—é€Ÿåº¦æ¥µå¿«
```python
# å³ä½¿æ˜¯ 100 è¬æ¨£æœ¬
compute_class_weight(...)  # < 0.01 ç§’
```

---

#### 4. è‡ªå‹•é©æ‡‰æ•¸æ“šè®ŠåŒ–
```python
# å¦‚æœè¨“ç·´é›†æ”¹è®Šï¼Œæ¬Šé‡è‡ªå‹•æ›´æ–°
train_set_v1 = load_data(config_v1)
weights_v1 = compute_weights(train_set_v1)  # è‡ªå‹•é©æ‡‰ v1

train_set_v2 = load_data(config_v2)
weights_v2 = compute_weights(train_set_v2)  # è‡ªå‹•é©æ‡‰ v2
```

---

#### 5. ç¬¦åˆä¸»æµæ¡†æ¶æ…£ä¾‹
```python
# PyTorch å®˜æ–¹ç¯„ä¾‹
class_weights = compute_class_weight(...)
criterion = nn.CrossEntropyLoss(weight=torch.FloatTensor(class_weights))

# TensorFlow å®˜æ–¹ç¯„ä¾‹
class_weight = {0: 1.0, 1: 2.0}
model.fit(X, y, class_weight=class_weight)
```

---

### âŒ æ–¹æ¡ˆ B çš„ç¼ºé»

#### 1. æ¯æ¬¡è¨“ç·´éƒ½è¦è¨ˆç®—
```python
# æ¯æ¬¡é‹è¡Œè¨“ç·´éƒ½éœ€è¦é€™æ®µä»£ç¢¼
weights = compute_class_weight(...)  # é›–ç„¶åªéœ€ 0.01 ç§’
```
**å½±éŸ¿**: å¾®ä¹å…¶å¾®ï¼ˆ< è¨“ç·´ç¸½æ™‚é–“çš„ 0.001%ï¼‰

---

#### 2. å¯èƒ½ä¸åŒå¯¦é©—ä¸ä¸€è‡´
```python
# å¦‚æœè¨“ç·´é›†ç•¥æœ‰è®ŠåŒ–
exp1_weights = compute_weights(train_set_1)  # [1.10, 0.84, 1.12]
exp2_weights = compute_weights(train_set_2)  # [1.12, 0.82, 1.11]
```
**å½±éŸ¿**: ä½†é€™é€šå¸¸æ˜¯**æ­£ç¢ºè¡Œç‚º**ï¼ˆæ¬Šé‡æ‡‰è©²åæ˜ å¯¦éš›è¨“ç·´é›†ï¼‰

---

## æ±ºç­–æ¨¹ ğŸŒ³

```
æ˜¯å¦åœ¨é è™•ç†æ™‚ä¿å­˜æ¬Šé‡ï¼Ÿ
â”‚
â”œâ”€ æ˜¯ï¼Œå¦‚æœï¼š
â”‚   â”œâ”€ æ¬Šé‡è¨ˆç®—éå¸¸è¤‡é›œï¼ˆ> 1 åˆ†é˜ï¼‰
â”‚   â”œâ”€ éœ€è¦å¤šç¨®é è¨ˆç®—ç­–ç•¥ä¾›é¸æ“‡
â”‚   â”œâ”€ å¯¦é©—ä¸€è‡´æ€§æ¥µåº¦é‡è¦
â”‚   â””â”€ ç£ç¢Ÿç©ºé–“å……è¶³ï¼ˆ+20%ï¼‰
â”‚
â””â”€ å¦ï¼Œå¦‚æœï¼šï¼ˆæ¨è–¦ âœ…ï¼‰
    â”œâ”€ æ¬Šé‡è¨ˆç®—å¿«é€Ÿï¼ˆ< 1 ç§’ï¼‰
    â”œâ”€ éœ€è¦éˆæ´»èª¿æ•´æ¬Šé‡ç­–ç•¥
    â”œâ”€ ç£ç¢Ÿç©ºé–“æœ‰é™
    â””â”€ éµå¾ªä¸»æµæ¡†æ¶æ…£ä¾‹
```

---

## æ¨è–¦æ–¹æ¡ˆ ğŸ¯

### å»ºè­°ï¼šæ–¹æ¡ˆ Bï¼ˆè¨“ç·´æ™‚å‹•æ…‹è¨ˆç®—ï¼‰âœ…

**ç†ç”±**ï¼š

1. âš¡ **æ¬Šé‡è¨ˆç®—æ¥µå¿«**ï¼ˆ< 0.01 ç§’ï¼‰ï¼Œé å…ˆè¨ˆç®—å„ªå‹¢æ¥µå°
2. ğŸ”§ **éˆæ´»æ€§é—œéµ**ï¼šèª¿åƒæ™‚ç¶“å¸¸éœ€è¦å˜—è©¦ä¸åŒæ¬Šé‡ç­–ç•¥
3. ğŸ’¾ **ç¯€çœç©ºé–“**ï¼šå…¨å¹´æ•¸æ“šå¯ç¯€çœ 2.5 GB
4. ğŸ“ **æ¨™æº–åšæ³•**ï¼šPyTorch/TensorFlow éƒ½æ˜¯è¨“ç·´æ™‚è¨ˆç®—
5. ğŸ”„ **è‡ªå‹•é©æ‡‰**ï¼šè¨“ç·´é›†è®ŠåŒ–æ™‚æ¬Šé‡è‡ªå‹•æ›´æ–°

---

## ç‰¹æ®Šæƒ…æ³ï¼šä½•æ™‚è€ƒæ…®æ–¹æ¡ˆ Aï¼Ÿ

### æƒ…å¢ƒ 1ï¼šè¶…å¤§è¦æ¨¡æ•¸æ“šé›†
```python
# å¦‚æœæ•¸æ“šé‡æ¥µå¤§ï¼Œæ¬Šé‡è¨ˆç®—å¯èƒ½è¼ƒæ…¢
n_samples = 100_000_000  # 1 å„„æ¨£æœ¬
compute_weights(...)  # å¯èƒ½éœ€è¦å¹¾åˆ†é˜
```
**æ­¤æ™‚**: é å…ˆè¨ˆç®—å¯ç¯€çœæ™‚é–“

---

### æƒ…å¢ƒ 2ï¼šè¤‡é›œè‡ªå®šç¾©æ¬Šé‡
```python
# å¦‚æœæ¬Šé‡åŸºæ–¼å¤–éƒ¨è¤‡é›œè¨ˆç®—
def compute_complex_weights(labels, market_data, sentiment_scores):
    # è¤‡é›œè¨ˆç®—é‚è¼¯ï¼ˆå¯èƒ½éœ€è¦å¹¾åˆ†é˜ï¼‰
    weights = []
    for i, label in enumerate(labels):
        market_factor = analyze_market(market_data[i])
        sentiment_factor = analyze_sentiment(sentiment_scores[i])
        weight = custom_formula(label, market_factor, sentiment_factor)
        weights.append(weight)
    return np.array(weights)
```
**æ­¤æ™‚**: é å…ˆè¨ˆç®—é¿å…é‡è¤‡è€—æ™‚

---

### æƒ…å¢ƒ 3ï¼šéœ€è¦å¤šç¨®ç­–ç•¥å°æ¯”
```python
# å¦‚æœæƒ³åŒæ™‚å˜—è©¦å¤šç¨®æ¬Šé‡ç­–ç•¥
metadata['weight_strategies'] = {
    'balanced': {...},
    'focal_gamma_1': {...},
    'focal_gamma_2': {...},
    'effective_num_beta_0.9': {...},
    'effective_num_beta_0.99': {...},
    'custom_v1': {...},
    'custom_v2': {...}
}
```
**æ­¤æ™‚**: é å…ˆè¨ˆç®—å¯åœ¨è¨“ç·´æ™‚å¿«é€Ÿåˆ‡æ›

---

## å¯¦ä½œå»ºè­° ğŸ’¡

### å¦‚æœé¸æ“‡æ–¹æ¡ˆ Aï¼ˆé å…ˆä¿å­˜ï¼‰

**æœ€ä½³å¯¦è¸**ï¼š
1. **ä¿å­˜å¤šç¨®ç­–ç•¥**ï¼Œè€Œéå–®ä¸€ç­–ç•¥
2. **åœ¨ metadata ä¸­è¨˜éŒ„**æ¬Šé‡è¨ˆç®—æ–¹æ³•
3. **æä¾›é™ç´šæ–¹æ¡ˆ**ï¼ˆå¦‚æœ NPZ ä¸­ç„¡æ¬Šé‡ï¼Œè‡ªå‹•è¨ˆç®—ï¼‰
4. **å®šæœŸé©—è­‰**æ¬Šé‡èˆ‡æ¨™ç±¤çš„ä¸€è‡´æ€§

**ç¯„ä¾‹å¯¦ä½œ**ï¼š
```python
# preprocess_single_day.py
weight_strategies = {
    'balanced': compute_balanced_weights(labels),
    'focal': compute_focal_weights(labels, gamma=2.0),
    'none': np.ones_like(labels)  # ä¸ä½¿ç”¨æ¬Šé‡
}

metadata['weight_strategies'] = {
    name: {
        'class_weights': {...},
        'method': '...',
        'params': {...}
    }
    for name, weights in weight_strategies.items()
}

# åªä¿å­˜ä¸€ç¨®åˆ° NPZï¼ˆç¯€çœç©ºé–“ï¼‰ï¼Œå…¶ä»–å­˜ metadata
np.savez_compressed(
    npz_path,
    sample_weights=weight_strategies['balanced'],  # é è¨­ä½¿ç”¨ balanced
    ...
)
```

---

### å¦‚æœé¸æ“‡æ–¹æ¡ˆ Bï¼ˆè¨“ç·´æ™‚è¨ˆç®—ï¼‰

**æœ€ä½³å¯¦è¸**ï¼š
1. **å°è£æ¬Šé‡è¨ˆç®—å‡½æ•¸**
2. **è¨˜éŒ„ä½¿ç”¨çš„æ¬Šé‡ç­–ç•¥**
3. **æä¾›å¤šç¨®ç­–ç•¥é¸é …**

**ç¯„ä¾‹å¯¦ä½œ**ï¼š
```python
# utils/weight_utils.py
def get_sample_weights(labels, strategy='balanced', **kwargs):
    """
    è¨ˆç®—æ¨£æœ¬æ¬Šé‡

    Args:
        labels: æ¨™ç±¤é™£åˆ—
        strategy: 'balanced', 'focal', 'effective_num', 'custom'
        **kwargs: ç­–ç•¥åƒæ•¸
    """
    if strategy == 'balanced':
        return compute_balanced_weights(labels)
    elif strategy == 'focal':
        gamma = kwargs.get('gamma', 2.0)
        return compute_focal_weights(labels, gamma)
    elif strategy == 'effective_num':
        beta = kwargs.get('beta', 0.999)
        return compute_effective_num_weights(labels, beta)
    elif strategy == 'custom':
        return kwargs.get('weights')
    else:
        return np.ones_like(labels)

# train_deeplob_generic.py
weights = get_sample_weights(
    train_labels,
    strategy=config.weight_strategy,  # å¾é…ç½®è®€å–
    gamma=config.focal_gamma
)
```

---

## çµè«–è¡¨æ ¼

| è©•ä¼°ç¶­åº¦ | æ–¹æ¡ˆ Aï¼ˆé å…ˆä¿å­˜ï¼‰ | æ–¹æ¡ˆ Bï¼ˆè¨“ç·´æ™‚è¨ˆç®—ï¼‰ | æ¨è–¦ |
|---------|-------------------|---------------------|------|
| **è¨ˆç®—é–‹éŠ·** | é è™•ç†æ™‚ä¸€æ¬¡ | è¨“ç·´æ™‚ < 0.01s | - |
| **éˆæ´»æ€§** | â­â­ | â­â­â­â­â­ | âœ… B |
| **æ–‡ä»¶å¤§å°** | +20% | ç„¡å¢åŠ  | âœ… B |
| **å¯¦é©—ä¸€è‡´æ€§** | â­â­â­â­â­ | â­â­â­â­ | - |
| **ç¶­è­·æˆæœ¬** | è¼ƒé«˜ | è¼ƒä½ | âœ… B |
| **æ¨™æº–åšæ³•** | âŒ | âœ… | âœ… B |
| **é©æ‡‰æ€§** | â­â­ | â­â­â­â­â­ | âœ… B |
| **è¤‡é›œåº¦** | è¼ƒé«˜ | è¼ƒä½ | âœ… B |

---

## æœ€çµ‚æ¨è–¦ ğŸ†

**æ¨è–¦æ–¹æ¡ˆ Bï¼šè¨“ç·´æ™‚å‹•æ…‹è¨ˆç®—æ¬Šé‡**

**æ ¸å¿ƒç†ç”±**ï¼š
1. æ¬Šé‡è¨ˆç®—é€Ÿåº¦æ¥µå¿«ï¼ˆ< 0.01 ç§’ï¼‰ï¼Œé å…ˆè¨ˆç®—ç„¡æ˜é¡¯å„ªå‹¢
2. èª¿åƒéˆæ´»æ€§è‡³é—œé‡è¦ï¼Œé å…ˆä¿å­˜æœƒé™åˆ¶å¯¦é©—
3. ç¬¦åˆ PyTorch/TensorFlow ä¸»æµåšæ³•
4. ç¯€çœç£ç¢Ÿç©ºé–“ï¼ˆå…¨å¹´æ•¸æ“šå¯ç¯€çœ 2.5 GBï¼‰

**é©ç”¨å ´æ™¯**ï¼šâœ… 99% çš„æƒ…æ³

---

**ä¾‹å¤–æƒ…æ³**ï¼šåªæœ‰åœ¨ä»¥ä¸‹**åŒæ™‚æ»¿è¶³**æ™‚æ‰è€ƒæ…®æ–¹æ¡ˆ Aï¼š
1. æ¬Šé‡è¨ˆç®—éå¸¸è¤‡é›œï¼ˆ> 1 åˆ†é˜ï¼‰
2. éœ€è¦å¤šæ¬¡é‡è¤‡è¨“ç·´ï¼ˆ> 100 æ¬¡ï¼‰
3. ç£ç¢Ÿç©ºé–“å……è¶³

---

**æ—¥æœŸ**: 2025-10-23
**ç‰ˆæœ¬**: v1.0
