# DeepLOB 調參歷史

### 每次保留數據及成果,簡要說明(其他廢話不用),這是改下次調參參考用

#### 環境: conda activate deeplob-pro

---

## 2025-10-18 #1 - 嚴重過擬合

**數據（Epoch 1→19）**：

- Train Loss: 0.799→0.601 / Val Loss: 0.792→**0.851**（上升）
- Train Acc: 49.2%→63.0% / Val Acc: 43.9%→45.4%（差距17.6%）
- Grad Norm: 1.55→**11.89** / Val F1: 28.8%→31.8%

**問題**：驗證損失持續上升、梯度爆炸、正則化不足

**調整**：
```yaml
batch_size: 512→256
lstm/fc_hidden: 64→48
dropout: 0.3→0.5
weight_decay: 0.0001→0.001
grad_clip: 0.2→0.5
label_smoothing: 0.05→0.1
epochs: 50→30 / patience: 10→5
```

**預期**：val loss下降、train/val差距<10%、grad_norm<8、val_f1>40%

**下次關注**：若仍過擬合→lstm=32；若欠擬合→lr=3e-5

---

## 2025-10-18 #2 - 欠擬合（模型完全學不動）

**數據（Epoch 1→10）**：

- Train Loss: 0.276→0.248 / Val Loss: 0.273→0.277（微升）
- Train Acc: 2.87%→9.91% / Val Acc: 2.73%→**3.04%**（災難性低）
- Val F1 (W): 0.0152→0.0222 / Grad Norm: 0.59→1.74（過低）

**問題**：學習率太低 + 過度正則化 → 10 epochs 後 Val Acc 才 3%（隨機猜 33.3%）

**調整**：
```yaml
lr: 0.00001→0.0003（提高30倍）
weight_decay: 0.001→0.0001（降低10倍）
grad_clip: 0.5→2.0（放寬4倍）
dropout: 0.5→0.3（降低正則化）
label_smoothing: 0.1→0.05（降低平滑）
warmup_ratio: 0.05→0.1（延長穩定期）
patience: 5→10（給更多機會）
```

**預期**：Epoch3 Val Acc>40% / Epoch5 Val Acc>60% / Epoch10 Val F1(W)>0.65

**下次關注**：若 Epoch3 仍<30% → lr=0.0005；若過擬合回來 → dropout=0.4

