# V7.1 修復：滑動窗口不跨文件

**版本**: v7.1.0-no-cross-file
**更新日期**: 2025-10-25
**問題類型**: 數據質量 / 時間序列完整性
**嚴重程度**: ⭐⭐⭐⭐⭐ 高

---

## 問題描述

### 發現的問題

在 V7.0.0 版本中，滑動窗口生成邏輯存在**跨文件拼接**問題：

**V7.0.0 錯誤邏輯**：
```python
# 合併該股票所有天的數據
all_features = []
for date, features, labels, ... in sorted(stock_data[sym]):
    all_features.append(features)  # ❌ 直接拼接不同日期

concat_features = np.vstack(all_features)  # ❌ 跨文件合併

# 生成滑動窗口
for i in range(T - SEQ_LEN):
    X_window = concat_features[i:i+SEQ_LEN]  # ❌ 可能包含不連續時間點
```

**問題示例**：
```
2025-09-01.npz (2313 股票):
  時間戳: 09:00:00 ~ 13:30:00 (450 筆)
  最後一筆: 13:30:00

2025-09-02.npz (2313 股票):
  第一筆: 09:00:00  # ❌ 與前一天相隔 16 小時！

錯誤窗口示例:
  窗口 [351:451]:
    - [0:50]   來自 9/1 最後 50 筆 (12:40 ~ 13:30)
    - [50:100] 來自 9/2 前 50 筆 (09:00 ~ 09:50)  # ❌ 中間斷層！
```

### 影響範圍

1. **時間連續性破壞**：
   - 同一個窗口內包含不連續的時間點（隔夜間隔 16 小時）
   - DeepLOB 模型期望連續的 LOB 數據

2. **標籤污染**：
   - 窗口最後一個 timestep 的標籤可能是跨日的
   - 價格變動計算錯誤（9/1 收盤價 vs 9/2 開盤價）

3. **預測失真**：
   - 模型學習到錯誤的價格模式
   - 可能導致過擬合或準確率下降

4. **數據規模影響**：
   - 假設每個股票每天 400 筆，100 timestep 窗口
   - 每天最後 99 筆會與次日數據混合
   - 影響樣本數：`99 / 400 × 總樣本數 ≈ 24.75%`

---

## 解決方案

### V7.1 修復邏輯

**核心改動**：改為**逐個文件獨立處理**，確保滑動窗口不跨越文件邊界。

**V7.1 正確邏輯**：
```python
# 逐個日期文件處理（避免跨文件窗口）
file_data_list = sorted(stock_data[sym], key=lambda x: x[0])

for date, features, labels, last_prices, last_volumes, total_volumes in file_data_list:
    # 檢查該文件是否有價格/成交量數據
    has_price_data = (last_prices is not None and ...)

    # 應用標準化到當前文件
    features_norm = zscore_apply(features, ...)

    # 檢查文件長度
    T = len(features_norm)
    if T < SEQ_LEN:
        continue  # 數據不足 100，跳過此文件

    # 【關鍵】在單個文件內生成滑動窗口（確保時間連續性）
    for i in range(T - SEQ_LEN + 1):  # +1 確保最後一個窗口也被包含
        X_window = features_norm[i:i+SEQ_LEN]  # (100, 20) - 來自同一天
        y_label = labels[i+SEQ_LEN-1]          # 同一天的標籤

        # 提取價格和成交量窗口
        if all_have_price_data and has_price_data:
            price_window = last_prices[i:i+SEQ_LEN]  # (100,) - 來自同一天
            volume_window = ...

        # 分配到 train/val/test
        if sym in train_symbols:
            train_X.append(X_window)
            ...
```

### 關鍵修復點

1. **逐文件循環**（第 820 行）：
   ```python
   for date, features, labels, ... in file_data_list:
   ```

2. **文件內滑窗**（第 849 行）：
   ```python
   for i in range(T - SEQ_LEN + 1):  # 只在當前文件內滑動
   ```

3. **長度檢查**（第 843-846 行）：
   ```python
   if T < SEQ_LEN:
       continue  # 跳過長度不足的文件
   ```

4. **統計修正**（第 916-918 行）：
   ```python
   windows_in_file = T - SEQ_LEN + 1
   global_stats["valid_windows"] += windows_in_file
   ```

---

## 數據影響分析

### 樣本數量變化

**理論影響**：
- **V7.0.0（錯誤）**：每個股票所有天的數據拼接，生成 `T_total - 100` 個窗口
- **V7.1（正確）**：每天獨立生成窗口，總窗口數 = `Σ (T_day - 100 + 1)`

**舉例**（假設某股票有 3 天數據）：
```
日期          筆數   V7.0 窗口貢獻   V7.1 窗口貢獻
2025-09-01    400       -              301 (400-100+1)
2025-09-02    420       -              321
2025-09-03    390       -              291
--------------------------------
拼接後        1210     1111           913 (總計)

差異：V7.1 比 V7.0 少 198 個窗口 (-17.8%)
原因：每個文件邊界損失 99 個窗口（2 個邊界 = 198）
```

**預期結果**：
- 樣本數量**略微減少**（約 10-20%，取決於平均文件長度）
- 數據質量**顯著提升**（100% 時間連續）

### 檔案長度不足處理

**新增邏輯**：
```python
if T < SEQ_LEN:
    continue  # 跳過長度不足 100 的文件
```

**影響分析**：
- 假設聚合因子 `aggregation_factor = 10` 秒
- 100 timesteps = 1000 秒 ≈ 16.7 分鐘
- 交易時間 9:00-13:30 = 4.5 小時 = 270 分鐘
- 預期每天筆數：`270 × 60 / 10 = 1620` 筆
- 結論：**正常交易日都會遠超 100 筆，影響極小**

---

## 驗證方法

### 測試腳本

運行以下命令驗證修復：
```bash
# 1. 重新生成訓練數據
python scripts/extract_tw_stock_data_v7.py \
    --preprocessed-dir ./data/preprocessed_v5 \
    --output-dir ./data/processed_v7_test \
    --config ./configs/config_pro_v7_optimal.yaml

# 2. 檢查樣本數量
python -c "
import numpy as np
train = np.load('data/processed_v7_test/npz/stock_embedding_train.npz')
print(f'Train samples: {len(train[\"X\"])}')
print(f'Train labels: {len(train[\"y\"])}')
print(f'Shape: {train[\"X\"].shape}')
"

# 3. 檢查是否有跨文件窗口（需額外腳本）
```

### 檢查項目

1. **樣本數量**：
   - V7.1 應比 V7.0 略少（10-20%）
   - train + val + test 總數應一致

2. **標籤分布**：
   - 檢查 `normalization_meta.json`
   - 三類標籤比例應接近目標（30/40/30）

3. **時間連續性**（可選）：
   - 檢查每個窗口的 `stock_ids` 和日期標記
   - 確保同一窗口內沒有日期跳變

---

## 後續建議

### 數據增強（可選）

如果希望增加樣本數量，可考慮：

1. **降低 `aggregation_factor`**：
   - 從 10 秒降到 5 秒
   - 每天樣本數翻倍（1620 → 3240）
   - 代價：數據冗餘增加

2. **多尺度窗口**：
   - 同時使用 50/100/200 timesteps
   - 需修改模型架構

3. **數據擴增**（不推薦）：
   - 添加噪聲、時間平移等
   - 可能破壞 LOB 數據的真實性

### 模型訓練

1. **對比測試**：
   - 使用 V7.0 和 V7.1 數據分別訓練
   - 對比測試準確率（預期 V7.1 更高）

2. **監控指標**：
   - 檢查 F1 Score、Sharpe Ratio
   - 關注是否有過擬合改善

---

## 相關文件

- **修改文件**：[scripts/extract_tw_stock_data_v7.py](../scripts/extract_tw_stock_data_v7.py)
- **配置文件**：[configs/config_pro_v7_optimal.yaml](../configs/config_pro_v7_optimal.yaml)
- **預處理文檔**：[V6_TWO_STAGE_PIPELINE_GUIDE.md](V6_TWO_STAGE_PIPELINE_GUIDE.md)
- **標籤方法**：[TREND_LABELING_IMPLEMENTATION.md](TREND_LABELING_IMPLEMENTATION.md)

---

## 總結

### 修復內容

✅ **修復滑動窗口跨文件問題**
✅ **改為逐個 NPZ 文件獨立處理**
✅ **確保每個窗口的 100 個 timesteps 來自同一天（時間連續）**
✅ **避免不同日期數據混合（例如 9/1 最後 50 筆 + 9/2 前 50 筆）**

### 影響評估

- **數據質量**: ⬆️⬆️⬆️ 顯著提升（時間連續性 100%）
- **樣本數量**: ⬇️ 略微減少（10-20%，可接受）
- **訓練速度**: ➡️ 無影響
- **模型準確率**: ⬆️ 預期提升（避免學習錯誤模式）

### 下一步

1. 重新生成訓練數據（使用 V7.1）
2. 對比 V7.0 和 V7.1 的模型性能
3. 更新 CLAUDE.md 和相關文檔

---

**版本歷史**:
- **v7.0.0-simplified** (2025-10-23): 簡化版初始發布
- **v7.1.0-no-cross-file** (2025-10-25): 修復滑動窗口跨文件問題
