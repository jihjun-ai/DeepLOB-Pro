# 當日收盤未平倉懲罰機制說明

## 文件信息

- **建立日期**: 2025-10-27
- **版本**: v1.0
- **適用版本**: PPO_12+
- **核心模組**: `src/envs/tw_lob_trading_env.py`

---

## 一、策略背景

### 問題演進

| 版本 | 獎勵策略 | 問題 | 結果 |
|------|---------|------|------|
| **PPO_9** | PnL - 成本 | 模型學會「永不交易」 | Buy 0%，獎勵 0.00 |
| **PPO_11** | PnL - 成本 + 持倉激勵 (0.05) | 模型學會「永遠持倉」 | Buy 99.4%，幾乎不離場 |
| **PPO_12** | PnL - 成本 + **當日收盤未平倉懲罰** | ✅ 鼓勵當沖交易 | 預期 Buy 20-40%，正常平倉 |

### 為什麼選擇「當日收盤未平倉懲罰」？

1. **符合台股當沖規則** ⭐⭐⭐⭐⭐
   - 台股當沖必須當日平倉
   - 避免隔夜風險
   - 模擬實際交易約束

2. **解決「永遠持倉」問題**
   - PPO_11: 持倉激勵導致模型一直持倉到 Episode 結束
   - PPO_12: 收盤未平倉會受到懲罰，模型被迫學習離場時機

3. **不影響日內交易靈活性**
   - 只在 Episode 結束時檢查
   - 日內可以自由持倉
   - 不限制交易頻率

---

## 二、實施方式

### 代碼實現

**文件**: `src/envs/tw_lob_trading_env.py` (Line 376-385)

```python
# ===== 當日收盤未平倉懲罰 ===== (策略2: 台股當沖規則)
# 如果 Episode 結束時仍有持倉，給予懲罰（模擬當沖必須平倉的規則）
if truncated and self.position > 0:
    # 未平倉懲罰：基於持倉大小
    unclosed_penalty = -10.0 * self.position  # 每單位持倉懲罰 10 元
    reward += unclosed_penalty

    # 記錄到 reward_info
    if 'unclosed_position_penalty' not in reward_info:
        reward_info['unclosed_position_penalty'] = unclosed_penalty
```

### 關鍵參數

| 參數 | 值 | 說明 |
|------|---|------|
| **懲罰係數** | -10.0 | 每單位持倉懲罰 10 元 |
| **觸發條件** | `truncated and position > 0` | Episode 結束且有持倉 |
| **懲罰計算** | `-10.0 × position` | 持倉越大，懲罰越重 |

---

## 三、懲罰強度分析

### 數值設計理念

**為什麼是 -10.0？**

1. **對比持倉激勵**（PPO_11）:
   - 持倉激勵: +0.05/步 × 500 步 = +25 元
   - 未平倉懲罰: -10.0 × 1 = -10 元
   - **結論**: 日內持倉仍有正向激勵，但收盤必須平倉

2. **對比交易成本**:
   - 台股往返成本: ~0.24%
   - 100 元股票往返成本: ~0.24 元
   - 未平倉懲罰 (-10 元) >> 交易成本 (-0.24 元)
   - **結論**: 懲罰足夠強，模型會主動平倉

3. **對比典型 PnL**:
   - PPO_11 平均 Episode 獎勵: 24.15 元
   - 未平倉懲罰: -10.0 元（約 41% 的獎勵）
   - **結論**: 懲罰顯著但不極端，模型仍有盈利空間

### 懲罰強度對比表

| 情境 | 懲罵值 | 佔總獎勵比例 | 影響 |
|------|--------|-------------|------|
| **持倉 0.5 單位** | -5.0 元 | ~21% | 輕度懲罰 |
| **持倉 1.0 單位** | -10.0 元 | ~41% | 中度懲罰 ⭐ |
| **持倉 1.5 單位** | -15.0 元 | ~62% | 重度懲罰 |
| **持倉 2.0 單位** | -20.0 元 | ~83% | 極重懲罰 |

---

## 四、預期效果

### 訓練目標

1. **學習平倉時機** ⭐⭐⭐⭐⭐
   - 模型會學習在 Episode 結束前平倉
   - 預期最後 10-50 步會出現賣出動作

2. **適度交易頻率**
   - Buy 動作比例: 20-40%（目標）
   - 實際交易次數: 5-20 次/Episode（目標）
   - 避免「永不交易」和「永遠持倉」兩個極端

3. **符合當沖規則**
   - 模型學會當日買入、當日賣出
   - 模擬實際台股當沖交易

### 與其他策略對比

| 策略 | PPO_9 | PPO_11 | PPO_12 (新) |
|------|-------|--------|------------|
| **獎勵函數** | PnL - 成本 | PnL - 成本 + 持倉激勵 | PnL - 成本 + 未平倉懲罰 |
| **Buy 比例** | 0% | 99.4% | 20-40% (預期) |
| **交易行為** | 永不交易 | 永遠持倉 | 適度交易 (預期) |
| **平倉行為** | N/A | 幾乎不平倉 | 收盤前平倉 (預期) |
| **符合當沖** | ❌ | ❌ | ✅ |

---

## 五、實施步驟

### Step 1: 移除持倉激勵

**文件**: `src/envs/reward_shaper.py` (Line 206-209)

```python
# ===== 組件5: 持倉激勵 ===== (已移除，PPO_12)
# 移除原因: PPO_11 證明持倉激勵導致「永遠持倉」問題
#           改用「當日收盤未平倉懲罰」策略（在環境中實施）
reward_components['holding_bonus'] = 0.0
```

### Step 2: 添加未平倉懲罰

**文件**: `src/envs/tw_lob_trading_env.py` (Line 376-385)

```python
# ===== 當日收盤未平倉懲罰 =====
if truncated and self.position > 0:
    unclosed_penalty = -10.0 * self.position
    reward += unclosed_penalty
    reward_info['unclosed_position_penalty'] = unclosed_penalty
```

### Step 3: 訓練 PPO_12

```bash
# 配置
total_timesteps: 200000  # 快速測試
# 其他參數保持 PPO_11 配置

# 訓練
python scripts/train_sb3_deeplob.py --config configs/sb3_deeplob_config.yaml

# 驗證
python scripts/check_trading_behavior.py --model checkpoints/sb3/ppo_deeplob/best_model
```

---

## 六、診斷與調整

### 如果 Buy 比例仍過高（> 50%）

**問題**: 懲罰不夠強
**解決**: 增加懲罰係數

```python
unclosed_penalty = -20.0 * self.position  # 從 -10.0 提高到 -20.0
```

### 如果 Buy 比例過低（< 10%）

**問題**: 懲罰過強或回到「永不交易」
**解決**: 降低懲罰係數或增加小額持倉激勵

```python
unclosed_penalty = -5.0 * self.position  # 從 -10.0 降低到 -5.0

# 或結合小額持倉激勵
holding_bonus = 0.01  # 從 0.05 降低到 0.01
```

### 如果模型仍不平倉

**問題**: 懲罰係數太小
**解決**: 大幅提高懲罰或改用百分比懲罰

```python
# 方案1: 提高固定懲罰
unclosed_penalty = -50.0 * self.position

# 方案2: 使用百分比懲罰（基於 Episode 總獎勵）
unclosed_penalty = -0.5 * episode_total_reward * self.position
```

---

## 七、監控指標

### 訓練期間監控

1. **unclosed_position_penalty**: 查看 TensorBoard 或訓練日誌
   - 數值越接近 0 → 模型學會平倉
   - 數值仍為 -10.0 → 模型未學會平倉

2. **Episode 最後 50 步的動作分布**:
   - 應該看到更多 Sell (action=0) 動作
   - 表示模型學習在收盤前平倉

3. **測試集驗證** (check_trading_behavior.py):
   - Buy 比例: 目標 20-40%
   - 實際交易次數: 目標 5-20 次/Episode
   - 收盤持倉比例: 目標 < 10%

---

## 八、優勢與限制

### 優勢 ✅

1. **符合台股規則**: 模擬當沖必須當日平倉
2. **簡單有效**: 只需修改一處代碼
3. **不影響日內靈活性**: 只在收盤時懲罰
4. **易於調整**: 懲罰係數可配置
5. **解決極端策略**: 避免「永不交易」和「永遠持倉」

### 限制 ⚠️

1. **固定懲罰**: 不考慮市場狀況（如大幅盈利時可能值得持倉過夜）
2. **二元判斷**: 只檢查是否平倉，不考慮平倉價格是否優質
3. **可能過於保守**: 某些情況下提前平倉可能錯失更大利潤

### 改進方向

1. **動態懲罰**: 基於當前盈虧調整懲罰強度
2. **分階段懲罰**: Episode 最後 N 步逐漸增加懲罰
3. **結合 DeepLOB 預測**: 如果預測上漲，允許持倉過夜

---

## 九、總結

**當日收盤未平倉懲罰**是一種簡單但有效的策略，用於：
1. ✅ 解決 PPO_11 的「永遠持倉」問題
2. ✅ 符合台股當沖交易規則
3. ✅ 鼓勵模型學習完整的進場-離場策略

**推薦配置**:
- 懲罰係數: `-10.0` （可根據訓練結果調整）
- 適用場景: 當沖交易、日內短線
- 預期效果: Buy 20-40%，正常平倉行為

**下一步**:
- 訓練 PPO_12 (200K steps)
- 驗證交易行為
- 根據結果調整懲罰係數

---

**最後更新**: 2025-10-27
**版本**: v1.0
