"""å°è‚¡æ•¸æ“šæä¾›è€… - è¼‰å…¥é è™•ç†å¥½çš„å°è‚¡ LOB embedding æ•¸æ“š

æ­¤æ¨¡çµ„è² è²¬è¼‰å…¥å·²é è™•ç†çš„å°è‚¡ LOB æ•¸æ“šï¼ˆstock_embedding_*.npzï¼‰ï¼Œ
ä¸¦æä¾›çµ¦äº¤æ˜“ç’°å¢ƒä½¿ç”¨ã€‚èˆ‡ EnvDataProvider ä¸åŒï¼Œé€™å€‹æä¾›è€…ï¼š
  1. è¼‰å…¥ .npz æ ¼å¼çš„é è™•ç†æ•¸æ“šï¼ˆä¸æ˜¯åŸå§‹ CSVï¼‰
  2. æ”¯æ´ 5æª” LOBï¼ˆ20ç¶­ç‰¹å¾µï¼‰è€Œé 10æª”ï¼ˆ40ç¶­ï¼‰
  3. ç›´æ¥ä½¿ç”¨ embedding æ•¸æ“šï¼Œç„¡éœ€é¡å¤–é è™•ç†

æ•¸æ“šæ ¼å¼:
    X: (N, 100, 20) - Nå€‹æ¨£æœ¬ï¼Œæ¯å€‹100æ™‚é–“æ­¥ Ã— 20ç¶­LOBç‰¹å¾µ
    y: (N,) - åƒ¹æ ¼è®Šå‹•æ¨™ç±¤ {0: ä¸‹è·Œ, 1: æŒå¹³, 2: ä¸Šæ¼²}
    stock_ids: (N,) - è‚¡ç¥¨ç·¨è™Ÿ

ä½¿ç”¨ç¯„ä¾‹:
    >>> provider = TaiwanStockDataProvider(data_dir="data/processed")
    >>> train_data, train_labels = provider.get_train_data()
    >>> print(train_data.shape)  # (5584553, 100, 20)

ä½œè€…: RLlib-DeepLOB å°ˆæ¡ˆåœ˜éšŠ
æ›´æ–°: 2025-10-12
"""

import numpy as np
from pathlib import Path
from typing import Tuple, Optional, Dict
import logging

logger = logging.getLogger(__name__)

# å…¨åŸŸæ•¸æ“šå¿«å–ï¼ˆè·¨ Worker å…±äº«ï¼‰
_GLOBAL_DATA_CACHE = {
    'train': None,
    'val': None,
    'test': None,
}


class TaiwanStockDataProvider:
    """å°è‚¡æ•¸æ“šæä¾›è€…

    æ­¤é¡åˆ¥è² è²¬ç‚ºäº¤æ˜“ç’°å¢ƒæä¾›é è™•ç†å¥½çš„å°è‚¡ LOB æ•¸æ“šã€‚
    èˆ‡ FI-2010 æ•¸æ“šä¸åŒï¼Œå°è‚¡æ•¸æ“šå·²ç¶“éå®Œæ•´é è™•ç†ä¸¦ä¿å­˜ç‚º .npz æ ¼å¼ã€‚

    è¨­è¨ˆç‰¹é»:
        - è¼‰å…¥é è™•ç†æ•¸æ“šï¼šç›´æ¥è®€å– stock_embedding_*.npz
        - 5æª” LOBï¼š20ç¶­ç‰¹å¾µï¼ˆè²·è³£å„5æª”åƒ¹é‡ï¼‰
        - æ™‚åºçª—å£ï¼š100æ™‚é–“æ­¥
        - æ•¸æ“šå¿«å–ï¼šé¿å…é‡è¤‡è¼‰å…¥
        - åˆ†å‰²ç®¡ç†ï¼štrain/val/test ä¸‰å€‹åˆ†å‰²

    æ•¸æ“šç¶­åº¦:
        - LOB ç‰¹å¾µ: (N, 100, 20)
          * N: æ¨£æœ¬æ•¸
          * 100: æ™‚é–“æ­¥æ•¸
          * 20: 5æª” LOB ç‰¹å¾µï¼ˆbid_priceÃ—5 + bid_volÃ—5 + ask_priceÃ—5 + ask_volÃ—5ï¼‰
        - æ¨™ç±¤: (N,) - {0: ä¸‹è·Œ, 1: æŒå¹³, 2: ä¸Šæ¼²}
        - è‚¡ç¥¨ID: (N,) - è‚¡ç¥¨ç·¨è™Ÿ
    """

    def __init__(
        self,
        data_dir: str = "data/processed",
        use_embedding: bool = True,
        sample_ratio: float = 1.0,
    ):
        """åˆå§‹åŒ–å°è‚¡æ•¸æ“šæä¾›è€…

        åƒæ•¸:
            data_dir: æ•¸æ“šç›®éŒ„ï¼ˆåŒ…å« stock_embedding_*.npz æ–‡ä»¶ï¼‰
            use_embedding: æ˜¯å¦ä½¿ç”¨ embedding æ•¸æ“šï¼ˆé è¨­ Trueï¼‰
            sample_ratio: æ•¸æ“šæ¡æ¨£æ¯”ä¾‹ï¼ˆ0.0-1.0ï¼‰ï¼Œé è¨­ 1.0ï¼ˆå…¨éƒ¨æ•¸æ“šï¼‰
                         è¨­ç‚º 0.1 å¯åªä½¿ç”¨ 10% æ•¸æ“šï¼Œå¤§å¹…æ¸›å°‘è¨˜æ†¶é«”ä½¿ç”¨
        """
        self.data_dir = Path(data_dir)
        self.use_embedding = use_embedding
        self.sample_ratio = max(0.01, min(1.0, sample_ratio))  # é™åˆ¶åœ¨ 1%-100%

        # æ•¸æ“šå¿«å–
        self._train_data = None
        self._train_labels = None
        self._val_data = None
        self._val_labels = None
        self._test_data = None
        self._test_labels = None

        # è¼‰å…¥ç‹€æ…‹
        self._is_loaded = False

        logger.info(
            f"å°è‚¡æ•¸æ“šæä¾›è€…å·²åˆå§‹åŒ–: "
            f"æ•¸æ“šç›®éŒ„={data_dir}, "
            f"ä½¿ç”¨ embedding={use_embedding}, "
            f"æ¡æ¨£æ¯”ä¾‹={self.sample_ratio:.1%}"
        )

    def _load_npz(self, filename: str) -> Tuple[np.ndarray, np.ndarray]:
        """è¼‰å…¥ .npz æ•¸æ“šæ–‡ä»¶ï¼ˆå„ªåŒ–è¨˜æ†¶é«”ä½¿ç”¨ï¼‰

        ã€ä¿®æ­£ã€‘ä½¿ç”¨åˆ†å¡Šè¼‰å…¥é¿å…è¨˜æ†¶é«”æº¢å‡º
        å•é¡Œï¼šX_full[indices] æœƒå¼·åˆ¶ NumPy å…ˆè¼‰å…¥æ•´å€‹é™£åˆ—ï¼ˆ41.6 GBï¼‰
        è§£æ³•ï¼šä½¿ç”¨ np.take åˆ†æ‰¹è¼‰å…¥ï¼Œæˆ–ç›´æ¥ç”¨é€£çºŒç´¢å¼•åˆ‡ç‰‡

        åƒæ•¸:
            filename: æ•¸æ“šæ–‡ä»¶åï¼ˆä¾‹å¦‚ "stock_embedding_train.npz"ï¼‰

        è¿”å›:
            (data, labels):
                - data: (N, 100, 20) LOB åºåˆ—
                - labels: (N,) åƒ¹æ ¼è®Šå‹•æ¨™ç±¤
        """
        filepath = self.data_dir / filename

        if not filepath.exists():
            raise FileNotFoundError(f"æ‰¾ä¸åˆ°æ•¸æ“šæ–‡ä»¶: {filepath}")

        logger.info(f"è¼‰å…¥æ•¸æ“šæ–‡ä»¶: {filepath}")

        # 1. å…ˆè¼‰å…¥æ¨™ç±¤ï¼ˆå¾ˆå°ï¼Œå¹¾ MBï¼‰
        logger.info(f"â³ æ­¥é©Ÿ 1/2: è¼‰å…¥æ¨™ç±¤...")
        with np.load(filepath) as npz_data:
            if 'X' not in npz_data or 'y' not in npz_data:
                raise ValueError(f"æ•¸æ“šæ–‡ä»¶æ ¼å¼éŒ¯èª¤ï¼Œç¼ºå°‘ 'X' æˆ– 'y' éµ: {filepath}")

            # åªè¼‰å…¥ yï¼ˆæ¨™ç±¤ï¼‰åˆ°è¨˜æ†¶é«”
            y_full = npz_data['y'][:]  # ä½¿ç”¨ [:] ç¢ºä¿è¤‡è£½
            n_samples = len(y_full)
            logger.info(f"âœ… æ¨™ç±¤è¼‰å…¥å®Œæˆ: {n_samples:,} å€‹æ¨£æœ¬")

        # 2. è¨ˆç®—æ¡æ¨£ç´¢å¼•
        if self.sample_ratio < 1.0:
            n_sampled = int(n_samples * self.sample_ratio)
            np.random.seed(42)  # å›ºå®šç¨®å­
            indices = np.random.choice(n_samples, size=n_sampled, replace=False)
            indices = np.sort(indices)  # ä¿æŒæ™‚åºæ€§
            logger.info(
                f"ğŸ“‰ æ•¸æ“šæ¡æ¨£: {n_samples:,} â†’ {n_sampled:,} æ¨£æœ¬ ({self.sample_ratio:.1%})"
            )
        else:
            indices = None
            n_sampled = n_samples

        # 3. è¼‰å…¥ Xï¼ˆå¤§æ•¸æ“šï¼Œä½¿ç”¨åˆ†å¡Šè¼‰å…¥é¿å…è¨˜æ†¶é«”çˆ†ç‚¸ï¼‰
        logger.info(f"â³ æ­¥é©Ÿ 2/2: è¼‰å…¥ LOB æ•¸æ“š...")

        if indices is not None:
            # ã€é—œéµä¿®æ­£ã€‘æ¡æ¨£æ¨¡å¼ï¼šä½¿ç”¨åˆ†å¡Šè¼‰å…¥é¿å…ä¸€æ¬¡æ€§è¼‰å…¥å…¨éƒ¨æ•¸æ“š
            # å•é¡Œï¼šX_full[indices] æœƒè§¸ç™¼ NumPy è¼‰å…¥æ•´å€‹ 41.6 GB é™£åˆ—
            # è§£æ³•ï¼šåˆ†æ‰¹è®€å–å°å¡Šæ•¸æ“š

            chunk_size = 50000  # æ¯æ¬¡è®€å– 5è¬å€‹æ¨£æœ¬ (~190 MB)
            X_list = []
            y = y_full[indices]  # å…ˆæ¡æ¨£æ¨™ç±¤

            logger.info(f"ğŸ“¦ ä½¿ç”¨åˆ†å¡Šè¼‰å…¥ (chunk_size={chunk_size:,})...")

            with np.load(filepath, mmap_mode='r') as npz_data:
                X_mmap = npz_data['X']

                # å°‡ç´¢å¼•åˆ†çµ„ï¼Œæ¯çµ„å¾é€£çºŒç¯„åœè®€å–
                n_chunks = (len(indices) + chunk_size - 1) // chunk_size
                for i in range(n_chunks):
                    start_idx = i * chunk_size
                    end_idx = min((i + 1) * chunk_size, len(indices))
                    chunk_indices = indices[start_idx:end_idx]

                    # è®€å–é€™ä¸€å¡Šçš„æ•¸æ“š
                    X_chunk = X_mmap[chunk_indices]
                    X_list.append(X_chunk.copy())  # è¤‡è£½åˆ°è¨˜æ†¶é«”

                    if (i + 1) % 10 == 0 or i == n_chunks - 1:
                        logger.info(f"  é€²åº¦: {i+1}/{n_chunks} chunks ({(i+1)/n_chunks*100:.1f}%)")

            # åˆä½µæ‰€æœ‰å¡Š
            X = np.concatenate(X_list, axis=0)

            # é©—è­‰å½¢ç‹€
            assert X.ndim == 3, f"X ç¶­åº¦éŒ¯èª¤: æœŸæœ›3ç¶­ï¼Œå¯¦éš›{X.ndim}ç¶­"
            assert X.shape[1] == 100, f"æ™‚é–“æ­¥éŒ¯èª¤: æœŸæœ›100ï¼Œå¯¦éš›{X.shape[1]}"
            assert X.shape[2] == 20, f"ç‰¹å¾µç¶­åº¦éŒ¯èª¤: æœŸæœ›20ï¼Œå¯¦éš›{X.shape[2]}"

            saved_memory = (n_samples - n_sampled) * 100 * 20 * 4 / 1e9
            logger.info(f"âœ… æ¡æ¨£æ•¸æ“šè¼‰å…¥å®Œæˆï¼Œç¯€çœè¨˜æ†¶é«”: {saved_memory:.2f} GB")
        else:
            # å…¨é‡æ¨¡å¼ï¼šè¼‰å…¥å…¨éƒ¨æ•¸æ“š
            with np.load(filepath) as npz_data:
                X = npz_data['X'][:]
            y = y_full
            logger.info(f"âœ… å…¨é‡æ•¸æ“šè¼‰å…¥å®Œæˆ: {len(X):,} å€‹æ¨£æœ¬")

        logger.info(
            f"âœ… æ•¸æ“šè¼‰å…¥æˆåŠŸ: å½¢ç‹€={X.shape}, dtype={X.dtype}, "
            f"è¨˜æ†¶é«”ä½¿ç”¨: {X.nbytes / 1e9:.2f} GB"
        )

        return X, y

    def get_train_data(self) -> Tuple[np.ndarray, np.ndarray]:
        """ç²å–è¨“ç·´é›†æ•¸æ“šï¼ˆä½¿ç”¨å…¨åŸŸå¿«å–é¿å…é‡è¤‡è¼‰å…¥ï¼‰

        è¿”å›:
            (train_data, train_labels):
                - train_data: (N, 100, 20) LOB åºåˆ—
                - train_labels: (N,) æ¨™ç±¤
        """
        global _GLOBAL_DATA_CACHE

        # å„ªå…ˆä½¿ç”¨å…¨åŸŸå¿«å–ï¼ˆè·¨ Worker å…±äº«ï¼‰
        if _GLOBAL_DATA_CACHE['train'] is not None:
            logger.info("âœ… ä½¿ç”¨å…¨åŸŸå¿«å–çš„è¨“ç·´é›†æ•¸æ“šï¼ˆé¿å…é‡è¤‡è¼‰å…¥ï¼‰")
            return _GLOBAL_DATA_CACHE['train']

        # å¦‚æœå…¨åŸŸå¿«å–ä¸å­˜åœ¨ï¼Œä½¿ç”¨å¯¦ä¾‹å¿«å–
        if self._train_data is None:
            logger.info("é¦–æ¬¡è¼‰å…¥è¨“ç·´é›†æ•¸æ“š...")
            self._train_data, self._train_labels = self._load_npz("stock_embedding_train.npz")
            # æ›´æ–°å…¨åŸŸå¿«å–
            _GLOBAL_DATA_CACHE['train'] = (self._train_data, self._train_labels)

        return self._train_data, self._train_labels

    def get_val_data(self) -> Tuple[np.ndarray, np.ndarray]:
        """ç²å–é©—è­‰é›†æ•¸æ“šï¼ˆä½¿ç”¨å…¨åŸŸå¿«å–ï¼‰

        è¿”å›:
            (val_data, val_labels):
                - val_data: (N, 100, 20) LOB åºåˆ—
                - val_labels: (N,) æ¨™ç±¤
        """
        global _GLOBAL_DATA_CACHE

        if _GLOBAL_DATA_CACHE['val'] is not None:
            logger.info("âœ… ä½¿ç”¨å…¨åŸŸå¿«å–çš„é©—è­‰é›†æ•¸æ“š")
            return _GLOBAL_DATA_CACHE['val']

        if self._val_data is None:
            logger.info("é¦–æ¬¡è¼‰å…¥é©—è­‰é›†æ•¸æ“š...")
            self._val_data, self._val_labels = self._load_npz("stock_embedding_val.npz")
            _GLOBAL_DATA_CACHE['val'] = (self._val_data, self._val_labels)

        return self._val_data, self._val_labels

    def get_test_data(self) -> Tuple[np.ndarray, np.ndarray]:
        """ç²å–æ¸¬è©¦é›†æ•¸æ“šï¼ˆä½¿ç”¨å…¨åŸŸå¿«å–ï¼‰

        è¿”å›:
            (test_data, test_labels):
                - test_data: (N, 100, 20) LOB åºåˆ—
                - test_labels: (N,) æ¨™ç±¤
        """
        global _GLOBAL_DATA_CACHE

        if _GLOBAL_DATA_CACHE['test'] is not None:
            logger.info("âœ… ä½¿ç”¨å…¨åŸŸå¿«å–çš„æ¸¬è©¦é›†æ•¸æ“š")
            return _GLOBAL_DATA_CACHE['test']

        if self._test_data is None:
            logger.info("é¦–æ¬¡è¼‰å…¥æ¸¬è©¦é›†æ•¸æ“š...")
            self._test_data, self._test_labels = self._load_npz("stock_embedding_test.npz")
            _GLOBAL_DATA_CACHE['test'] = (self._test_data, self._test_labels)

        return self._test_data, self._test_labels

    def get_prices(self, mode: str = 'train') -> Optional[np.ndarray]:
        """ç²å–çœŸå¯¦åƒ¹æ ¼æ•¸æ“šï¼ˆå¦‚æœæœ‰ï¼‰

        åƒæ•¸:
            mode: æ•¸æ“šæ¨¡å¼ 'train'/'val'/'test'

        è¿”å›:
            prices: (N,) åƒ¹æ ¼åºåˆ—ï¼Œå¦‚æœæ•¸æ“šä¸­æ²’æœ‰åƒ¹æ ¼å‰‡è¿”å› None
        """
        filename_map = {
            'train': 'stock_embedding_train.npz',
            'val': 'stock_embedding_val.npz',
            'test': 'stock_embedding_test.npz'
        }

        filepath = self.data_dir / filename_map[mode]

        if not filepath.exists():
            logger.warning(f"æ‰¾ä¸åˆ°æ•¸æ“šæ–‡ä»¶: {filepath}")
            return None

        try:
            with np.load(filepath) as data:
                # V7 æ•¸æ“šä½¿ç”¨ 'prices' (è¤‡æ•¸), å‘å¾Œå…¼å®¹ 'price' (å–®æ•¸)
                if 'prices' in data.keys():
                    prices_data = data['prices'][:]
                    # V7 æ ¼å¼: (N, 100) - æ¯å€‹æ¨£æœ¬ 100 å€‹æ™‚é–“æ­¥çš„åƒ¹æ ¼
                    # æˆ‘å€‘å–æœ€å¾Œä¸€å€‹æ™‚é–“æ­¥çš„åƒ¹æ ¼ (ç”¨æ–¼äº¤æ˜“æ±ºç­–)
                    if prices_data.ndim == 2:
                        logger.info(f"âœ… è¼‰å…¥çœŸå¯¦åƒ¹æ ¼æ•¸æ“š (prices): {mode} é›†, å½¢ç‹€ {prices_data.shape}, å–æœ€å¾Œæ™‚é–“æ­¥")
                        return prices_data[:, -1]  # å–æ¯å€‹æ¨£æœ¬çš„æœ€å¾Œä¸€å€‹åƒ¹æ ¼
                    else:
                        logger.info(f"âœ… è¼‰å…¥çœŸå¯¦åƒ¹æ ¼æ•¸æ“š (prices): {mode} é›†, å½¢ç‹€ {prices_data.shape}")
                        return prices_data
                elif 'price' in data.keys():
                    price_data = data['price'][:]
                    logger.info(f"âœ… è¼‰å…¥çœŸå¯¦åƒ¹æ ¼æ•¸æ“š (price): {mode} é›†, å½¢ç‹€ {price_data.shape}")
                    # èˆŠæ ¼å¼æ‡‰è©²æ˜¯ (N,) ä¸€ç¶­
                    if price_data.ndim == 2:
                        return price_data[:, -1]
                    return price_data
                else:
                    logger.warning(f"âš ï¸  æ•¸æ“šæ–‡ä»¶ä¸­æ²’æœ‰ 'price' æˆ– 'prices' å­—æ®µ: {filepath}")
                    return None
        except Exception as e:
            logger.error(f"âŒ è¼‰å…¥åƒ¹æ ¼æ•¸æ“šå¤±æ•—: {e}")
            return None

    def get_metadata(self) -> Dict:
        """ç²å–æ•¸æ“šé›†å…ƒæ•¸æ“š

        è¿”å›:
            metadata: åŒ…å«æ•¸æ“šé›†çµ±è¨ˆä¿¡æ¯çš„å­—å…¸
        """
        import json
        meta_path = self.data_dir / "meta.json"

        if not meta_path.exists():
            logger.warning(f"æ‰¾ä¸åˆ°å…ƒæ•¸æ“šæ–‡ä»¶: {meta_path}")
            return {}

        with open(meta_path, 'r', encoding='utf-8') as f:
            metadata = json.load(f)

        return metadata
