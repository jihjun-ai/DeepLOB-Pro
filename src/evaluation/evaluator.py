"""ç­–ç•¥è©•ä¼°å™¨ - è¨ˆç®—äº¤æ˜“ç¸¾æ•ˆæŒ‡æ¨™

æ­¤æ¨¡çµ„æä¾›å®Œæ•´çš„äº¤æ˜“ç­–ç•¥è©•ä¼°åŠŸèƒ½ï¼Œè¨ˆç®—å„ç¨®ç¸¾æ•ˆæŒ‡æ¨™ä¸¦èˆ‡åŸºæº–ç­–ç•¥å°æ¯”ã€‚

ä¸»è¦åŠŸèƒ½:
    1. é›¢ç·šè©•ä¼° RL ç­–ç•¥
    2. è¨ˆç®—ç¸¾æ•ˆæŒ‡æ¨™ (Sharpe, MDD, Win Rate, etc.)
    3. ç”Ÿæˆè©³ç´°è©•ä¼°å ±å‘Š
    4. èˆ‡åŸºæº–ç­–ç•¥å°æ¯”

ç¸¾æ•ˆæŒ‡æ¨™:
    - Sharpe Ratio: é¢¨éšªèª¿æ•´å¾Œæ”¶ç›Š
    - Max Drawdown (MDD): æœ€å¤§å›æ’¤
    - Calmar Ratio: æ”¶ç›Š/æœ€å¤§å›æ’¤
    - Win Rate: å‹ç‡
    - Profit Factor: ç›ˆè™§æ¯”
    - Total Return: ç¸½æ”¶ç›Šç‡
    - Avg Return per Trade: å¹³å‡æ¯ç­†äº¤æ˜“æ”¶ç›Š

ä½œè€…: RLlib-DeepLOB å°ˆæ¡ˆåœ˜éšŠ
æ›´æ–°: 2025-01-10
"""

import numpy as np
from typing import Dict, List, Tuple, Optional, Any
import logging
from pathlib import Path
import json
from datetime import datetime

logger = logging.getLogger(__name__)


# ============================================================================
# ç¸¾æ•ˆæŒ‡æ¨™è¨ˆç®—å‡½æ•¸
# ============================================================================

def calculate_sharpe_ratio(returns: np.ndarray, risk_free_rate: float = 0.0,
                          periods_per_year: int = 252) -> float:
    """è¨ˆç®— Sharpe Ratio (å¤æ™®æ¯”ç‡)

    Sharpe Ratio è¡¡é‡æ¯å–®ä½é¢¨éšªçš„è¶…é¡æ”¶ç›Šï¼Œæ˜¯æœ€å¸¸ç”¨çš„é¢¨éšªèª¿æ•´ç¸¾æ•ˆæŒ‡æ¨™ã€‚

    å…¬å¼: SR = (E[R] - Rf) / Ïƒ[R] * âˆšT

    åƒæ•¸:
        returns: æ”¶ç›Šç‡åºåˆ— (æ¯å€‹ episode æˆ–æ¯æ—¥æ”¶ç›Š)
        risk_free_rate: ç„¡é¢¨éšªåˆ©ç‡ (å¹´åŒ–)ï¼Œé è¨­ 0.0
        periods_per_year: å¹´åŒ–ä¿‚æ•¸ï¼Œé è¨­ 252 (äº¤æ˜“æ—¥)

    è¿”å›:
        sharpe_ratio: å¹´åŒ– Sharpe Ratio

    è§£é‡‹:
        - SR > 2.0: å„ªç§€
        - SR > 1.0: è‰¯å¥½
        - SR > 0.5: å¯æ¥å—
        - SR < 0: è² æ”¶ç›Š

    æ³¨æ„:
        - å‡è¨­æ”¶ç›Šç‡æœå¾æ­£æ…‹åˆ†ä½ˆ
        - å°æ¥µç«¯å€¼æ•æ„Ÿ
    """
    if len(returns) == 0:
        return 0.0

    mean_return = np.mean(returns)
    std_return = np.std(returns)

    if std_return == 0:
        return 0.0

    # å¹´åŒ– Sharpe Ratio
    sharpe = (mean_return - risk_free_rate) / std_return * np.sqrt(periods_per_year)
    return float(sharpe)


def calculate_max_drawdown(returns: np.ndarray) -> Tuple[float, int, int]:
    """è¨ˆç®—æœ€å¤§å›æ’¤ (Maximum Drawdown)

    æœ€å¤§å›æ’¤æ˜¯æŠ•è³‡çµ„åˆå¾å³°å€¼åˆ°è°·å€¼çš„æœ€å¤§è·Œå¹…ï¼Œè¡¡é‡ä¸‹è¡Œé¢¨éšªã€‚

    å…¬å¼: MDD = max((Peak - Trough) / Peak)

    åƒæ•¸:
        returns: æ”¶ç›Šç‡åºåˆ—æˆ–ç´¯ç©æ”¶ç›Šåºåˆ—

    è¿”å›:
        mdd: æœ€å¤§å›æ’¤æ¯”ä¾‹ (0-1ï¼Œå¦‚ 0.2 è¡¨ç¤º 20% å›æ’¤)
        start_idx: å›æ’¤é–‹å§‹ä½ç½® (å³°å€¼)
        end_idx: å›æ’¤çµæŸä½ç½® (è°·å€¼)

    è§£é‡‹:
        - MDD < 10%: é¢¨éšªå¾ˆä½
        - MDD < 20%: å¯æ¥å—
        - MDD > 30%: é¢¨éšªè¼ƒé«˜
        - MDD > 50%: é¢¨éšªå¾ˆé«˜

    æ³¨æ„:
        - å›æ’¤æœŸé–“å¯èƒ½å¾ˆé•·
        - ä¸è€ƒæ…®å›æ’¤ç™¼ç”Ÿçš„é »ç‡
    """
    if len(returns) == 0:
        return 0.0, 0, 0

    # è¨ˆç®—ç´¯ç©æ”¶ç›Šæ›²ç·š
    cumulative = np.cumsum(returns)

    # è¨ˆç®—æ­·å²æœ€é«˜é»
    running_max = np.maximum.accumulate(cumulative)

    # è¨ˆç®—å›æ’¤åºåˆ—
    drawdown = (cumulative - running_max)

    # æ‰¾åˆ°æœ€å¤§å›æ’¤
    max_dd_idx = np.argmin(drawdown)
    max_dd = drawdown[max_dd_idx]

    # æ‰¾åˆ°å°æ‡‰çš„å³°å€¼ä½ç½®
    peak_idx = np.argmax(cumulative[:max_dd_idx + 1]) if max_dd_idx > 0 else 0

    # æ¨™æº–åŒ–ç‚ºæ¯”ä¾‹
    if running_max[peak_idx] != 0:
        mdd_ratio = abs(max_dd / running_max[peak_idx])
    else:
        mdd_ratio = 0.0

    return float(mdd_ratio), int(peak_idx), int(max_dd_idx)


def calculate_calmar_ratio(total_return: float, max_drawdown: float) -> float:
    """è¨ˆç®— Calmar Ratio (å¡ç‘ªæ¯”ç‡)

    Calmar Ratio æ˜¯å¹´åŒ–æ”¶ç›Šç‡èˆ‡æœ€å¤§å›æ’¤çš„æ¯”å€¼ï¼Œè¡¡é‡æ¯å–®ä½å›æ’¤çš„æ”¶ç›Šã€‚

    å…¬å¼: Calmar = å¹´åŒ–æ”¶ç›Šç‡ / MDD

    åƒæ•¸:
        total_return: ç¸½æ”¶ç›Šç‡ (å¦‚ 0.5 è¡¨ç¤º 50% æ”¶ç›Š)
        max_drawdown: æœ€å¤§å›æ’¤æ¯”ä¾‹ (å¦‚ 0.2 è¡¨ç¤º 20% å›æ’¤)

    è¿”å›:
        calmar_ratio: Calmar Ratio

    è§£é‡‹:
        - Calmar > 3.0: å„ªç§€
        - Calmar > 1.0: è‰¯å¥½
        - Calmar > 0.5: å¯æ¥å—
        - Calmar < 0: è² æ”¶ç›Š

    æ³¨æ„:
        - æ¯” Sharpe Ratio æ›´é—œæ³¨æ¥µç«¯é¢¨éšª
        - é©åˆè¶¨å‹¢ç­–ç•¥è©•ä¼°
    """
    if max_drawdown == 0:
        return float('inf') if total_return > 0 else 0.0

    return float(total_return / max_drawdown)


def calculate_win_rate(returns: np.ndarray) -> float:
    """è¨ˆç®—å‹ç‡ (Win Rate)

    å‹ç‡æ˜¯ç›ˆåˆ©äº¤æ˜“æ¬¡æ•¸ä½”ç¸½äº¤æ˜“æ¬¡æ•¸çš„æ¯”ä¾‹ã€‚

    å…¬å¼: Win Rate = ç›ˆåˆ©æ¬¡æ•¸ / ç¸½æ¬¡æ•¸

    åƒæ•¸:
        returns: æ”¶ç›Šåºåˆ— (æ¯ç­†äº¤æ˜“æˆ–æ¯æ—¥æ”¶ç›Š)

    è¿”å›:
        win_rate: å‹ç‡ (0-1ï¼Œå¦‚ 0.55 è¡¨ç¤º 55%)

    è§£é‡‹:
        - WR > 60%: å„ªç§€
        - WR > 50%: è‰¯å¥½
        - WR = 50%: éš¨æ©Ÿ
        - WR < 50%: éœ€æ”¹é€²

    æ³¨æ„:
        - ä¸è€ƒæ…®ç›ˆè™§å¤§å°
        - é«˜å‹ç‡ä¸ä¸€å®šä»£è¡¨é«˜æ”¶ç›Š
    """
    if len(returns) == 0:
        return 0.0

    winning_trades = np.sum(returns > 0)
    total_trades = len(returns)

    return float(winning_trades / total_trades)


def calculate_profit_factor(returns: np.ndarray) -> float:
    """è¨ˆç®—ç›ˆè™§æ¯” (Profit Factor)

    ç›ˆè™§æ¯”æ˜¯ç¸½ç›ˆåˆ©èˆ‡ç¸½è™§æçš„æ¯”å€¼ï¼Œè¡¡é‡ç›ˆåˆ©èƒ½åŠ›ã€‚

    å…¬å¼: PF = ç¸½ç›ˆåˆ© / ç¸½è™§æ

    åƒæ•¸:
        returns: æ”¶ç›Šåºåˆ—

    è¿”å›:
        profit_factor: ç›ˆè™§æ¯”

    è§£é‡‹:
        - PF > 2.0: å„ªç§€
        - PF > 1.5: è‰¯å¥½
        - PF > 1.0: ç›ˆåˆ©
        - PF = 1.0: æ‰“å¹³
        - PF < 1.0: è™§æ

    æ³¨æ„:
        - è€ƒæ…®äº†ç›ˆè™§å¤§å°
        - èˆ‡å‹ç‡äº’è£œ
    """
    if len(returns) == 0:
        return 0.0

    profits = returns[returns > 0].sum()
    losses = abs(returns[returns < 0].sum())

    if losses == 0:
        return float('inf') if profits > 0 else 0.0

    return float(profits / losses)


# ============================================================================
# ç­–ç•¥è©•ä¼°å™¨é¡åˆ¥
# ============================================================================

class RLStrategyEvaluator:
    """RL ç­–ç•¥è©•ä¼°å™¨

    æ­¤é¡åˆ¥æä¾›å®Œæ•´çš„ç­–ç•¥è©•ä¼°åŠŸèƒ½ï¼ŒåŒ…æ‹¬ï¼š
    - åœ¨æ¸¬è©¦ç’°å¢ƒä¸­é‹è¡Œç­–ç•¥
    - æ”¶é›†äº¤æ˜“è¨˜éŒ„
    - è¨ˆç®—ç¸¾æ•ˆæŒ‡æ¨™
    - ç”Ÿæˆè©•ä¼°å ±å‘Š
    - èˆ‡åŸºæº–ç­–ç•¥å°æ¯”

    ä½¿ç”¨ç¯„ä¾‹:
        >>> evaluator = RLStrategyEvaluator(test_env, policy)
        >>> results = evaluator.evaluate(num_episodes=100)
        >>> evaluator.print_report(results)
        >>> evaluator.save_report(results, 'results/evaluation/')
    """

    def __init__(self, env, policy, baseline_strategies: Optional[Dict] = None):
        """åˆå§‹åŒ–è©•ä¼°å™¨

        åƒæ•¸:
            env: æ¸¬è©¦ç’°å¢ƒ (LOBTradingEnv å¯¦ä¾‹)
            policy: è¨“ç·´å¥½çš„ç­–ç•¥ (RLlib Policy æˆ– Algorithm)
            baseline_strategies: åŸºæº–ç­–ç•¥å­—å…¸ (å¯é¸)
        """
        self.env = env
        self.policy = policy
        self.baseline_strategies = baseline_strategies or {}

        logger.info("âœ… RLStrategyEvaluator å·²åˆå§‹åŒ–")

    def evaluate(self, num_episodes: int = 100,
                deterministic: bool = True) -> Dict[str, Any]:
        """è©•ä¼°ç­–ç•¥æ€§èƒ½

        åœ¨æ¸¬è©¦é›†ä¸Šé‹è¡Œå¤šå€‹ episodeï¼Œæ”¶é›†çµ±è¨ˆæ•¸æ“šã€‚

        åƒæ•¸:
            num_episodes: è©•ä¼° episode æ•¸é‡
            deterministic: æ˜¯å¦ä½¿ç”¨ç¢ºå®šæ€§ç­–ç•¥ (ä¸æ¢ç´¢)

        è¿”å›:
            results: è©•ä¼°çµæœå­—å…¸
        """
        logger.info(f"é–‹å§‹è©•ä¼°ç­–ç•¥ (episodes={num_episodes})")

        episode_returns = []
        episode_lengths = []
        all_trades = []
        all_actions = []
        all_positions = []

        for ep in range(num_episodes):
            obs, info = self.env.reset()
            done = False
            truncated = False
            episode_return = 0.0
            episode_length = 0

            while not (done or truncated):
                # é¸æ“‡å‹•ä½œ
                if hasattr(self.policy, 'compute_single_action'):
                    result = self.policy.compute_single_action(
                        obs,
                        explore=not deterministic
                    )
                    # è™•ç†ä¸åŒçš„è¿”å›æ ¼å¼
                    if isinstance(result, tuple):
                        action = result[0]
                    else:
                        action = result
                else:
                    action = self.policy.compute_actions([obs])[0][0]

                # åŸ·è¡Œå‹•ä½œ
                obs, reward, done, truncated, info = self.env.step(action)

                episode_return += reward
                episode_length += 1

                # è¨˜éŒ„äº¤æ˜“ä¿¡æ¯
                all_actions.append(action)
                if 'position' in info:
                    all_positions.append(info['position'])

            episode_returns.append(episode_return)
            episode_lengths.append(episode_length)

            if (ep + 1) % 20 == 0:
                logger.info(f"  å®Œæˆ {ep + 1}/{num_episodes} episodes")

        # è¨ˆç®—ç¸¾æ•ˆæŒ‡æ¨™
        returns_array = np.array(episode_returns)

        total_return = np.sum(returns_array)
        avg_return = np.mean(returns_array)
        sharpe = calculate_sharpe_ratio(returns_array)
        mdd, peak_idx, trough_idx = calculate_max_drawdown(returns_array)
        calmar = calculate_calmar_ratio(total_return, mdd) if mdd > 0 else 0.0
        win_rate = calculate_win_rate(returns_array)
        profit_factor = calculate_profit_factor(returns_array)

        results = {
            'num_episodes': num_episodes,
            'total_return': float(total_return),
            'avg_return_per_episode': float(avg_return),
            'avg_episode_length': float(np.mean(episode_lengths)),
            'sharpe_ratio': float(sharpe),
            'max_drawdown': float(mdd),
            'calmar_ratio': float(calmar),
            'win_rate': float(win_rate),
            'profit_factor': float(profit_factor),
            'returns_std': float(np.std(returns_array)),
            'returns_min': float(np.min(returns_array)),
            'returns_max': float(np.max(returns_array)),
            'action_distribution': {
                'hold': float(np.mean(np.array(all_actions) == 0)),
                'buy': float(np.mean(np.array(all_actions) == 1)),
                'sell': float(np.mean(np.array(all_actions) == 2)),
            }
        }

        logger.info("âœ… ç­–ç•¥è©•ä¼°å®Œæˆ")
        return results

    def print_report(self, results: Dict):
        """æ‰“å°è©•ä¼°å ±å‘Š

        åƒæ•¸:
            results: evaluate() è¿”å›çš„çµæœå­—å…¸
        """
        print("\n" + "="*80)
        print("RL ç­–ç•¥è©•ä¼°å ±å‘Š")
        print("="*80)
        print(f"\nğŸ“Š åŸºç¤çµ±è¨ˆ:")
        print(f"  - Episodes: {results['num_episodes']}")
        print(f"  - å¹³å‡ Episode é•·åº¦: {results['avg_episode_length']:.1f}")
        print(f"\nğŸ’° æ”¶ç›ŠæŒ‡æ¨™:")
        print(f"  - ç¸½æ”¶ç›Š: {results['total_return']:.4f}")
        print(f"  - å¹³å‡ Episode æ”¶ç›Š: {results['avg_return_per_episode']:.4f}")
        print(f"  - æ”¶ç›Šæ¨™æº–å·®: {results['returns_std']:.4f}")
        print(f"  - æœ€å°æ”¶ç›Š: {results['returns_min']:.4f}")
        print(f"  - æœ€å¤§æ”¶ç›Š: {results['returns_max']:.4f}")
        print(f"\nğŸ“ˆ é¢¨éšªèª¿æ•´æŒ‡æ¨™:")
        print(f"  - Sharpe Ratio: {results['sharpe_ratio']:.4f}")
        print(f"  - æœ€å¤§å›æ’¤: {results['max_drawdown']*100:.2f}%")
        print(f"  - Calmar Ratio: {results['calmar_ratio']:.4f}")
        print(f"\nğŸ¯ äº¤æ˜“çµ±è¨ˆ:")
        print(f"  - å‹ç‡: {results['win_rate']*100:.2f}%")
        print(f"  - ç›ˆè™§æ¯”: {results['profit_factor']:.4f}")
        print(f"\nğŸ¬ å‹•ä½œåˆ†ä½ˆ:")
        print(f"  - Hold: {results['action_distribution']['hold']*100:.1f}%")
        print(f"  - Buy: {results['action_distribution']['buy']*100:.1f}%")
        print(f"  - Sell: {results['action_distribution']['sell']*100:.1f}%")
        print("="*80 + "\n")

    def save_report(self, results: Dict, output_dir: str):
        """ä¿å­˜è©•ä¼°å ±å‘Šåˆ°æ–‡ä»¶

        åƒæ•¸:
            results: evaluate() è¿”å›çš„çµæœå­—å…¸
            output_dir: è¼¸å‡ºç›®éŒ„
        """
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)

        # ä¿å­˜ JSON
        json_path = output_path / 'rl_evaluation_results.json'
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)

        logger.info(f"âœ… è©•ä¼°å ±å‘Šå·²ä¿å­˜: {json_path}")
