"""è¨“ç·´ç›£æ§å›èª¿ - TrainingMonitorCallbacks

æ­¤æ¨¡çµ„å¯¦ä½œ RLlib è¨“ç·´å›èª¿ï¼Œç”¨æ–¼ç›£æ§å¼·åŒ–å­¸ç¿’è¨“ç·´éç¨‹ä¸­çš„å„ç¨®æŒ‡æ¨™ã€‚
æä¾›å¯¦æ™‚æ€§èƒ½ç›£æ§ã€ç•°å¸¸æª¢æ¸¬å’Œè¨“ç·´è¨ºæ–·åŠŸèƒ½ã€‚

ä¸»è¦åŠŸèƒ½:
    1. ç›£æ§è¨“ç·´æŒ‡æ¨™ (reward, loss, entropy, KL divergence)
    2. ç›£æ§ç’°å¢ƒæŒ‡æ¨™ (äº¤æ˜“çµ±è¨ˆ, PnL, å‹ç‡)
    3. ç›£æ§æ¨¡å‹å¥åº· (æ¢¯åº¦ç¯„æ•¸, å‹•ä½œåˆ†ä½ˆ)
    4. TensorBoard æ—¥èªŒè¨˜éŒ„
    5. ç•°å¸¸æª¢æ¸¬èˆ‡å‘Šè­¦

å›èª¿è§¸ç™¼é»:
    - on_episode_start: Episode é–‹å§‹æ™‚
    - on_episode_step: æ¯å€‹ step å¾Œ
    - on_episode_end: Episode çµæŸæ™‚
    - on_train_result: æ¯å€‹è¨“ç·´è¿­ä»£å¾Œ
    - on_algorithm_init: ç®—æ³•åˆå§‹åŒ–æ™‚

ä½¿ç”¨ç¯„ä¾‹:
    >>> from ray.rllib.algorithms.ppo import PPOConfig
    >>> config = PPOConfig()
    >>> config.callbacks(TrainingMonitorCallbacks)
    >>> algo = config.build()
    >>> algo.train()

ä½œè€…: RLlib-DeepLOB å°ˆæ¡ˆåœ˜éšŠ
æ›´æ–°: 2025-01-10
"""

import numpy as np
from typing import Dict, Optional, Any
import logging
from collections import defaultdict, deque

from ray.rllib.algorithms.callbacks import DefaultCallbacks
from ray.rllib.env import BaseEnv
from ray.rllib.evaluation import RolloutWorker
from ray.rllib.evaluation.episode_v2 import EpisodeV2
from ray.rllib.policy import Policy
from ray.rllib.policy.sample_batch import SampleBatch
from ray.rllib.utils.typing import PolicyID

# å…¼å®¹èˆŠç‰ˆ RLlibï¼šEpisode å¯èƒ½ä¸å­˜åœ¨ï¼Œä½¿ç”¨ EpisodeV2
try:
    from ray.rllib.evaluation import Episode
except ImportError:
    Episode = EpisodeV2  # èˆŠç‰ˆ Ray ä½¿ç”¨ EpisodeV2

logger = logging.getLogger(__name__)


class TrainingMonitorCallbacks(DefaultCallbacks):
    """è¨“ç·´ç›£æ§å›èª¿é¡åˆ¥

    æ­¤é¡åˆ¥æ“´å±• RLlib çš„ DefaultCallbacksï¼Œæ·»åŠ è‡ªå®šç¾©çš„ç›£æ§é‚è¼¯ã€‚
    åœ¨è¨“ç·´éç¨‹ä¸­æ”¶é›†å’Œè¨˜éŒ„å„ç¨®æ€§èƒ½æŒ‡æ¨™ï¼Œå¹«åŠ©è¨ºæ–·è¨“ç·´å•é¡Œã€‚

    ç›£æ§é¡åˆ¥:
        1. **åŸºç¤è¨“ç·´æŒ‡æ¨™**:
           - episode_reward: Episode ç¸½çå‹µ
           - episode_length: Episode é•·åº¦
           - policy_loss, vf_loss: ç­–ç•¥å’Œåƒ¹å€¼å‡½æ•¸æå¤±
           - entropy: ç­–ç•¥ç†µï¼ˆæ¢ç´¢åº¦ï¼‰
           - kl: KL æ•£åº¦

        2. **äº¤æ˜“çµ±è¨ˆ**:
           - num_trades: äº¤æ˜“æ¬¡æ•¸
           - win_rate: å‹ç‡
           - avg_pnl: å¹³å‡ç›ˆè™§
           - total_cost: äº¤æ˜“æˆæœ¬
           - position_distribution: æŒå€‰åˆ†ä½ˆ

        3. **æ¨¡å‹å¥åº·**:
           - grad_norm: æ¢¯åº¦ç¯„æ•¸
           - action_distribution: å‹•ä½œåˆ†ä½ˆ
           - deeplob_prediction_variance: é æ¸¬æ–¹å·®

        4. **ç•°å¸¸æª¢æ¸¬**:
           - reward æ˜¯å¦é€€åŒ–ï¼ˆç¸½æ˜¯ 0 æˆ–å¸¸æ•¸ï¼‰
           - å‹•ä½œåˆ†ä½ˆæ˜¯å¦é€€åŒ–ï¼ˆåªé¸ä¸€å€‹å‹•ä½œï¼‰
           - æ¢¯åº¦çˆ†ç‚¸/æ¶ˆå¤±

    å±¬æ€§:
        episode_rewards (deque): æœ€è¿‘ N å€‹ episode çš„çå‹µ
        episode_lengths (deque): æœ€è¿‘ N å€‹ episode çš„é•·åº¦
        action_counts (defaultdict): å‹•ä½œé¸æ“‡æ¬¡æ•¸çµ±è¨ˆ
        trade_stats (dict): äº¤æ˜“çµ±è¨ˆè³‡è¨Š
    """

    def __init__(self):
        """åˆå§‹åŒ–å›èª¿

        è¨­ç½®å…§éƒ¨ç‹€æ…‹è®Šæ•¸å’Œçµ±è¨ˆå®¹å™¨ã€‚
        """
        super().__init__()

        # ===== æ€§èƒ½çµ±è¨ˆå®¹å™¨ =====
        # ä¿æŒæœ€è¿‘ 100 å€‹ episode çš„çå‹µæ­·å²
        self.episode_rewards = deque(maxlen=100)
        # ä¿æŒæœ€è¿‘ 100 å€‹ episode çš„é•·åº¦æ­·å²
        self.episode_lengths = deque(maxlen=100)

        # ===== äº¤æ˜“çµ±è¨ˆ =====
        self.action_counts = defaultdict(int)  # å‹•ä½œè¨ˆæ•¸
        self.trade_stats = {
            'num_trades': 0,
            'total_pnl': 0.0,
            'winning_trades': 0,
            'losing_trades': 0,
        }

        # ===== è¨ºæ–·æ¨™èªŒ =====
        self.iteration_count = 0  # è¨“ç·´è¿­ä»£è¨ˆæ•¸å™¨
        self.last_log_iter = 0    # ä¸Šæ¬¡è¨˜éŒ„æ—¥èªŒçš„è¿­ä»£

        # ===== Episode æ•¸æ“šå­˜å„² (æ–° API å…¼å®¹) =====
        # SingleAgentEpisode ä¸å…è¨±å‹•æ…‹æ·»åŠ å±¬æ€§ï¼Œä½¿ç”¨å¤–éƒ¨å­—å…¸
        self.episode_data = {}  # {episode_id: {'actions': [], 'positions': [], ...}}

        logger.info("[OK] TrainingMonitorCallbacks å·²åˆå§‹åŒ–")

    def _get_episode_id(self, episode):
        """ç²å– episode çš„å”¯ä¸€ ID"""
        if hasattr(episode, 'episode_id'):
            return episode.episode_id
        elif hasattr(episode, 'id_'):
            return episode.id_
        else:
            return id(episode)  # ä½¿ç”¨å°è±¡ ID ä½œç‚ºfallback

    def _get_episode_data(self, episode, key: str, default=None):
        """çµ±ä¸€ç²å– episode æ•¸æ“šï¼ˆå…¼å®¹æ–°èˆŠ APIï¼‰"""
        if hasattr(episode, 'user_data'):
            # èˆŠ API
            return episode.user_data.get(key, default) if default is not None else episode.user_data[key]
        else:
            # æ–° API - ä½¿ç”¨å¤–éƒ¨å­˜å„²
            episode_id = self._get_episode_id(episode)
            if episode_id not in self.episode_data:
                return default
            return self.episode_data[episode_id].get(key, default)

    def _set_episode_data(self, episode, key: str, value):
        """çµ±ä¸€è¨­ç½® episode æ•¸æ“šï¼ˆå…¼å®¹æ–°èˆŠ APIï¼‰"""
        if hasattr(episode, 'user_data'):
            # èˆŠ API
            episode.user_data[key] = value
        else:
            # æ–° API - ä½¿ç”¨å¤–éƒ¨å­˜å„²
            episode_id = self._get_episode_id(episode)
            if episode_id not in self.episode_data:
                self.episode_data[episode_id] = {}
            self.episode_data[episode_id][key] = value

    def _append_episode_data(self, episode, key: str, value):
        """çµ±ä¸€è¿½åŠ  episode æ•¸æ“šï¼ˆå…¼å®¹æ–°èˆŠ APIï¼‰"""
        data = self._get_episode_data(episode, key, [])
        if data is None:
            data = []
        data.append(value)
        self._set_episode_data(episode, key, data)

    def _cleanup_episode_data(self, episode):
        """æ¸…ç†å®Œæˆçš„ episode æ•¸æ“š"""
        if not hasattr(episode, 'user_data'):
            # åªåœ¨æ–° API éœ€è¦æ¸…ç†
            episode_id = self._get_episode_id(episode)
            if episode_id in self.episode_data:
                del self.episode_data[episode_id]

    def on_episode_start(
        self,
        *,
        episode: Episode | EpisodeV2,
        worker: Optional[RolloutWorker] = None,
        base_env: Optional[BaseEnv] = None,
        policies: Optional[Dict[PolicyID, Policy]] = None,
        env_runner: Optional[Any] = None,
        env: Optional[Any] = None,
        env_index: int = 0,
        **kwargs
    ) -> None:
        """Episode é–‹å§‹æ™‚çš„å›èª¿

        åœ¨æ¯å€‹ episode é–‹å§‹æ™‚èª¿ç”¨ï¼Œç”¨æ–¼åˆå§‹åŒ– episode ç´šåˆ¥çš„çµ±è¨ˆã€‚

        åƒæ•¸:
            worker: RolloutWorker å¯¦ä¾‹
            base_env: ç’°å¢ƒ
            policies: ç­–ç•¥å­—å…¸
            episode: Episode ç‰©ä»¶
            **kwargs: é¡å¤–åƒæ•¸

        åˆå§‹åŒ–:
            - é‡ç½® episode çµ±è¨ˆ
            - åˆå§‹åŒ–è‡ªå®šç¾©çµ±è¨ˆè®Šæ•¸
        """
        # åˆå§‹åŒ– episode è‡ªå®šç¾©çµ±è¨ˆï¼ˆå…¼å®¹æ–°èˆŠ APIï¼‰
        self._set_episode_data(episode, 'actions', [])
        self._set_episode_data(episode, 'positions', [])
        self._set_episode_data(episode, 'trades', [])
        self._set_episode_data(episode, 'pnl_components', [])

    def on_episode_step(
        self,
        *,
        episode: Episode | EpisodeV2,
        worker: Optional[RolloutWorker] = None,
        base_env: Optional[BaseEnv] = None,
        policies: Optional[Dict[PolicyID, Policy]] = None,
        env_runner: Optional[Any] = None,
        env: Optional[Any] = None,
        env_index: int = 0,
        **kwargs
    ) -> None:
        """Episode æ¯æ­¥åŸ·è¡Œå¾Œçš„å›èª¿

        åœ¨æ¯å€‹æ™‚é–“æ­¥åŸ·è¡Œå¾Œèª¿ç”¨ï¼Œç”¨æ–¼æ”¶é›†ç´°ç²’åº¦çš„çµ±è¨ˆè³‡è¨Šã€‚

        åƒæ•¸:
            worker: RolloutWorker å¯¦ä¾‹
            base_env: ç’°å¢ƒ
            policies: ç­–ç•¥å­—å…¸
            episode: Episode ç‰©ä»¶
            **kwargs: é¡å¤–åƒæ•¸

        æ”¶é›†ä¿¡æ¯:
            - å‹•ä½œé¸æ“‡
            - æŒå€‰ç‹€æ…‹
            - äº¤æ˜“äº‹ä»¶
            - çå‹µçµ„ä»¶
        """
        # ç²å–æœ€å¾Œä¸€å€‹ step çš„è³‡è¨Š
        # å…¼å®¹æ–°èˆŠ API: SingleAgentEpisode ä½¿ç”¨ t, EpisodeV2 ä½¿ç”¨ length
        episode_len = episode.t if hasattr(episode, 't') else episode.length
        if episode_len == 0:
            return  # è·³éç¬¬ä¸€å€‹ step

        # å¾ç’°å¢ƒç²å– info
        try:
            # å˜—è©¦å¾ base_env ç²å–ç’°å¢ƒ ID
            sub_envs = base_env.get_sub_environments()
            if isinstance(sub_envs, dict):
                env_id = list(sub_envs.keys())[0]
            elif isinstance(sub_envs, list):
                env_id = 0  # ä½¿ç”¨ç´¢å¼• 0
            else:
                env_id = 0  # é è¨­ä½¿ç”¨ 0

            last_info = episode.last_info_for(env_id)
        except Exception:
            # ç„¡æ³•ç²å– infoï¼Œè·³é
            return

        # è¨˜éŒ„å‹•ä½œ (å…¼å®¹ EpisodeV2 API)
        try:
            # å˜—è©¦æ–° API
            if hasattr(episode, 'last_action_for'):
                last_action = episode.last_action_for()
            else:
                # EpisodeV2 ä½¿ç”¨ _last_actions å­—å…¸
                last_action = episode._last_actions.get(env_id)
        except Exception:
            last_action = None

        if last_action is not None:
            self._append_episode_data(episode, 'actions', int(last_action))
            self.action_counts[int(last_action)] += 1

        # ===== é—œéµä¿®æ­£: æª¢æŸ¥ last_info æ˜¯å¦ç‚º None =====
        if last_info is None:
            return  # å¦‚æœ last_info ç‚º Noneï¼Œç›´æ¥è¿”å›

        # è¨˜éŒ„æŒå€‰
        if 'position' in last_info:
            self._append_episode_data(episode, 'positions', last_info['position'])

        # è¨˜éŒ„äº¤æ˜“
        if last_info.get('num_trades', 0) > len(self._get_episode_data(episode, 'trades', [])):
            # å…¼å®¹æ–°èˆŠ API
            step = episode.t if hasattr(episode, 't') else episode.length
            self._append_episode_data(episode, 'trades', {
                'step': step,
                'action': int(last_action) if last_action is not None else None,
                'cost': last_info.get('transaction_cost', 0.0)
            })

        # è¨˜éŒ„çå‹µçµ„ä»¶ï¼ˆå¦‚æœç’°å¢ƒæä¾›ï¼‰
        if 'pnl' in last_info:
            self._append_episode_data(episode, 'pnl_components', {
                'pnl': last_info.get('pnl', 0.0),
                'cost': last_info.get('cost', 0.0),
                'inventory_penalty': last_info.get('inventory_penalty', 0.0),
                'risk_penalty': last_info.get('risk_penalty', 0.0),
            })

    def on_episode_end(
        self,
        *,
        episode: Episode | EpisodeV2,
        worker: Optional[RolloutWorker] = None,
        base_env: Optional[BaseEnv] = None,
        policies: Optional[Dict[PolicyID, Policy]] = None,
        env_runner: Optional[Any] = None,
        env: Optional[Any] = None,
        env_index: int = 0,
        **kwargs
    ) -> None:
        """Episode çµæŸæ™‚çš„å›èª¿

        åœ¨æ¯å€‹ episode çµæŸæ™‚èª¿ç”¨ï¼Œç”¨æ–¼èšåˆå’Œè¨˜éŒ„ episode çµ±è¨ˆã€‚

        åƒæ•¸:
            worker: RolloutWorker å¯¦ä¾‹
            base_env: ç’°å¢ƒ
            policies: ç­–ç•¥å­—å…¸
            episode: Episode ç‰©ä»¶
            **kwargs: é¡å¤–åƒæ•¸

        è¨˜éŒ„æŒ‡æ¨™:
            - episode_reward: ç¸½çå‹µ
            - episode_length: é•·åº¦
            - num_trades: äº¤æ˜“æ¬¡æ•¸
            - action_entropy: å‹•ä½œç†µ
            - avg_pnl: å¹³å‡ PnL
        """
        # è¨˜éŒ„åŸºç¤æŒ‡æ¨™
        # å…¼å®¹æ–°èˆŠ API: SingleAgentEpisode ä½¿ç”¨ t å’Œ get_return(), EpisodeV2 ä½¿ç”¨ length å’Œ total_reward
        episode_reward = episode.get_return() if hasattr(episode, 'get_return') else episode.total_reward
        episode_length = episode.t if hasattr(episode, 't') else episode.length

        self.episode_rewards.append(episode_reward)
        self.episode_lengths.append(episode_length)

        # ===== äº¤æ˜“çµ±è¨ˆ =====
        num_trades = len(self._get_episode_data(episode, 'trades', []))

        # å…¼å®¹æ–°èˆŠ API: SingleAgentEpisode æ²’æœ‰ custom_metricsï¼Œæš«å­˜åœ¨è‡ªå®šç¾©å­—å…¸
        def set_custom_metric(episode, key, value):
            if hasattr(episode, 'custom_metrics'):
                episode.custom_metrics[key] = value
            else:
                # æ–° API: æš«å­˜åœ¨ episode_dataï¼Œç¨å¾Œåœ¨ on_train_result ä¸­èšåˆ
                self._set_episode_data(episode, f'metric_{key}', value)

        set_custom_metric(episode, 'num_trades', num_trades)

        # å‹•ä½œåˆ†ä½ˆç†µï¼ˆæ¢ç´¢åº¦ï¼‰
        if len(self._get_episode_data(episode, 'actions', [])) > 0:
            actions = np.array(self._get_episode_data(episode, 'actions', []))
            # è¨ˆç®—å‹•ä½œåˆ†ä½ˆ
            action_dist = np.bincount(actions, minlength=3) / len(actions)
            # è¨ˆç®—ç†µ: -sum(p * log(p))
            entropy = -np.sum(action_dist * np.log(action_dist + 1e-10))
            set_custom_metric(episode, 'action_entropy', entropy)

            # è¨˜éŒ„æ¯å€‹å‹•ä½œçš„æ¯”ä¾‹
            set_custom_metric(episode, 'action_0_ratio', action_dist[0])  # Hold
            set_custom_metric(episode, 'action_1_ratio', action_dist[1])  # Buy
            set_custom_metric(episode, 'action_2_ratio', action_dist[2])  # Sell

        # ===== æŒå€‰çµ±è¨ˆ =====
        if len(self._get_episode_data(episode, 'positions', [])) > 0:
            positions = np.array(self._get_episode_data(episode, 'positions', []))
            # æŒå€‰æ™‚é–“æ¯”ä¾‹
            long_ratio = np.mean(positions == 1)   # å¤šå€‰
            short_ratio = np.mean(positions == -1) # ç©ºå€‰
            flat_ratio = np.mean(positions == 0)   # å¹³å€‰

            set_custom_metric(episode, 'long_position_ratio', long_ratio)
            set_custom_metric(episode, 'short_position_ratio', short_ratio)
            set_custom_metric(episode, 'flat_position_ratio', flat_ratio)

        # ===== PnL çµ„ä»¶çµ±è¨ˆ =====
        if len(self._get_episode_data(episode, 'pnl_components', [])) > 0:
            pnl_components = self._get_episode_data(episode, 'pnl_components', [])

            # å¹³å‡å„çµ„ä»¶
            avg_pnl = np.mean([c['pnl'] for c in pnl_components])
            avg_cost = np.mean([c['cost'] for c in pnl_components])
            avg_inventory_penalty = np.mean([c['inventory_penalty'] for c in pnl_components])
            avg_risk_penalty = np.mean([c['risk_penalty'] for c in pnl_components])

            set_custom_metric(episode, 'avg_pnl', avg_pnl)
            set_custom_metric(episode, 'avg_cost', avg_cost)
            set_custom_metric(episode, 'avg_inventory_penalty', avg_inventory_penalty)
            set_custom_metric(episode, 'avg_risk_penalty', avg_risk_penalty)

            # ç¸½ PnL
            total_pnl = sum([c['pnl'] for c in pnl_components])
            set_custom_metric(episode, 'total_pnl', total_pnl)

            # å‹ç‡ï¼ˆæ­£ PnL æ­¥æ•¸æ¯”ä¾‹ï¼‰
            if len(pnl_components) > 0:
                win_steps = sum(1 for c in pnl_components if c['pnl'] > 0)
                win_rate = win_steps / len(pnl_components)
                set_custom_metric(episode, 'win_rate', win_rate)

        # æ¸…ç† episode æ•¸æ“š
        self._cleanup_episode_data(episode)

    def on_train_result(
        self,
        *,
        algorithm,
        result: Dict,
        **kwargs
    ) -> None:
        """è¨“ç·´è¿­ä»£çµæŸæ™‚çš„å›èª¿

        åœ¨æ¯å€‹è¨“ç·´è¿­ä»£ï¼ˆå¤šå€‹ episodeï¼‰å¾Œèª¿ç”¨ï¼Œç”¨æ–¼è¨˜éŒ„è¨“ç·´ç´šåˆ¥çš„çµ±è¨ˆã€‚

        åƒæ•¸:
            algorithm: è¨“ç·´ç®—æ³•å¯¦ä¾‹
            result: è¨“ç·´çµæœå­—å…¸
            **kwargs: é¡å¤–åƒæ•¸

        è¨˜éŒ„å…§å®¹:
            - è¨“ç·´æå¤±ï¼ˆpolicy_loss, vf_lossï¼‰
            - æ¢ç´¢æŒ‡æ¨™ï¼ˆentropy, KLï¼‰
            - æ€§èƒ½è¶¨å‹¢ï¼ˆmoving averageï¼‰
            - ç•°å¸¸æª¢æ¸¬
        """
        self.iteration_count += 1

        # ===== åŸºç¤è¨“ç·´æŒ‡æ¨™ =====
        # é€™äº›æŒ‡æ¨™ç”± RLlib è‡ªå‹•è¨ˆç®—
        metrics = {
            'episode_reward_mean': result.get('episode_reward_mean', 0.0),
            'episode_len_mean': result.get('episode_len_mean', 0.0),
            'episodes_this_iter': result.get('episodes_this_iter', 0),
            'timesteps_this_iter': result.get('timesteps_this_iter', 0),
        }

        # ===== ç­–ç•¥è¨“ç·´æŒ‡æ¨™ =====
        # å¾ learner_stats ç²å–
        learner_stats = result.get('info', {}).get('learner', {}).get('default_policy', {})
        if learner_stats:
            metrics.update({
                'policy_loss': learner_stats.get('learner_stats', {}).get('policy_loss', 0.0),
                'vf_loss': learner_stats.get('learner_stats', {}).get('vf_loss', 0.0),
                'entropy': learner_stats.get('learner_stats', {}).get('entropy', 0.0),
                'kl': learner_stats.get('learner_stats', {}).get('kl', 0.0),
                'grad_gnorm': learner_stats.get('learner_stats', {}).get('grad_gnorm', 0.0),
            })

        # ===== ç§»å‹•å¹³å‡çµ±è¨ˆ =====
        if len(self.episode_rewards) > 0:
            result['custom_metrics']['reward_moving_avg'] = np.mean(list(self.episode_rewards))
            result['custom_metrics']['reward_std'] = np.std(list(self.episode_rewards))

        if len(self.episode_lengths) > 0:
            result['custom_metrics']['length_moving_avg'] = np.mean(list(self.episode_lengths))

        # ===== å‹•ä½œåˆ†ä½ˆçµ±è¨ˆ =====
        total_actions = sum(self.action_counts.values())
        if total_actions > 0:
            action_dist = {
                f'action_{k}_ratio': v / total_actions
                for k, v in self.action_counts.items()
            }
            result['custom_metrics'].update(action_dist)

        # ===== ç•°å¸¸æª¢æ¸¬ =====
        warnings = []

        # æª¢æ¸¬ 1: Reward é€€åŒ–ï¼ˆç¸½æ˜¯ 0ï¼‰
        if len(self.episode_rewards) >= 10:
            recent_rewards = list(self.episode_rewards)[-10:]
            if all(abs(r) < 1e-6 for r in recent_rewards):
                warnings.append("[WARN] Reward å…¨ç‚º 0ï¼æª¢æŸ¥çå‹µå‡½æ•¸ã€‚")

        # æª¢æ¸¬ 2: å‹•ä½œé€€åŒ–ï¼ˆåªé¸ä¸€å€‹å‹•ä½œï¼‰
        if total_actions > 100:
            max_action_ratio = max(self.action_counts.values()) / total_actions
            if max_action_ratio > 0.95:
                warnings.append(f"[WARN] å‹•ä½œåˆ†ä½ˆé€€åŒ–ï¼{max_action_ratio*100:.1f}% é¸åŒä¸€å‹•ä½œã€‚")

        # æª¢æ¸¬ 3: æ¢¯åº¦çˆ†ç‚¸
        grad_norm = metrics.get('grad_gnorm', 0.0)
        if grad_norm > 10.0:
            warnings.append(f"[WARN] æ¢¯åº¦ç¯„æ•¸éå¤§: {grad_norm:.2f}ã€‚å¯èƒ½éœ€è¦æ¢¯åº¦è£å‰ªã€‚")

        # è¼¸å‡ºè­¦å‘Š
        if warnings and self.iteration_count % 10 == 0:
            for warning in warnings:
                logger.warning(warning)

        # ===== å®šæœŸæ—¥èªŒè¼¸å‡º =====
        if self.iteration_count % 10 == 0:
            # æ§‹å»ºå‹•ä½œåˆ†ä½ˆå­—ä¸²ï¼ˆåƒ…ç•¶æœ‰å‹•ä½œæ•¸æ“šæ™‚ï¼‰
            action_dist_str = ""
            if total_actions > 0:
                action_dist_str = (
                    f"\n"
                    f"ğŸ¯ å‹•ä½œåˆ†ä½ˆ:\n"
                    f"  - Hold: {action_dist.get('action_0_ratio', 0)*100:.1f}%\n"
                    f"  - Buy: {action_dist.get('action_1_ratio', 0)*100:.1f}%\n"
                    f"  - Sell: {action_dist.get('action_2_ratio', 0)*100:.1f}%\n"
                )

            logger.info(
                f"\n{'='*80}\n"
                f"è¨“ç·´è¿­ä»£ {self.iteration_count}\n"
                f"{'='*80}\n"
                f"ğŸ“Š æ€§èƒ½æŒ‡æ¨™:\n"
                f"  - Episode Reward (å¹³å‡): {metrics.get('episode_reward_mean', 0):.4f}\n"
                f"  - Episode Length (å¹³å‡): {metrics.get('episode_len_mean', 0):.1f}\n"
                f"  - Episodes: {metrics.get('episodes_this_iter', 0)}\n"
                f"  - Timesteps: {metrics.get('timesteps_this_iter', 0)}\n"
                f"\n"
                f"ğŸ§  è¨“ç·´æŒ‡æ¨™:\n"
                f"  - Policy Loss: {metrics.get('policy_loss', 0):.4f}\n"
                f"  - Value Loss: {metrics.get('vf_loss', 0):.4f}\n"
                f"  - Entropy: {metrics.get('entropy', 0):.4f}\n"
                f"  - KL: {metrics.get('kl', 0):.6f}\n"
                f"  - Grad Norm: {metrics.get('grad_gnorm', 0):.4f}\n"
                f"{action_dist_str}"
                f"{'='*80}\n"
            )
