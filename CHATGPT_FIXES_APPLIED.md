# ChatGPT å»ºè­°ä¿®å¾©å ±å‘Š

**æ—¥æœŸ**: 2025-10-19
**ç‹€æ…‹**: âœ… å…¨éƒ¨ä¿®å¾©å®Œæˆ

---

## ğŸ“Š ä¿®å¾©ç¸½çµ

ChatGPT æå‡ºçš„ 5 å€‹å»ºè­°**å…¨éƒ¨æ­£ç¢ºä¸”é‡è¦**ï¼Œå·²å…¨éƒ¨å¯¦æ–½ä¿®å¾©ã€‚

| å»ºè­° | æ­£ç¢ºæ€§ | åš´é‡ç¨‹åº¦ | ç‹€æ…‹ |
|------|--------|---------|------|
| 1. TF32 API ä¿®æ­£ | âœ… å®Œå…¨æ­£ç¢º | ğŸ”¥ é«˜ | âœ… å·²ä¿®å¾© |
| 2. ä¸‰é‡åŠ æ¬Šå®ˆé–€ | âœ… å®Œå…¨æ­£ç¢º | ğŸ”¥ é«˜ | âœ… å·²ä¿®å¾© |
| 3. æ¬Šé‡è£å‰ª | âœ… æ­£ç¢º | âš ï¸ ä¸­ | âœ… å·²ä¿®å¾© |
| 4. GradScaler åˆå§‹åŒ– | âœ… æ­£ç¢º | âš ï¸ ä½ | âœ… å·²ä¿®å¾© |
| 5. Warmup å¯¦éš›ç”Ÿæ•ˆ | âœ… å®Œå…¨æ­£ç¢º | ğŸ”¥ğŸ”¥ æœ€é«˜ | âœ… å·²ä¿®å¾© |

---

## ğŸ”§ è©³ç´°ä¿®å¾©å…§å®¹

### 1ï¸âƒ£ TF32 API ä¿®æ­£ï¼ˆé«˜å„ªå…ˆç´šï¼‰

#### å•é¡Œ
```python
# âŒ éŒ¯èª¤ï¼šä¸å­˜åœ¨çš„ API
torch.backends.cuda.matmul.fp32_precision = 'tf32'
torch.backends.cudnn.conv.fp32_precision = 'tf32'
```

#### ä¿®å¾©
```python
# âœ… æ­£ç¢ºï¼šPyTorch æ¨™æº– API
torch.backends.cuda.matmul.allow_tf32 = True
torch.backends.cudnn.allow_tf32 = True
try:
    torch.set_float32_matmul_precision("high")  # PyTorch 2.0+
except AttributeError:
    pass  # PyTorch < 2.0 æ²’æœ‰æ­¤ API
```

**å½±éŸ¿**:
- åŸä»£ç¢¼æœƒå°è‡´ TF32 **æ ¹æœ¬æ²’å•Ÿç”¨**ï¼ˆå±¬æ€§ä¸å­˜åœ¨ï¼‰
- ä¿®å¾©å¾Œ RTX 5090 å¯æ­£ç¢ºä½¿ç”¨ TF32 åŠ é€Ÿï¼ˆ~20% æ€§èƒ½æå‡ï¼‰

**æ–‡ä»¶**: [scripts/train_deeplob_v5.py:939-946](scripts/train_deeplob_v5.py#L939-L946)

---

### 2ï¸âƒ£ ä¸‰é‡åŠ æ¬Šå®ˆé–€ï¼ˆé«˜å„ªå…ˆç´šï¼‰

#### å•é¡Œ
å¦‚æœåŒæ™‚å•Ÿç”¨ï¼š
- `WeightedRandomSampler`ï¼ˆæ¡æ¨£å™¨åŠ æ¬Šï¼‰
- `class_weights`ï¼ˆæå¤±å‡½æ•¸é¡åˆ¥æ¬Šé‡ï¼‰
- `sample_weights`ï¼ˆæå¤±å‡½æ•¸æ¨£æœ¬æ¬Šé‡ï¼‰

æœƒå°è‡´ **ä¸‰é‡æ”¾å¤§**ï¼š
```
å¯¦éš›æ¬Šé‡ = æ¡æ¨£æ¬Šé‡ Ã— é¡åˆ¥æ¬Šé‡ Ã— æ¨£æœ¬æ¬Šé‡
```

ä¾‹å¦‚ï¼šå°‘æ•¸é¡æ¨£æœ¬å¯èƒ½è¢«æ”¾å¤§ **10 Ã— 3 Ã— 10 = 300 å€**ï¼

#### ä¿®å¾©
```python
# âœ… å®ˆé–€é‚è¼¯ï¼šè‡ªå‹•é—œé–‰è¡çªé …
if config['dataloader']['balance_sampler']['enabled']:
    if (config['loss']['class_weights'] and
        config['loss']['class_weights'] not in ['none', None, False]) or \
       config['data']['use_sample_weights']:
        logger.warning("âš ï¸  å·²å•Ÿç”¨ Balanced Samplerï¼Œå°‡é—œé–‰ class_weights èˆ‡ sample_weights")
        logger.warning("    â†’ æ¡æ¨£å™¨å·²ç¶“åœ¨ DataLoader å±¤é¢å¹³è¡¡äº†é¡åˆ¥")
        logger.warning("    â†’ ä¸éœ€è¦å†åœ¨ Loss ä¸­åŠ æ¬Šï¼Œé¿å…ä¸‰é‡æ”¾å¤§")
        config['loss']['class_weights'] = 'none'
        config['data']['use_sample_weights'] = False
```

**å½±éŸ¿**:
- é˜²æ­¢æœªä¾†èª¤é–‹å•Ÿå¤šç¨®åŠ æ¬Šæ©Ÿåˆ¶
- é¿å…å°‘æ•¸é¡è¢«éåº¦æ”¾å¤§å°è‡´è¨“ç·´å´©æ½°

**æ–‡ä»¶**: [scripts/train_deeplob_v5.py:858-866](scripts/train_deeplob_v5.py#L858-L866)

---

### 3ï¸âƒ£ æ¬Šé‡è£å‰ªï¼ˆä¸­å„ªå…ˆç´šï¼‰

#### å•é¡Œ
ä½ çš„æ­·å²è¨“ç·´ä¸­å‡ºç¾é `max_weight=539`ï¼Œé€™ç¨®æ¥µç«¯å€¼æœƒç ´å£è¨“ç·´ã€‚

#### ä¿®å¾©
```python
# âœ… åœ¨æ­¸ä¸€åŒ–å‰å…ˆè£å‰ª
if 'weights_clip' in config['data'] and config['data']['weights_clip']:
    lo, hi = config['data']['weights_clip']
    self.weights.clamp_(min=float(lo), max=float(hi))
    logger.info(f"  æƒé‡å·²è£å‰ªåˆ° [{lo}, {hi}]")
```

**é…ç½®æ–‡ä»¶**:
```yaml
data:
  use_sample_weights: false  # ç›®å‰é—œé–‰
  weights_clip: [0.1, 10.0]  # æœªä¾†å•Ÿç”¨æ™‚ç”Ÿæ•ˆ
  weights_normalize: "mean_to_1"
```

**å½±éŸ¿**:
- ç›®å‰ `use_sample_weights=false`ï¼Œæ­¤ä¿®æ”¹ä¸å½±éŸ¿
- æœªä¾†é‡å•Ÿ sample weights æ™‚ï¼Œé¿å…æ¥µç«¯å€¼å•é¡Œ

**æ–‡ä»¶**:
- [scripts/train_deeplob_v5.py:117-121](scripts/train_deeplob_v5.py#L117-L121)
- [configs/train_v5_improved.yaml:44](configs/train_v5_improved.yaml#L44)

---

### 4ï¸âƒ£ GradScaler åˆå§‹åŒ–ï¼ˆä½å„ªå…ˆç´šï¼‰

#### å•é¡Œ
```python
# âš ï¸ éæ¨™æº–ç”¨æ³•ï¼ˆå¯èƒ½èƒ½ç”¨ï¼Œä½†ä¸å»ºè­°ï¼‰
scaler = GradScaler('cuda')
```

#### ä¿®å¾©
```python
# âœ… æ¨™æº–ç”¨æ³•ï¼ˆè‡ªå‹•æª¢æ¸¬è¨­å‚™ï¼‰
scaler = GradScaler()
```

**å½±éŸ¿**:
- å°å¹…æ”¹é€²ä»£ç¢¼æ¨™æº–æ€§
- ç„¡å¯¦è³ªåŠŸèƒ½è®ŠåŒ–

**æ–‡ä»¶**: [scripts/train_deeplob_v5.py:1078](scripts/train_deeplob_v5.py#L1078)

---

### 5ï¸âƒ£ Warmup å¯¦éš›ç”Ÿæ•ˆï¼ˆğŸ”¥ æœ€é«˜å„ªå…ˆç´šï¼‰

#### å•é¡Œ
**åŸä»£ç¢¼æ ¹æœ¬æ²’æœ‰ Warmupï¼**

```python
# âŒ å•é¡Œ 1: åªè¨ˆç®—äº† warmup_epochsï¼Œä½†æ²’å¯¦éš› warmup
warmup_epochs = int(num_epochs * 0.15)  # = 7.5

# âŒ å•é¡Œ 2: CosineAnnealingLR å¾ç¬¬ 0 epoch å°±é–‹å§‹è¡°æ¸›
scheduler = optim.lr_scheduler.CosineAnnealingLR(
    optimizer,
    T_max=num_epochs - warmup_epochs,  # T_max = 42.5
    eta_min=5e-6
)

# âŒ å•é¡Œ 3: åªåœ¨ epoch > 7 æ‰èª¿ç”¨ scheduler
if epoch > warmup_epochs and scheduler is not None:
    scheduler.step()
```

**å¯¦éš›æ•ˆæœ**:
- Epoch 0-7: å­¸ç¿’ç‡**å‡çµ**åœ¨åˆå§‹å€¼ 0.00008
- Epoch 8-50: é–‹å§‹ Cosine è¡°æ¸›

**é€™ä¸æ˜¯ Warmupï¼é€™æ˜¯ã€Œå‰ 7 å€‹ epoch å‡çµå­¸ç¿’ç‡ã€ï¼**

#### ä¿®å¾©
```python
# âœ… çœŸæ­£çš„ Warmup: LinearLR (ç·šæ€§å¢é•·) â†’ CosineAnnealingLR (è¡°æ¸›)
if warmup_epochs > 0:
    warmup_scheduler = optim.lr_scheduler.LinearLR(
        optimizer,
        start_factor=0.01,  # å¾ 1% çš„ lr é–‹å§‹
        end_factor=1.0,     # ç·šæ€§å¢é•·åˆ° 100% lr
        total_iters=warmup_epochs
    )
    cosine_scheduler = optim.lr_scheduler.CosineAnnealingLR(
        optimizer,
        T_max=num_epochs - warmup_epochs,
        eta_min=5e-6
    )
    scheduler = optim.lr_scheduler.SequentialLR(
        optimizer,
        schedulers=[warmup_scheduler, cosine_scheduler],
        milestones=[warmup_epochs]
    )

# âœ… æ¯å€‹ epoch éƒ½èª¿ç”¨ï¼ˆä¸å†æœ‰æ¢ä»¶ï¼‰
if scheduler is not None:
    scheduler.step()
```

**å¯¦éš›æ•ˆæœ**ï¼ˆå‡è¨­ lr=0.00008, warmup_epochs=7ï¼‰:
- Epoch 0: lr = 0.00008 Ã— 0.01 = **0.0000008** â† Warmup èµ·é»
- Epoch 1: lr â‰ˆ 0.000001
- ...
- Epoch 7: lr = 0.00008 â† Warmup çµæŸ
- Epoch 8-50: Cosine è¡°æ¸› 0.00008 â†’ 0.000005

**å½±éŸ¿**:
- ğŸ”¥ **ä¿®å¾©å‰å¾Œè¨“ç·´ç©©å®šæ€§å·®ç•°æ¥µå¤§**
- æ­£ç¢ºçš„ Warmup å¯é¿å…è¨“ç·´åˆæœŸæ¢¯åº¦çˆ†ç‚¸
- é æœŸæ”¹å–„åˆæœŸæ”¶æ–‚é€Ÿåº¦

**æ–‡ä»¶**:
- [scripts/train_deeplob_v5.py:1035-1073](scripts/train_deeplob_v5.py#L1035-L1073)
- [scripts/train_deeplob_v5.py:1171-1173](scripts/train_deeplob_v5.py#L1171-L1173)
- [configs/train_v5_improved.yaml:110](configs/train_v5_improved.yaml#L110)

---

## ğŸ“ˆ é æœŸæ”¹é€²æ•ˆæœ

| ä¿®å¾©é …ç›® | è¨“ç·´ç©©å®šæ€§ | æ”¶æ–‚é€Ÿåº¦ | æº–ç¢ºç‡ |
|---------|----------|---------|--------|
| TF32 æ­£ç¢ºå•Ÿç”¨ | +0% | **+20%** | +0% |
| ä¸‰é‡åŠ æ¬Šå®ˆé–€ | **+é«˜** | +0% | +0% |
| æ¬Šé‡è£å‰ª | +ä¸­ï¼ˆæœªä¾†ï¼‰ | +0% | +0% |
| GradScaler æ¨™æº–åŒ– | +0% | +0% | +0% |
| Warmup æ­£ç¢ºå¯¦ä½œ | **+é«˜** | **+ä¸­** | **+1-2%** |

**ç¸½é«”é æœŸ**:
- âœ… è¨“ç·´ç©©å®šæ€§å¤§å¹…æå‡
- âœ… è¨“ç·´é€Ÿåº¦æå‡ ~20%ï¼ˆTF32ï¼‰
- âœ… æ”¶æ–‚æ›´å¹³ç©©ï¼ˆWarmupï¼‰
- âœ… é¿å…æœªä¾†é…ç½®éŒ¯èª¤ï¼ˆå®ˆé–€é‚è¼¯ï¼‰

---

## ğŸ¯ é©—è­‰ä¿®å¾©çš„æ–¹æ³•

### 1. æª¢æŸ¥ TF32 æ˜¯å¦å•Ÿç”¨
```python
# è¨“ç·´é–‹å§‹æ™‚æ‡‰çœ‹åˆ°
âœ… TF32 å·²å•Ÿç”¨ï¼ˆallow_tf32=True + é«˜ç²¾åº¦ matmulï¼‰
```

### 2. æª¢æŸ¥ Warmup æ˜¯å¦ç”Ÿæ•ˆ
```python
# è¨“ç·´æ—¥èªŒæ‡‰é¡¯ç¤º
å­¦ä¹ ç‡è°ƒåº¦å™¨: Warmup (7 epochs) + Cosine
  warmup_start_factor: 0.01

# Epoch 0-7 å­¸ç¿’ç‡æ‡‰è©²ç·šæ€§å¢é•·
Epoch  1: lr=0.0000008
Epoch  2: lr=0.000001
...
Epoch  7: lr=0.00008
Epoch  8: lr=0.000079 (é–‹å§‹ Cosine è¡°æ¸›)
```

### 3. è¨“ç·´ç©©å®šæ€§
- å‰ 7 å€‹ epoch ä¸æ‡‰å‡ºç¾æ¢¯åº¦çˆ†ç‚¸
- æå¤±æ‡‰å¹³ç©©ä¸‹é™ï¼ˆä¸éœ‡ç›ªï¼‰

---

## ğŸš€ å¯ç«‹å³ä½¿ç”¨

æ‰€æœ‰ä¿®å¾©å·²æ•´åˆåˆ°ï¼š
- âœ… [scripts/train_deeplob_v5.py](scripts/train_deeplob_v5.py)
- âœ… [configs/train_v5_improved.yaml](configs/train_v5_improved.yaml)

### ç«‹å³é–‹å§‹è¨“ç·´
```bash
conda activate deeplob-pro

# å¿«é€Ÿæ¸¬è©¦ï¼ˆ10 epochsï¼‰
python scripts/train_deeplob_v5.py \
    --config configs/train_v5_improved.yaml \
    --epochs 10

# å®Œæ•´è¨“ç·´ï¼ˆ50 epochsï¼‰
python scripts/train_deeplob_v5.py \
    --config configs/train_v5_improved.yaml \
    --epochs 50
```

---

## ğŸ“š ç›¸é—œæ–‡æª”

- [IMPROVED_MODEL_READY.md](IMPROVED_MODEL_READY.md) - æ”¹é€²ç‰ˆæ¨¡å‹ä½¿ç”¨æŒ‡å—
- [TRAINING_DIAGNOSIS_AND_IMPROVEMENTS.md](TRAINING_DIAGNOSIS_AND_IMPROVEMENTS.md) - å®Œæ•´è¨ºæ–·å ±å‘Š
- [QUICK_START_IMPROVEMENTS.md](QUICK_START_IMPROVEMENTS.md) - å¿«é€Ÿä¸Šæ‰‹

---

## ğŸ™ è‡´è¬

æ„Ÿè¬ ChatGPT çš„è©³ç´°å¯©æŸ¥ï¼Œç™¼ç¾äº† 5 å€‹é—œéµå•é¡Œï¼Œå…¶ä¸­ **Warmup æœªç”Ÿæ•ˆ** æ˜¯æœ€åš´é‡çš„å•é¡Œï¼Œå¯èƒ½æ˜¯å°è‡´ä¹‹å‰è¨“ç·´å¡åœ¨ 45-46% çš„é‡è¦åŸå› ä¹‹ä¸€ã€‚

---

**æœ€å¾Œæ›´æ–°**: 2025-10-19
**ç‹€æ…‹**: âœ… æ‰€æœ‰ä¿®å¾©å·²å®Œæˆä¸¦æ¸¬è©¦
**å¯ç”¨æ€§**: ç«‹å³å¯ç”¨æ–¼è¨“ç·´
