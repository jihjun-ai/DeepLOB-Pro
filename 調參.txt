最佳點在更低的 LR 帶：LR≈3–4e-6 時，Val 指標到頂；之後 LR 再升（5e-6→6e-6）+ Grad 由 3.02→7.42，Val Loss 反彈、Val Acc 回落。

策略：這組資料/模型在低 LR 有效，一旦繼續拉高就進入不穩/過擬合邊緣。因此早停點應落在 Epoch 4 附近，或**warmup 到 4e-6 後改「常數 LR」**維持穩定。

立刻執行（Hotfix）

固定低 LR：warmup 至 4e-6 後改常數；或直接常數 3–4e-6。

啟用 EMA：ema.decay=0.999，用 EMA 權重做驗證/存檔，抑制波動。

早停規則：監控 val.f1_weighted；Val Loss 連 2 epoch 上升即停；或 Grad P95>4 即停。

若 Grad 尾部仍偏高：把 batch_size 從 512 → 384/256 小降一檔，保留 grad_clip_norm=1.0。

修改 train_v5.yaml 重點

lr: 4.0e-6、scheduler: constant + warmup_ratio: 0.30（暖起到 4e-6 後不再上調）

early_stopping / save_best 皆監控 val.f1_weighted

ema.enabled: true, decay: 0.999

grad_clip_norm: 1.0、weight_decay: 1e-3、dropout 維持你既有設定
（完整內容已寫入你的歷史檔，直接複製即可。）

20251022-deeplob調參歷史

驗收門檻（這條線過了才算「有學到」）

Val F1_w ≥ 基線 + 0.02

Train–Val 差距 ≤ 10–12%

Grad Norm：P50≈1–2、P95<4

以上是chatgpt 的分析,給你參考

訓練數據 (V5)
========================================================================================================================
| Epoch | Train Loss | Val Loss | Train Acc | Val Acc    | Val F1 (W) | Val F1 (U) | LR        | Grad |
| ----- | ---------- | -------- | --------- | ---------- | ---------- | ---------- | --------- | ---- |
| 1     | 0.9850     | 0.9306   | 45.57    % | 49.87%     | 0.3970     | 0.3970     | 0.000001  | 1.35 |
| 2     | 0.9358     | 0.8996   | 48.60    % | 51.11%     | 0.4579     | 0.4579     | 0.000002  | 1.46 |
| 3     | 0.9071     | 0.8858   | 50.29    % | 51.37%     | 0.4825     | 0.4825     | 0.000003  | 1.91 |
| 4     | 0.8782     | 0.8828   | 51.87    % | **51.55%** | 0.4793     | 0.4793     | 0.000004  | 3.02 |
| 5     | 0.8469     | 0.8917   | 53.66    % | 51.24%     | 0.4823     | 0.4823     | 0.000005  | 4.90 |
| 6     | 0.8102     | 0.9205   | 56.05    % | 50.81%     | 0.4750     | 0.4750     | 0.000006  | 7.42 |

參數檔:train_v5.yaml
參考調參歷史記錄：docs\20251022-deeplob調參歷史.md 記錄內容,調參後需更新供下次參考用