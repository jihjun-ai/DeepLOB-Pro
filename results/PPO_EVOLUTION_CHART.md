# PPO 調參演進路線圖

**時間線**: PPO_5 → PPO_11 → PPO_13 → **PPO_14** (當前)

---

## 📊 核心指標演進

```
┌─────────────────────────────────────────────────────────────────┐
│                    Episode 獎勵演進                              │
├─────────────────────────────────────────────────────────────────┤
│                                                                  │
│  +30 ┤                                                           │
│      │            ╭──────╮ PPO_11: +24.15                       │
│  +20 ┤            │      │  (持倉激勵成功)                      │
│      │            │      │                                      │
│  +10 ┤            │      │                                      │
│      │            │      │                                      │
│    0 ┼────────────┼──────┼────────────●─────── PPO_14: 目標 > 0│
│      │            │      │            PPO_13: -2.91            │
│  -10 ┤            │      │            (過度保守)                │
│      │            │      ╰────────────╯                         │
│  -20 ┤      ●─────╯                                             │
│      │      PPO_5: -24.37                                       │
│  -30 ┤      (基線實驗)                                          │
│      └────┴────┴────┴────┴────┴────┴────┴────┴────┴────┴────   │
│      PPO_5  PPO_6  PPO_8  PPO_9 PPO_11 PPO_13     PPO_14       │
└─────────────────────────────────────────────────────────────────┘
```

---

## 🎯 Buy 比例演進 (目標: 15%-40%)

```
  100% ┤
       │                 ╭────╮ PPO_11: 99.4%
   80% ┤                 │    │ (過度交易)
       │                 │    │
   60% ┤  ┌────────┐     │    │
       │  │  目標區 │     │    │
   40% ┤  │  15-40% │     │    │     ┌────────┐ PPO_14: 目標
       │  └────────┘     │    │     │  ???   │
   20% ┤                 │    │     └────────┘
       │                 │    ╰──────────╮
    0% ┼─────────────────┴────────────────●─── PPO_13: 2%
       └────┴────┴────┴────┴────┴────┴────┴────
       PPO_5  PPO_6  PPO_8  PPO_9 PPO_11 PPO_13  PPO_14
                                        (過度保守)
```

---

## 🔧 獎勵機制演進史

### PPO_5 (基線) - 簡化獎勵

```
獎勵 = PnL - 交易成本
```

**結果**:
- ❌ Episode 獎勵: -24.37
- ❌ 未學到交易策略
- 🔍 **問題**: 缺乏交易激勵

---

### PPO_11 (持倉激勵) - 第一次突破 ⭐⭐⭐⭐⭐

```
獎勵 = PnL - 交易成本 + 持倉激勵
                       ↑
                  +0.05/步 (有持倉時)
```

**結果**:
- ✅ Episode 獎勵: +24.15 (首次大幅盈利！)
- ✅ 模型開始交易 (Buy 0% → 99.4%)
- ❌ **問題**: 過度交易 (99.4% Buy)

**關鍵發現**:
> 持倉激勵確實有效，但 +0.05 過強

---

### PPO_13 (未平倉懲罰) - 矯枉過正 ⚠️

```
獎勵 = PnL - 交易成本 - 未平倉懲罰
                         ↑
                    Episode 末 -10/倉位
```

**結果**:
- ❌ Episode 獎勵: -2.91 (倒退 138%)
- ❌ Buy 比例: 2.0% (過度保守)
- ✅ KL 散度: 0.0036 (穩定)
- ✅ 解釋方差: 0.966 (優秀)

**關鍵發現**:
> 訓練指標優秀 ≠ 實際性能良好
> -10 元懲罰過於嚴苛 (= 40% 持倉獎勵)

---

### PPO_14 (降低懲罰) - 尋找平衡點 🎯

```
獎勵 = PnL - 交易成本 - 未平倉懲罰
                         ↑
                    Episode 末 -2/倉位 (降低 80%)
```

**目標**:
- 🎯 Episode 獎勵: > 0 (實際盈利)
- 🎯 Buy 比例: 15%-40% (適度交易)
- 🎯 保持訓練穩定 (KL < 0.02)

**設計理念**:
- -2 元 = 8% 持倉獎勵 (vs. PPO_13 的 40%)
- 仍鼓勵當日平倉，但不過度抑制交易

---

## 📈 關鍵指標對比表

| 實驗 | Episode 獎勵 | Buy 比例 | KL 散度 | 解釋方差 | 評價 |
|------|-------------|---------|---------|----------|------|
| **PPO_5** | -24.37 | - | 0.129 ❌ | -0.524 ❌ | 基線 |
| **PPO_11** | **+24.15** ✅ | 99.4% ❌ | 0.0076 ✅ | 0.827 ✅ | 過度交易 |
| **PPO_13** | -2.91 ❌ | 2.0% ❌ | 0.0036 ✅ | 0.966 ✅ | 過度保守 |
| **PPO_14** | 🎯 **> 0** | 🎯 **15-40%** | 🎯 **< 0.02** | 🎯 **> 0.7** | **待驗證** |

---

## 🔄 策略演進邏輯圖

```
┌─────────────────────────────────────────────────────────────────┐
│                                                                  │
│   PPO_5 基線                                                     │
│   ├─ 問題: 不交易                                               │
│   └─ 方向: 需要交易激勵                                         │
│        │                                                         │
│        ▼                                                         │
│   PPO_11 持倉激勵 (+0.05/步)                                    │
│   ├─ 成功: Episode 獎勵 +24.15                                  │
│   ├─ 問題: 過度交易 (99.4% Buy)                                │
│   └─ 方向: 需要抑制過度持倉                                     │
│        │                                                         │
│        ▼                                                         │
│   PPO_13 未平倉懲罰 (-10/倉位)                                  │
│   ├─ 問題: 過度保守 (2% Buy)                                   │
│   ├─ 診斷: 懲罰過重 (-10 = 40% 獎勵)                           │
│   └─ 方向: 降低懲罰強度                                         │
│        │                                                         │
│        ▼                                                         │
│   PPO_14 降低懲罰 (-2/倉位) ← 我們在這裡                        │
│   ├─ 目標: 15-40% Buy (適度交易)                               │
│   ├─ 目標: Episode 獎勵 > 0 (盈利)                             │
│   └─ 備案: 如失敗 → 方案 B/C/D                                 │
│        │                                                         │
│        ▼                                                         │
│   如成功: 延長訓練至 500K-1M steps                              │
│   如失敗: 切換漸進式懲罰 / 混合策略                             │
│                                                                  │
└─────────────────────────────────────────────────────────────────┘
```

---

## 💡 核心洞察總結

### 1. 獎勵設計的平衡藝術

```
過度激勵 (PPO_11)     平衡點 (PPO_14)     過度懲罰 (PPO_13)
     │                     │                      │
  +0.05/步              -2/倉位                -10/倉位
     │                     │                      │
  99.4% Buy    →     15-40% Buy (目標)  ←     2% Buy
   (過度)                (適度)                 (保守)
```

### 2. 訓練指標 vs. 實際性能

```
PPO_13 案例:
  訓練指標 ✅  ≠  實際性能 ❌

  ✅ KL 散度: 0.0036 (穩定)
  ✅ 解釋方差: 0.966 (優秀)
  ❌ Buy 比例: 2% (不交易)
  ❌ Episode 獎勵: -2.91 (虧損)

  教訓: 必須驗證實際交易行為！
```

### 3. 參數調整的黃金法則

```
1. 小步調整: -10 → -2 (降低 80%)
2. 快速驗證: 200K steps (~28 分鐘)
3. 多方案備案: 準備 B/C/D 方案
4. 參考成功經驗: PPO_11 證明 +24 可達成
```

---

## 🚀 下一步行動

### 立即執行 (PPO_14)

```bash
# 一鍵訓練
run_ppo14.bat

# 或手動執行
conda activate deeplob-pro
python scripts/train_sb3_deeplob.py --config configs/sb3_deeplob_config.yaml
```

### 訓練後驗證 (3 步驟)

```bash
# 1. TensorBoard 分析
python scripts/analyze_tensorboard.py --log-dir logs/sb3_deeplob/PPO_14

# 2. 交易行為檢查
python scripts/check_trading_behavior.py --model checkpoints/sb3/ppo_deeplob/best_model

# 3. 完整性能評估
python scripts/evaluate_sb3.py --model checkpoints/sb3/ppo_deeplob/best_model --n_episodes 20
```

### 根據結果決策

```
如果 Buy 比例 > 15% 且獎勵 > 0:
  ✅ 成功！延長訓練至 500K-1M steps

如果 Buy 比例 5%-15%:
  ⚠️ 部分成功，進一步降低懲罰 (-2 → -1)

如果 Buy 比例 < 5%:
  ❌ 失敗，切換方案 B (漸進式懲罰)
```

---

**最後更新**: 2025-10-27
**當前階段**: PPO_14 準備訓練
**預計時間**: ~28 分鐘 (200K steps)
