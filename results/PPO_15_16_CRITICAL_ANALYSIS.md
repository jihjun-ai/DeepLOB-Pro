# PPO_15 失敗分析與 PPO_16 最後一搏

**日期**: 2025-10-27
**狀態**: 🚨 **關鍵時刻 - 黃金區間最後搜索**

---

## 🚨 嚴峻現實

經過 **6 次實驗** (PPO_11/13/14/15)，我們仍未找到穩定盈利的配置：

| 實驗 | 懲罰值 | Buy 比例 | Episode 獎勵 | 評價 |
|------|--------|---------|-------------|------|
| PPO_11 | 0 (+0.05) | 99.4% 🔴 | **+24.15** ✅ | 唯一盈利，但過度交易 |
| PPO_13 | -10 | 2.0% 🔴 | -2.91 ❌ | 過度保守 |
| PPO_14 | -2 | 77.8% 🔴 | -3.18 ❌ | 過度激進 |
| **PPO_15** | **-5** | **4.4%** 🔴 | **-1.31** ❌ | **仍保守** |

---

## 📊 PPO_15 關鍵發現

### 懲罰曲線非線性（重大發現）

```
懲罰強度 → Buy 比例
-10 元 → 2.0%   (極度保守)
 -5 元 → 4.4%   (仍保守) ← PPO_15
 -3 元 → ???%   (未知) ← PPO_16 最後一搏
 -2 元 → 77.8%  (極度激進)
```

**核心洞察**:
- ⚠️ **懲罰曲線陡峭**: -5 到 -2 之間，Buy 比例從 4.4% 暴漲至 77.8%
- ⚠️ **黃金區間極窄**: -3 到 -4 可能是唯一可行範圍
- ✅ **有盈利潛力**: PPO_15 最佳 Episode 達 0.00 (盈虧平衡)

### vf_coef 提升有效

```
解釋方差改善:
PPO_14 (vf_coef=1.0): 0.064 ❌ (價值函數崩潰)
PPO_15 (vf_coef=1.5): 0.123 ⚠️ (改善 92%，但仍偏低)
```

**結論**: vf_coef=1.5 有效，應保持

---

## 🎯 PPO_16 最後一搏

### 核心策略

**未平倉懲罰**: `-5 元 → -3 元` (降低 40%)

**理由**:
1. **黃金區間下界**: -3 到 -4 之間，選擇 -3 偏向激進
2. **最後機會**: 若 -3 仍失敗，固定懲罰方案徹底失敗
3. **備用方案**: 準備切換至 ChatGPT 動態獎勵方案

### 預期結果

**樂觀預期** (Buy 30-50%):
```
✅ 成功找到平衡點
→ 延長訓練至 1M steps
→ 項目成功
```

**保守預期** (Buy 10-30%):
```
⚠️ 接近目標但仍需微調
→ PPO_17: -3.5 元 (在 -3 和 -4 之間)
→ 或改用 -4 元
```

**失敗預期** (Buy < 10% 或 > 60%):
```
❌ 固定懲罰方案徹底失敗
→ 立即切換 ChatGPT 動態獎勵方案
→ 持倉激勵衰減 0.02→0.005 (50K steps)
```

---

## 🔄 備用方案（若 PPO_16 失敗）

### 方案 B: ChatGPT 動態獎勵方案 ⭐⭐⭐⭐⭐

**核心設計**: 持倉激勵隨訓練衰減

```python
# reward_shaper.py (新增)
class DecayingHoldingBonus:
    def __init__(self):
        self.start_bonus = 0.02      # 初始激勵
        self.end_bonus = 0.005       # 最終激勵
        self.warmup_steps = 50000    # 衰減週期
        self.current_step = 0

    def get_bonus(self, position):
        progress = min(1.0, self.current_step / self.warmup_steps)
        current_bonus = self.start_bonus - (self.start_bonus - self.end_bonus) * progress
        return current_bonus * position if position > 0 else 0.0

# tw_lob_trading_env.py (修改)
reward += self.holding_bonus.get_bonus(self.position)
if episode_end and position > 0:
    reward -= 5.0 * position  # 保持適度未平倉懲罰
```

**優勢**:
- ✅ 早期鼓勵交易（0.02），後期降低激勵（0.005）
- ✅ 避免「永遠持倉」（PPO_11 的 99.4% 問題）
- ✅ 避免「永遠不動」（PPO_13 的 2% 問題）
- ✅ 自適應調整，無需手動尋找黃金區間

**實施時間**: ~2 小時（代碼修改 + 測試）

### 方案 C: 回到 PPO_11 基礎 ⭐⭐⭐

**核心設計**: 降低固定持倉激勵

```python
# 移除未平倉懲罰
unclosed_penalty = 0

# 降低持倉激勵 80%
holding_bonus = +0.01 * position  # PPO_11: +0.05
```

**優勢**:
- ✅ PPO_11 已證明可盈利 (+24.15)
- ✅ 實施簡單，無需新代碼
- ✅ 可能 Buy 比例降至 40-60%（仍偏高但可接受）

**風險**: 可能仍過度交易（50-70% Buy）

---

## 📈 六代演進總結

```
PPO_5  (基線)      → -24.37 (不交易)
PPO_11 (持倉+0.05)  → +24.15 (盈利！但 Buy 99.4%)
PPO_13 (懲罰-10)    → -2.91  (Buy 2%，過度保守)
PPO_14 (懲罰-2)     → -3.18  (Buy 77.8%，過度激進)
PPO_15 (懲罰-5)     → -1.31  (Buy 4.4%，仍保守)
PPO_16 (懲罰-3)     → ???    (最後一搏)
```

**教訓**:
1. ❌ **固定獎勵難以平衡**: 6 次實驗未找到穩定配置
2. ❌ **懲罰曲線陡峭**: -2 到 -5 之間變化劇烈
3. ✅ **動態獎勵是出路**: ChatGPT 方案可能是唯一解

---

## ⏰ 決策時間線

### 現在 (16:00)
```
✅ PPO_16 配置已更新（-3 元/倉位）
⏳ 準備開始訓練 (~23 分鐘)
```

### 16:30 (訓練完成)
```
檢查 Buy 比例:

如果 15-40%:
  🎉 成功！延長至 1M steps
  → 項目成功完成

如果 10-60%:
  ⚠️ 需微調，PPO_17: -3.5 或 -4
  → 再給固定懲罰一次機會

如果 < 10% 或 > 60%:
  ❌ 固定懲罰徹底失敗
  → 立即切換動態獎勵方案 (方案 B)
```

---

## 💡 關鍵教訓

### 為什麼固定懲罰如此困難？

1. **懲罰曲線非線性**:
   - 小幅調整 → 巨大行為變化
   - -5 → -3 (降 40%)，Buy 可能從 4% 跳到 50%+

2. **價值函數適應慢**:
   - 獎勵劇變 → 價值函數失效（PPO_14 解釋方差崩潰）
   - vf_coef 提升只是緩解，非根治

3. **探索 vs 利用失衡**:
   - 過度懲罰 → 不敢探索 (PPO_13/15)
   - 過輕懲罰 → 過度探索 (PPO_14)

### 為什麼動態獎勵可能成功？

1. **漸進式調整**:
   - 早期: 高激勵 → 鼓勵探索
   - 後期: 低激勵 → 精煉策略
   - 價值函數有時間適應

2. **避免極端**:
   - 沒有固定「懸崖」
   - 策略平滑演化

3. **業界驗證**:
   - Curriculum Learning 成熟技術
   - ChatGPT 建議基於業界經驗

---

## 🚨 最後警告

**若 PPO_16 失敗，我們必須承認**:

1. ❌ **固定未平倉懲罰方案不可行**
   - 6 次實驗未找到穩定配置
   - 黃金區間過窄，難以命中

2. ✅ **需要範式轉移**
   - 從「固定懲罰」→「動態獎勵」
   - 從「手動調參」→「自適應學習」

3. ⏰ **時間成本考量**
   - 每次實驗 ~23 分鐘
   - 繼續盲目搜索 = 浪費時間
   - 切換方案 = 2 小時一次性投入

---

## 📋 PPO_16 檢查清單

### 訓練前 (✅ 已完成)
- [x] unclosed_penalty = -3.0 (tw_lob_trading_env.py:380)
- [x] vf_coef = 1.5 (sb3_deeplob_config.yaml:89)
- [x] total_timesteps = 200K
- [x] 調參記錄已更新

### 訓練中 (~23 分鐘)
- [ ] 監控 TensorBoard: rollout/ep_rew_mean
- [ ] 監控 KL 散度 (< 0.02)
- [ ] 監控解釋方差 (> 0.2)

### 訓練後 (關鍵決策)
- [ ] 分析 TensorBoard: `analyze_tensorboard.py --log-dir logs/sb3_deeplob/PPO_16`
- [ ] **檢查交易行為**: `check_trading_behavior.py`
- [ ] **決策點**: Buy 比例是否在 15-40%？

---

## 🎯 成敗標準

### ✅ 成功 (機率: 30%)
```
Buy 比例: 15-40%
Episode 獎勵: > -1
→ 延長至 1M steps
→ 項目成功
```

### ⚠️ 接近 (機率: 30%)
```
Buy 比例: 10-60% (但不在 15-40%)
Episode 獎勵: -1 到 +1
→ PPO_17: 微調 -3.5 或 -4
→ 再給一次機會
```

### ❌ 失敗 (機率: 40%)
```
Buy 比例: < 10% 或 > 60%
Episode 獎勵: < -2
→ 固定懲罰方案徹底失敗
→ 切換動態獎勵方案 (2 小時實施)
```

---

**最後更新**: 2025-10-27 16:05
**狀態**: ⏳ PPO_16 待訓練 (最後一搏)
**下一步**: 執行訓練 → 根據結果決定項目方向
