# PPO_19 訓練失敗分析報告

**日期**: 2025-10-27
**訓練時長**: 2.78 小時（1.5M steps）
**結論**: ❌ **完全失敗 - 模型不交易**

---

## 執行摘要

PPO_19 嘗試通過降低未平倉懲罰至 -2.5 元來達成穩定盈利，但實測結果顯示**模型完全不交易**（Buy 比例 0%），與 PPO_11 的「永遠持倉」問題相反，陷入「永遠不動」陷阱。

---

## 訓練指標

### 正面指標
- ✅ **KL 散度**: 0.0047（< 0.02，歷史最佳穩定性）
- ✅ **總損失收斂**: 0.0538 → 3e-07（完全收斂）
- ✅ **訓練效率**: 149.7 steps/秒

### 負面指標
- ❌ **解釋方差崩潰**: 0.297（< 0.7，價值函數失效）
  - PPO_18: 0.989 → PPO_19: 0.297（-70% 退步）
- ❌ **平均獎勵仍負**: -2.23 ± 3.84（中位數虧損）
- ⚠️ **趨勢減弱**: R² = 0.372
  - PPO_16: 0.951 → PPO_18: 0.088 → PPO_19: 0.372

---

## 交易行為驗證（關鍵發現）

運行 `check_trading_behavior.py --model best_model --n_episodes 5`:

```
動作統計:
  Hold/Sell (0): 2500 (100.0%)
  Buy (1): 0 (0.0%)

交易統計:
  實際交易次數: 0
  平均每 Episode: 0.0 次

Episode 獎勵:
  平均獎勵: 0.00 ± 0.00
  最大獎勵: 0.00
  最小獎勵: 0.00

警告:
  [WARN] 模型完全不交易
  [WARN] Episode 獎勵 0.00 是假象，不是盈利
```

**結論**: 模型學到「永遠不動 = 最安全」策略。

---

## 根本原因分析

### 1. 懲罰強度的非線性陷阱

| 懲罰值 | Steps | Buy比例 | 實際獎勵 | 解釋方差 | 策略行為 |
|--------|-------|---------|---------|---------|---------|
| -3 元 | 200K | **7.3%** | **-0.15** | **0.596** ✅ | ✅ 短訓練：尚未收斂，仍在探索 |
| -3 元 | 1M | 3.4% | -0.51 | 0.989 ✅ | ⚠️ 長訓練：收斂至保守 |
| **-2.5 元** | **1.5M** | **0%** ❌ | **0.00（假）** | **0.297** ❌ | ❌ **長訓練：完全不交易** |
| -2 元 | 200K | 77.8% | -3.18 | 0.064 ❌ | ❌ 過度激進，價值函數崩潰 |

**關鍵洞察**:
- -3 → -2.5 降幅僅 **17%**，但 Buy 從 3.4% → **0%**（崩潰）
- -2 到 -3 之間存在**「死亡區間」**，任何微調都可能觸發陷阱

### 2. 解釋方差崩潰的惡性循環

```
解釋方差低 (0.297)
    ↓
價值函數無法準確評估
    ↓
策略選擇保守（不交易）
    ↓
經驗單一（只有 Hold）
    ↓
價值函數更難學習
    ↓
解釋方差更低...（循環）
```

### 3. 長訓練 ≠ 更好

- **PPO_16** (200K): Buy 7.3%，未收斂，仍在探索 ✅
- **PPO_18** (1M): Buy 3.4%，過度收斂至保守 ⚠️
- **PPO_19** (1.5M): Buy 0%，完全收斂至「不交易」 ❌

**結論**: 延長訓練只會讓策略「更完美」收斂至局部最優。

---

## 為什麼 -2.5 元失敗？

### 假設 vs 現實

| 項目 | 假設 | 現實 |
|-----|------|------|
| 懲罰效果 | -2.5 元可鼓勵適度交易 | 仍過重，模型學到「不交易 = 0」最優 |
| 解釋方差 | 保持 > 0.9（PPO_18 成功經驗） | 崩潰至 0.297（-70% 退步） |
| Buy 比例 | 5-10%（精選交易） | 0%（完全不交易） |
| 平均獎勵 | > 0（穩定盈利） | -2.23（仍虧損，最終 0 是假象） |

### 關鍵錯誤

1. ❌ **忽略解釋方差**: PPO_18 的 0.989 是因為 -3 元，降至 -2.5 導致價值函數崩潰
2. ❌ **誤判「黃金區間」**: -3 到 -2.5 之間沒有「黃金點」，是「死亡區間」
3. ❌ **過度優化**: 延長至 1.5M 讓策略完美收斂至錯誤的局部最優

---

## 對比 PPO_16（唯一成功配置）

| 指標 | PPO_16 (200K, -3元) | PPO_19 (1.5M, -2.5元) | 評價 |
|-----|---------------------|----------------------|------|
| Buy 比例 | **7.3%** ✅ | 0% ❌ | PPO_16 勝 |
| 最佳獎勵 | **+1.18** ✅ | 0（假象） ❌ | PPO_16 勝 |
| 勝率 | **40%** (2/5) ✅ | 0% (0/5) ❌ | PPO_16 勝 |
| 解釋方差 | **0.596** ✅ | 0.297 ❌ | PPO_16 勝 |
| 趨勢 R² | **0.951** ✅ | 0.372 ⚠️ | PPO_16 勝 |
| 訓練時間 | 21 分鐘 | 2.78 小時 | PPO_16 勝 |

**結論**: PPO_16 在所有指標上碾壓 PPO_19，且僅需 **1/8 訓練時間**。

---

## 教訓與洞察

### 關鍵教訓

1. ⚠️ **懲罰曲線非線性**: 微小調整（17%）可導致行為劇變（100% → 0%）
2. ⚠️ **解釋方差是命脈**: 低於 0.5 會觸發惡性循環，策略崩潰
3. ⚠️ **長訓練需謹慎**: 配合 LR 衰減，否則只會「完美」收斂至錯誤
4. ✅ **PPO_16 是唯一希望**: 趨勢強勁未收斂，延長訓練可能成功

### 成功路徑

```
PPO_16 (200K, -3元)
  ↓ 延長訓練 15 倍
  ↓ 配合 LR 線性衰減
  ↓ 保持所有其他參數
PPO_20 (3M, -3元, LR衰減)
  ↓ 預期結果
Buy 10-15%, 獎勵 > +2, 勝率 > 60%
```

---

## 下一步：PPO_20 配置

### 核心策略

- **放棄微調懲罰**，回到 PPO_16 成功配置
- **延長訓練至 3M** + **LR 線性衰減**（1e-4 → 3e-5）
- **保持所有其他參數**（vf_coef=1.5, -3 元懲罰）

### 配置更改

```yaml
# configs/sb3_deeplob_config.yaml
ppo:
  learning_rate: 0.0001         # 保持 1e-4 起始

training:
  total_timesteps: 3000000      # 200K → 3M（延長 15 倍）

advanced:
  lr_schedule: "linear"         # constant → linear（1e-4 → 3e-5）
```

```python
# src/envs/tw_lob_trading_env.py (Line 380)
unclosed_penalty = -3.0  # 保持 PPO_16 的值
```

### 預期結果

- Buy 比例: 7.3% → 10-15%
- Episode 獎勵: -0.15 → +2 到 +5
- 勝率: 40% → 60%+
- 訓練時間: ~5.5 小時

### 成功標準

- ✅ **成功**: Buy 10-20% + 獎勵 > +2 + 勝率 > 60%
- ⚠️ **部分成功**: Buy 7-15% + 獎勵 > 0 + 勝率 > 50%
- ❌ **失敗**: Buy < 5% 或獎勵 < 0 → 考慮增強價值函數（vf_coef=2.0）

---

## 執行指令

```bash
# 1. 執行訓練（5.5 小時）
run_ppo20.bat

# 2. 分析結果
python scripts\analyze_tensorboard.py --log-dir logs\sb3_deeplob\PPO_20

# 3. 驗證行為
python scripts\check_trading_behavior.py --model checkpoints\sb3\ppo_deeplob\best_model.zip --n_episodes 10

# 4. 評估性能
python scripts\evaluate_sb3.py --model checkpoints\sb3\ppo_deeplob\best_model.zip --n_episodes 20
```

---

## 結論

PPO_19 的失敗證明：
1. ❌ 微調懲罰（-3 → -2.5）風險極高
2. ❌ 長訓練不配合 LR 衰減會過度收斂
3. ✅ **PPO_16 是唯一經過驗證的成功配置**
4. 🎯 **回到 PPO_16 + 延長訓練 + LR 衰減 = PPO_20（最後希望）**

---

**報告生成時間**: 2025-10-27 23:00
**下一步**: 執行 PPO_20 訓練（預計 5.5 小時）
