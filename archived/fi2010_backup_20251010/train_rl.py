"""RLlib PPO å¼·åŒ–å­¸ç¿’è¨“ç·´è…³æœ¬

æ­¤è…³æœ¬è² è²¬è¨“ç·´åŸºæ–¼ PPO ç®—æ³•çš„é«˜é »äº¤æ˜“ç­–ç•¥ã€‚
æ•´åˆ DeepLOB åƒ¹æ ¼é æ¸¬æ¨¡å‹å’Œ LSTM Policy Networkï¼Œ
åœ¨ LOB äº¤æ˜“ç’°å¢ƒä¸­å­¸ç¿’æœ€å„ªäº¤æ˜“æ±ºç­–ã€‚

è¨“ç·´æ¶æ§‹:
    è¼¸å…¥: LOB ç‰¹å¾µ (40ç¶­) + DeepLOB é æ¸¬ (3ç¶­) + äº¤æ˜“ç‹€æ…‹ (5ç¶­)
    æ¨¡å‹: RecurrentPPO + LSTM (256 units, 2 layers)
    ç’°å¢ƒ: LOBTradingEnv (Gymnasium æ¨™æº–)
    è¼¸å‡º: äº¤æ˜“å‹•ä½œ {Hold, Buy, Sell}

è¨“ç·´ç›®æ¨™:
    - Sharpe Ratio > 2.0
    - å‹ç‡ > 55%
    - GPU åˆ©ç”¨ç‡ > 85%
    - ç©©å®šæ”¶æ–‚

ç¡¬é«”éœ€æ±‚:
    - NVIDIA RTX 5090 (32GB VRAM)
    - CUDA 12.9
    - 16+ CPU cores

ä½œè€…: RLlib-DeepLOB å°ˆæ¡ˆåœ˜éšŠ
æ›´æ–°: 2025-01-09
"""

import os
import sys
import argparse
import logging
from pathlib import Path
from datetime import datetime
import yaml

import ray
from ray import tune
from ray.rllib.algorithms.ppo import PPOConfig
from ray.rllib.models import ModelCatalog
from ray.tune.logger import pretty_print

# æ·»åŠ å°ˆæ¡ˆæ ¹ç›®éŒ„åˆ°è·¯å¾‘
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from src.envs.lob_trading_env import LOBTradingEnv
from src.models.trading_lstm_model import TradingLSTMModel, register_trading_lstm_model
from src.utils.logger import setup_logger
from src.utils.config_loader import load_config

# è¨­å®šæ—¥èªŒ
logger = setup_logger(__name__)


def register_environment():
    """è¨»å†Šè‡ªè¨‚äº¤æ˜“ç’°å¢ƒåˆ° RLlib

    æ­¤å‡½æ•¸å°‡ LOBTradingEnv è¨»å†Šåˆ° Ray Tune çš„ç’°å¢ƒè¨»å†Šè¡¨ï¼Œ
    ä½¿å…¶å¯ä»¥åœ¨ RLlib é…ç½®ä¸­é€šéåç¨±å¼•ç”¨ã€‚

    è¨»å†Šåç¨±: "LOBTradingEnv"

    ä½¿ç”¨æ–¹å¼:
        åœ¨ PPO é…ç½®ä¸­ä½¿ç”¨ .environment("LOBTradingEnv")
    """
    from ray.tune.registry import register_env

    def env_creator(env_config):
        """ç’°å¢ƒå‰µå»ºå‡½æ•¸

        æ­¤å‡½æ•¸ä½œç‚ºå·¥å» æ¨¡å¼å‰µå»ºç’°å¢ƒå¯¦ä¾‹ã€‚
        RLlib æœƒåœ¨æ¯å€‹ worker ä¸Šèª¿ç”¨æ­¤å‡½æ•¸å‰µå»ºç¨ç«‹çš„ç’°å¢ƒã€‚

        åƒæ•¸:
            env_config: ç’°å¢ƒé…ç½®å­—å…¸ï¼Œä¾†è‡ª PPOConfig

        è¿”å›:
            LOBTradingEnv: é…ç½®å¥½çš„ç’°å¢ƒå¯¦ä¾‹
        """
        return LOBTradingEnv(env_config)

    register_env("LOBTradingEnv", env_creator)
    logger.info("âœ“ å·²è¨»å†Š LOBTradingEnv ç’°å¢ƒ")


def create_ppo_config(args, env_config: dict, model_config: dict) -> PPOConfig:
    """å‰µå»º PPO ç®—æ³•é…ç½®

    æ­¤å‡½æ•¸æ§‹å»ºå®Œæ•´çš„ RecurrentPPO é…ç½®ï¼Œé‡å°é«˜é »äº¤æ˜“ä»»å‹™å„ªåŒ–ã€‚

    åƒæ•¸:
        args: å‘½ä»¤è¡Œåƒæ•¸
            åŒ…å«å­¸ç¿’ç‡ã€è¨“ç·´æ‰¹æ¬¡å¤§å°ç­‰è¶…åƒæ•¸

        env_config: ç’°å¢ƒé…ç½®å­—å…¸
            åŒ…å« max_steps, transaction_cost_rate ç­‰ç’°å¢ƒåƒæ•¸

        model_config: æ¨¡å‹é…ç½®å­—å…¸
            åŒ…å« lstm_cell_size, num_lstm_layers ç­‰æ¨¡å‹åƒæ•¸

    è¿”å›:
        PPOConfig: é…ç½®å®Œæ•´çš„ PPO é…ç½®å°è±¡

    é…ç½®é‡é»:
        1. å•Ÿç”¨ LSTMï¼ˆRecurrentPPOï¼‰
        2. GPU è³‡æºåˆ†é…
        3. è¨“ç·´æ‰¹æ¬¡å¤§å°å„ªåŒ–
        4. Actor-Critic åˆ†é›¢
        5. æ¢ç´¢ç­–ç•¥

    åƒè€ƒ:
        CLAUDE.md ä¸­çš„ PPO Config æœªå•Ÿç”¨ LSTM å•é¡Œä¿®æ­£
    """
    config = (
        PPOConfig()
        # ========== ç’°å¢ƒé…ç½® ==========
        .environment(
            env="LOBTradingEnv",
            env_config=env_config,
        )
        # ========== æ¡†æ¶é…ç½® ==========
        .framework("torch")
        # ========== è¨“ç·´è¶…åƒæ•¸ ==========
        .training(
            # å­¸ç¿’ç‡
            lr=args.lr,
            # æŠ˜æ‰£å› å­ï¼ˆè¶Šæ¥è¿‘1è¶Šé‡è¦–é•·æœŸå›å ±ï¼‰
            gamma=args.gamma,
            # GAE Lambdaï¼ˆå„ªå‹¢å‡½æ•¸ä¼°è¨ˆï¼‰
            lambda_=0.95,
            # PPO è£å‰ªåƒæ•¸ï¼ˆé˜²æ­¢ç­–ç•¥æ›´æ–°éå¤§ï¼‰
            clip_param=0.2,
            # ç†µä¿‚æ•¸ï¼ˆé¼“å‹µæ¢ç´¢ï¼‰
            entropy_coeff=args.entropy_coeff,
            # KL æ•£åº¦ç›®æ¨™ï¼ˆè‡ªé©æ‡‰ lrï¼‰
            kl_coeff=0.2,
            kl_target=0.01,
            # è¨“ç·´æ‰¹æ¬¡å¤§å°ï¼ˆå……åˆ†åˆ©ç”¨ RTX 5090ï¼‰
            train_batch_size=args.train_batch_size,
            # SGD å°æ‰¹æ¬¡å¤§å°
            sgd_minibatch_size=args.sgd_minibatch_size,
            # SGD è¿­ä»£æ¬¡æ•¸
            num_sgd_iter=10,
            # åƒ¹å€¼å‡½æ•¸æå¤±ä¿‚æ•¸
            vf_loss_coeff=0.5,
            # æ¢¯åº¦è£å‰ªï¼ˆé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸ï¼‰
            grad_clip=0.5,
        )
        # ========== Rollout é…ç½® ==========
        .rollouts(
            # Worker æ•¸é‡ï¼ˆCPU ä¸¦è¡Œæ¡æ¨£ï¼‰
            num_rollout_workers=args.num_workers,
            # æ¯å€‹ worker çš„ rollout ç‰‡æ®µé•·åº¦
            # éœ€è¦èˆ‡ episode é•·åº¦å”èª¿
            rollout_fragment_length=200,
            # æ‰¹æ¬¡æ¨¡å¼ï¼ˆcomplete_episodes ç¢ºä¿ LSTM ç‹€æ…‹æ­£ç¢ºï¼‰
            batch_mode="complete_episodes",
        )
        # ========== è³‡æºé…ç½® ==========
        .resources(
            # Learner GPUï¼ˆè¨“ç·´ç”¨ï¼‰
            num_gpus=1.0 if args.num_gpus > 0 else 0.0,
            # æ¯å€‹ worker çš„ CPU æ•¸é‡
            num_cpus_per_worker=2,
            # æ¯å€‹ worker çš„ GPU æ•¸é‡ï¼ˆå¯é¸ï¼Œç”¨æ–¼ç’°å¢ƒæ¨ç†ï¼‰
            num_gpus_per_worker=0,
        )
        # ========== æ¨¡å‹é…ç½®ï¼ˆé—œéµï¼ï¼‰==========
        .model({
            # ä½¿ç”¨è‡ªè¨‚ LSTM æ¨¡å‹
            "custom_model": "TradingLSTMModel",
            # è‡ªè¨‚æ¨¡å‹é…ç½®
            "custom_model_config": model_config,
            # å•Ÿç”¨ LSTMï¼ˆRecurrentPPOï¼‰
            "use_lstm": True,
            # LSTM éš±è—å±¤å¤§å°
            "lstm_cell_size": model_config.get("lstm_cell_size", 256),
            # LSTM æœ€å¤§åºåˆ—é•·åº¦
            "max_seq_len": model_config.get("max_seq_len", 100),
            # LSTM ä½¿ç”¨å‰ä¸€å‹•ä½œä½œç‚ºè¼¸å…¥
            "lstm_use_prev_action": True,
            # LSTM ä½¿ç”¨å‰ä¸€çå‹µä½œç‚ºè¼¸å…¥
            "lstm_use_prev_reward": True,
            # Actor å’Œ Critic ä¸å…±äº«å±¤
            "vf_share_layers": False,
        })
        # ========== æ¢ç´¢é…ç½® ==========
        .exploration(
            # åˆå§‹æ¢ç´¢ç‡ï¼ˆé«˜æ¢ç´¢ï¼‰
            explore=True,
        )
        # ========== è©•ä¼°é…ç½® ==========
        .evaluation(
            # è©•ä¼°é–“éš”ï¼ˆæ¯ N æ¬¡è¨“ç·´è¿­ä»£è©•ä¼°ä¸€æ¬¡ï¼‰
            evaluation_interval=10,
            # è©•ä¼° episode æ•¸é‡
            evaluation_num_workers=1,
            evaluation_duration=10,
            evaluation_duration_unit="episodes",
        )
        # ========== èª¿è©¦é…ç½® ==========
        .debugging(
            # æ—¥èªŒç´šåˆ¥
            log_level="INFO",
        )
    )

    return config


def train(args):
    """åŸ·è¡Œ RLlib PPO è¨“ç·´

    æ­¤å‡½æ•¸æ˜¯è¨“ç·´çš„ä¸»è¦æµç¨‹ï¼ŒåŒ…å«ï¼š
        1. Ray åˆå§‹åŒ–
        2. ç’°å¢ƒå’Œæ¨¡å‹è¨»å†Š
        3. PPO é…ç½®å‰µå»ºæˆ–æª¢æŸ¥é»è¼‰å…¥ï¼ˆæ”¯æ´çºŒè¨“ï¼‰
        4. è¨“ç·´å¾ªç’°
        5. æª¢æŸ¥é»ä¿å­˜
        6. æœ€ä½³æ¨¡å‹è¿½è¹¤

    åƒæ•¸:
        args: å‘½ä»¤è¡Œåƒæ•¸å°è±¡
            åŒ…å«æ‰€æœ‰è¨“ç·´è¶…åƒæ•¸å’Œé…ç½®

    è¨“ç·´æµç¨‹:
        1. è¼‰å…¥é…ç½®æ–‡ä»¶
        2. åˆå§‹åŒ– Ray é›†ç¾¤
        3. è¨»å†Šç’°å¢ƒå’Œæ¨¡å‹
        4. å‰µå»ºæˆ–è¼‰å…¥ PPO Trainerï¼ˆé»˜èªçºŒè¨“ï¼‰
           - å¦‚æœ args.resume=True (é»˜èª)ï¼Œè‡ªå‹•å°‹æ‰¾æœ€æ–°æª¢æŸ¥é»
           - æª¢æŸ¥é»æœå°‹é †åº: final_model > best_model > æœ€æ–° checkpoint_*
           - å¯é€šé args.resume_from æŒ‡å®šç‰¹å®šæª¢æŸ¥é»
        5. è¨“ç·´å¾ªç’°ï¼ˆnum_iterations æ¬¡ï¼Œå¾ä¸Šæ¬¡ä¸­æ–·è™•ç¹¼çºŒï¼‰
        6. å®šæœŸä¿å­˜æª¢æŸ¥é»
        7. è¿½è¹¤æœ€ä½³æ¨¡å‹ï¼ˆåŒæ™‚ä¿å­˜ best_reward.txtï¼‰
        8. è¨“ç·´å®Œæˆå¾Œä¿å­˜æœ€çµ‚æ¨¡å‹

    è¼¸å‡º:
        - checkpoints/rl/: å®šæœŸæª¢æŸ¥é»
        - checkpoints/rl/best_model/: æœ€ä½³æ¨¡å‹
        - checkpoints/rl/best_reward.txt: æœ€ä½³å›å ±å€¼ï¼ˆç”¨æ–¼çºŒè¨“ï¼‰
        - checkpoints/rl/final_model/: æœ€çµ‚æ¨¡å‹
        - logs/rl/: TensorBoard æ—¥èªŒ

    ç›£æ§æŒ‡æ¨™:
        - episode_reward_mean: å¹³å‡ episode å›å ±
        - policy_loss: ç­–ç•¥æå¤±
        - vf_loss: åƒ¹å€¼å‡½æ•¸æå¤±
        - entropy: ç­–ç•¥ç†µï¼ˆæ¢ç´¢åº¦ï¼‰
        - kl: KL æ•£åº¦ï¼ˆç­–ç•¥è®ŠåŒ–å¹…åº¦ï¼‰

    çºŒè¨“åŠŸèƒ½:
        - é»˜èªé–‹å•Ÿ (args.resume=True)
        - ä½¿ç”¨ --no-resume ç¦ç”¨çºŒè¨“
        - ä½¿ç”¨ --resume-from <path> æŒ‡å®šæª¢æŸ¥é»
        - è‡ªå‹•è¼‰å…¥è¿­ä»£æ¬¡æ•¸å’Œæœ€ä½³å›å ±å€¼
    """
    # ========== æ­¥é©Ÿ 1: è¼‰å…¥é…ç½® ==========
    logger.info("========== RLlib PPO è¨“ç·´é–‹å§‹ ==========")
    logger.info(f"é…ç½®æ–‡ä»¶: {args.config}")

    # è¼‰å…¥ RL é…ç½®
    rl_config = load_config(args.config)
    env_config = rl_config.get("env_config", {})
    model_config = rl_config.get("model_config", {})

    # è¦†è“‹é…ç½®ï¼ˆå‘½ä»¤è¡Œåƒæ•¸å„ªå…ˆï¼‰
    if args.deeplob_checkpoint:
        env_config["deeplob_checkpoint"] = args.deeplob_checkpoint

    logger.info(f"ç’°å¢ƒé…ç½®: {env_config}")
    logger.info(f"æ¨¡å‹é…ç½®: {model_config}")

    # ========== æ­¥é©Ÿ 2: åˆå§‹åŒ– Ray ==========
    logger.info("åˆå§‹åŒ– Ray é›†ç¾¤...")

    ray.init(
        num_cpus=args.num_cpus,
        num_gpus=args.num_gpus,
        include_dashboard=True,
        dashboard_host="0.0.0.0",
        dashboard_port=8265,
        ignore_reinit_error=True,
    )

    logger.info(f"âœ“ Ray å·²åˆå§‹åŒ–")
    logger.info(f"  - CPUs: {args.num_cpus}")
    logger.info(f"  - GPUs: {args.num_gpus}")
    logger.info(f"  - Dashboard: http://localhost:8265")

    # ========== æ­¥é©Ÿ 3: è¨»å†Šç’°å¢ƒå’Œæ¨¡å‹ ==========
    register_environment()
    register_trading_lstm_model()

    # ========== æ­¥é©Ÿ 4: å‰µå»ºæˆ–è¼‰å…¥ PPO Trainer ==========
    checkpoint_dir = Path(args.checkpoint_dir)
    checkpoint_dir.mkdir(parents=True, exist_ok=True)

    # æª¢æŸ¥æ˜¯å¦æœ‰æª¢æŸ¥é»å¯ä»¥çºŒè¨“
    resume_checkpoint = None
    start_iteration = 1
    best_reward = float('-inf')

    if args.resume:
        # å„ªå…ˆä½¿ç”¨æŒ‡å®šçš„æª¢æŸ¥é»è·¯å¾‘
        if args.resume_from:
            resume_path = Path(args.resume_from)
            if resume_path.exists():
                resume_checkpoint = str(resume_path)
                logger.info(f"âœ“ æ‰¾åˆ°æŒ‡å®šæª¢æŸ¥é»: {resume_checkpoint}")
            else:
                logger.warning(f"âš ï¸ æŒ‡å®šæª¢æŸ¥é»ä¸å­˜åœ¨: {args.resume_from}")

        # å¦‚æœæ²’æœ‰æŒ‡å®šè·¯å¾‘ï¼Œè‡ªå‹•å°‹æ‰¾æœ€æ–°æª¢æŸ¥é»
        if resume_checkpoint is None:
            # æœå°‹é †åº: 1. final_model 2. best_model 3. æœ€æ–° checkpoint_*
            candidates = [
                checkpoint_dir / "final_model",
                checkpoint_dir / "best_model",
            ]

            # æ·»åŠ æ‰€æœ‰ checkpoint_* ç›®éŒ„ (æŒ‰æ™‚é–“æ’åº)
            checkpoint_pattern = list(checkpoint_dir.glob("checkpoint_*"))
            checkpoint_pattern.sort(key=lambda p: p.stat().st_mtime, reverse=True)
            candidates.extend(checkpoint_pattern)

            for candidate in candidates:
                if candidate.exists() and candidate.is_dir():
                    # é©—è­‰æª¢æŸ¥é»æ˜¯å¦æœ‰æ•ˆ (åŒ…å«å¿…è¦æ–‡ä»¶)
                    if (candidate / "algorithm_state.pkl").exists():
                        resume_checkpoint = str(candidate)
                        logger.info(f"âœ“ è‡ªå‹•æ‰¾åˆ°æª¢æŸ¥é»: {resume_checkpoint}")
                        break

            if resume_checkpoint is None:
                logger.info("â„¹ï¸ æœªæ‰¾åˆ°å¯ç”¨æª¢æŸ¥é»ï¼Œå¾é ­é–‹å§‹è¨“ç·´")

    # å‰µå»ºæˆ–è¼‰å…¥ Trainer
    if resume_checkpoint:
        logger.info(f"ğŸ“‚ å¾æª¢æŸ¥é»è¼‰å…¥ Trainer: {resume_checkpoint}")
        try:
            from ray.rllib.algorithms.ppo import PPO
            trainer = PPO.from_checkpoint(resume_checkpoint)

            # å˜—è©¦å¾æª¢æŸ¥é»åç¨±æå–è¿­ä»£æ¬¡æ•¸
            checkpoint_name = Path(resume_checkpoint).name
            if checkpoint_name.startswith("checkpoint_"):
                try:
                    start_iteration = int(checkpoint_name.split("_")[-1]) + 1
                    logger.info(f"âœ“ å¾è¿­ä»£ {start_iteration} ç¹¼çºŒè¨“ç·´")
                except ValueError:
                    logger.info("âœ“ Trainer å·²è¼‰å…¥ (ç„¡æ³•æå–è¿­ä»£æ¬¡æ•¸ï¼Œå¾ 1 é–‹å§‹è¨ˆæ•¸)")
            else:
                logger.info(f"âœ“ å¾ {checkpoint_name} ç¹¼çºŒè¨“ç·´")

            # è¼‰å…¥ best_reward (å¦‚æœæœ‰ä¿å­˜)
            best_reward_file = checkpoint_dir / "best_reward.txt"
            if best_reward_file.exists():
                try:
                    with open(best_reward_file, 'r') as f:
                        best_reward = float(f.read().strip())
                    logger.info(f"âœ“ è¼‰å…¥æœ€ä½³å›å ±: {best_reward:.2f}")
                except Exception as e:
                    logger.warning(f"âš ï¸ ç„¡æ³•è¼‰å…¥æœ€ä½³å›å ±: {e}")

        except Exception as e:
            logger.error(f"âŒ è¼‰å…¥æª¢æŸ¥é»å¤±æ•—: {e}")
            logger.info("å¾é ­é–‹å§‹è¨“ç·´...")
            ppo_config = create_ppo_config(args, env_config, model_config)
            trainer = ppo_config.build()
    else:
        logger.info("å‰µå»ºæ–°çš„ PPO Trainer...")
        ppo_config = create_ppo_config(args, env_config, model_config)
        trainer = ppo_config.build()
        logger.info("âœ“ PPO Trainer å·²å‰µå»º")

    # ========== æ­¥é©Ÿ 5: è¨“ç·´å¾ªç’° ==========
    logger.info(f"é–‹å§‹è¨“ç·´ {args.num_iterations} æ¬¡è¿­ä»£ (å¾ç¬¬ {start_iteration} æ¬¡é–‹å§‹)...")

    for iteration in range(start_iteration, start_iteration + args.num_iterations):
        # åŸ·è¡Œä¸€æ¬¡è¨“ç·´è¿­ä»£
        result = trainer.train()

        # ===== è¨˜éŒ„è¨“ç·´æŒ‡æ¨™ =====
        episode_reward_mean = result.get("episode_reward_mean", 0)
        episode_len_mean = result.get("episode_len_mean", 0)
        policy_loss = result.get("info", {}).get("learner", {}).get("default_policy", {}).get("learner_stats", {}).get("policy_loss", 0)

        logger.info(
            f"è¿­ä»£ {iteration}/{args.num_iterations} | "
            f"å›å ±: {episode_reward_mean:.2f} | "
            f"Episodeé•·åº¦: {episode_len_mean:.1f} | "
            f"ç­–ç•¥æå¤±: {policy_loss:.4f}"
        )

        # ===== å®šæœŸä¿å­˜æª¢æŸ¥é» =====
        if iteration % args.checkpoint_interval == 0:
            checkpoint_path = trainer.save(checkpoint_dir)
            logger.info(f"âœ“ å·²ä¿å­˜æª¢æŸ¥é»: {checkpoint_path}")

        # ===== è¿½è¹¤æœ€ä½³æ¨¡å‹ =====
        if episode_reward_mean > best_reward:
            best_reward = episode_reward_mean
            best_model_dir = checkpoint_dir / "best_model"
            best_model_dir.mkdir(parents=True, exist_ok=True)
            best_checkpoint = trainer.save(best_model_dir)
            logger.info(f"ğŸ† æ–°çš„æœ€ä½³æ¨¡å‹ï¼å›å ±: {best_reward:.2f}")
            logger.info(f"   ä¿å­˜è‡³: {best_checkpoint}")

            # ä¿å­˜æœ€ä½³å›å ±å€¼ (ç”¨æ–¼çºŒè¨“)
            best_reward_file = checkpoint_dir / "best_reward.txt"
            with open(best_reward_file, 'w') as f:
                f.write(f"{best_reward:.4f}")

    # ========== æ­¥é©Ÿ 6: è¨“ç·´å®Œæˆ ==========
    logger.info("========== è¨“ç·´å®Œæˆ ==========")
    logger.info(f"æœ€ä½³å›å ±: {best_reward:.2f}")
    logger.info(f"ç¸½è¨“ç·´è¿­ä»£: {iteration}")

    # ä¿å­˜æœ€çµ‚æ¨¡å‹
    final_checkpoint = trainer.save(checkpoint_dir / "final_model")
    logger.info(f"âœ“ æœ€çµ‚æ¨¡å‹å·²ä¿å­˜: {final_checkpoint}")

    # é—œé–‰ Trainer å’Œ Ray
    trainer.stop()
    ray.shutdown()

    logger.info("âœ“ Ray å·²é—œé–‰")


def main():
    """ä¸»å‡½æ•¸ - è§£æå‘½ä»¤è¡Œåƒæ•¸ä¸¦å•Ÿå‹•è¨“ç·´

    æ­¤å‡½æ•¸è² è²¬ï¼š
        1. è§£æå‘½ä»¤è¡Œåƒæ•¸
        2. è¨­å®šè¨“ç·´ç’°å¢ƒ
        3. èª¿ç”¨è¨“ç·´å‡½æ•¸

    å‘½ä»¤è¡Œåƒæ•¸:
        --config: é…ç½®æ–‡ä»¶è·¯å¾‘
        --deeplob-checkpoint: DeepLOB é è¨“ç·´æ¨¡å‹è·¯å¾‘
        --num-iterations: è¨“ç·´è¿­ä»£æ¬¡æ•¸
        --lr: å­¸ç¿’ç‡
        --gamma: æŠ˜æ‰£å› å­
        --train-batch-size: è¨“ç·´æ‰¹æ¬¡å¤§å°
        --num-workers: Rollout workers æ•¸é‡
        --num-gpus: GPU æ•¸é‡
        --checkpoint-dir: æª¢æŸ¥é»ä¿å­˜ç›®éŒ„
    """
    parser = argparse.ArgumentParser(
        description="RLlib PPO å¼·åŒ–å­¸ç¿’è¨“ç·´ - é«˜é »äº¤æ˜“ç­–ç•¥"
    )

    # ===== é…ç½®æ–‡ä»¶ =====
    parser.add_argument(
        "--config",
        type=str,
        default="configs/rl_config.yaml",
        help="RL é…ç½®æ–‡ä»¶è·¯å¾‘"
    )

    # ===== DeepLOB æ¨¡å‹ =====
    parser.add_argument(
        "--deeplob-checkpoint",
        type=str,
        default=None,
        help="DeepLOB é è¨“ç·´æ¨¡å‹æª¢æŸ¥é»è·¯å¾‘"
    )

    # ===== çºŒè¨“åƒæ•¸ =====
    parser.add_argument(
        "--resume",
        action="store_true",
        default=True,
        help="æ˜¯å¦çºŒè¨“ (é»˜èª: Trueï¼Œè‡ªå‹•å°‹æ‰¾æœ€æ–°æª¢æŸ¥é»)"
    )

    parser.add_argument(
        "--no-resume",
        action="store_false",
        dest="resume",
        help="ç¦ç”¨çºŒè¨“ï¼Œå¾é ­é–‹å§‹è¨“ç·´"
    )

    parser.add_argument(
        "--resume-from",
        type=str,
        default=None,
        help="æŒ‡å®šçºŒè¨“æª¢æŸ¥é»è·¯å¾‘ (é»˜èª: è‡ªå‹•å°‹æ‰¾æœ€æ–°)"
    )

    # ===== è¨“ç·´åƒæ•¸ =====
    parser.add_argument(
        "--num-iterations",
        type=int,
        default=1000,
        help="è¨“ç·´è¿­ä»£æ¬¡æ•¸"
    )

    parser.add_argument(
        "--lr",
        type=float,
        default=3e-4,
        help="å­¸ç¿’ç‡"
    )

    parser.add_argument(
        "--gamma",
        type=float,
        default=0.99,
        help="æŠ˜æ‰£å› å­ï¼ˆGammaï¼‰"
    )

    parser.add_argument(
        "--entropy-coeff",
        type=float,
        default=0.01,
        help="ç†µä¿‚æ•¸ï¼ˆé¼“å‹µæ¢ç´¢ï¼‰"
    )

    parser.add_argument(
        "--train-batch-size",
        type=int,
        default=4096,
        help="è¨“ç·´æ‰¹æ¬¡å¤§å°"
    )

    parser.add_argument(
        "--sgd-minibatch-size",
        type=int,
        default=512,
        help="SGD å°æ‰¹æ¬¡å¤§å°"
    )

    # ===== è³‡æºé…ç½® =====
    parser.add_argument(
        "--num-workers",
        type=int,
        default=8,
        help="Rollout workers æ•¸é‡"
    )

    parser.add_argument(
        "--num-gpus",
        type=int,
        default=1,
        help="GPU æ•¸é‡"
    )

    parser.add_argument(
        "--num-cpus",
        type=int,
        default=16,
        help="CPU æ•¸é‡"
    )

    # ===== æª¢æŸ¥é» =====
    parser.add_argument(
        "--checkpoint-dir",
        type=str,
        default="checkpoints/rl",
        help="æª¢æŸ¥é»ä¿å­˜ç›®éŒ„"
    )

    parser.add_argument(
        "--checkpoint-interval",
        type=int,
        default=50,
        help="æª¢æŸ¥é»ä¿å­˜é–“éš”ï¼ˆè¿­ä»£æ¬¡æ•¸ï¼‰"
    )

    # ===== è§£æåƒæ•¸ =====
    args = parser.parse_args()

    # ===== å•Ÿå‹•è¨“ç·´ =====
    try:
        train(args)
    except KeyboardInterrupt:
        logger.info("\nè¨“ç·´è¢«ç”¨æˆ¶ä¸­æ–·")
        ray.shutdown()
    except Exception as e:
        logger.error(f"è¨“ç·´éç¨‹ç™¼ç”ŸéŒ¯èª¤: {e}", exc_info=True)
        ray.shutdown()
        raise


if __name__ == "__main__":
    main()
