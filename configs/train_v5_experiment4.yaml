# ===================================================================
# DeepLOB V5 實驗 4：Warmup + Cosine Decay 穩定訓練
# 基於 ChatGPT 建議 + 實驗 1-3 經驗
# 日期：2025-10-24
# ===================================================================

# ==================== 運行配置 ====================
run:
  seed: 42

# ==================== 標籤配置 ====================
labels:
  num_classes: 3
  class_names: ["下跌", "持平", "上漲"]
  class_names_en: ["down", "stationary", "up"]
  class_ids: [0, 1, 2]
  format: "v5"

  weight_calculation:
    epsilon: 1.0e-6
    normalize: true

# ==================== 可視化配置 ====================
visualization:
  confusion_matrix:
    normalize: true
    dpi: 150
    figsize: [8, 6]
    cmap: "Blues"

# ==================== 數據配置 ====================
data:
  train: "data/processed_v7/npz/stock_embedding_train.npz"
  val: "data/processed_v7/npz/stock_embedding_val.npz"
  test: "data/processed_v7/npz/stock_embedding_test.npz"
  norm_meta: "data/processed_v7/npz/normalization_meta.json"

  v5_labels: true

  y_raw_validation:
    enabled: true
    expected_values: [-1, 0, 1]

  # 【調整】啟用樣本權重（溫和處理不平衡）
  use_sample_weights: true
  weights_normalize: "mean_to_1"

  use_stock_ids: false
  fail_on_missing_keys: true

# ==================== DataLoader 配置 ====================
dataloader:
  batch_size: 384  # 保持穩定
  num_workers: 12
  pin_memory: true

  # 【關鍵】關閉平衡採樣器（避免過擬合少數類）
  balance_sampler:
    enabled: false
    strategy: "inv_freq"

# ==================== 模型配置 ====================
model:
  arch: "DeepLOB"

  input:
    shape: [100, 20]

  num_classes: 3

  # DeepLOB 架構
  conv1_filters: 32
  conv2_filters: 32
  conv3_filters: 32

  # 【調整】回到標準容量（實驗 3 的 28 → 32）
  # 理由：實驗 3 欠擬合，需要更大容量
  lstm_hidden_size: 32
  fc_hidden_size: 32

  # 【調整】提高 Dropout（0.7 → 0.75）
  # 理由：ChatGPT 建議 +0.05~+0.10，採用保守 +0.05
  dropout: 0.75

  embeddings:
    enabled: false
    num_stocks: 367
    dim: 16
    dropout: 0.1
    weight_decay: 0.0001
    max_norm: null
    unk_strategy: "explicit_unk"
    feature_dropout:
      p: 0.3
    conditioning: "film"

# ==================== 損失函數配置 ====================
loss:
  type: "ce"

  # 【調整】啟用溫和類別權重（處理 30.86%/42.96%/26.19% 不平衡）
  class_weights: "auto"

  # 【新增】標籤平滑（ChatGPT 建議 0.02）
  label_smoothing:
    global: 0.02  # 溫和平滑，抑制過度自信
    flat_bonus: 0.00

# ==================== 優化器配置 ====================
optim:
  name: "adamw"

  # 【核心】實施 Warmup + Cosine Decay 策略（ChatGPT 方案 A）
  # 峰值學習率（warmup 結束時達到此值）
  lr: 0.000003  # 3e-6（峰值學習率）

  # 【調整】降低 Weight Decay（0.005 → 0.0001）
  # 理由：ChatGPT 建議 1e-4，配合 Dropout 提升
  weight_decay: 0.0001

  # 【調整】更嚴格梯度裁剪（0.6 → 2.0）
  # 理由：ChatGPT 建議 2.0，目前最高飆到 17.34
  grad_clip: 2.0

  amp: false
  use_bf16: false

  betas: [0.9, 0.999]
  eps: 1.0e-8
  amsgrad: false

# ==================== 學習率調度器（Warmup + Cosine Decay）====================
sched:
  name: "cosine"

  # 【核心配置】Warmup + Cosine Decay
  # Warmup 階段：Epoch 0-2（2 個 epoch = 2/12 = 0.167）
  #   - 起點：3e-6 × 0.333 = 1e-6
  #   - 終點：3e-6 × 1.0 = 3e-6
  # Cosine Decay 階段：Epoch 2-12
  #   - 起點：3e-6
  #   - 終點：8e-7
  warmup_ratio: 0.167  # 2/12 = 0.167（2 個 epoch warmup）
  warmup_start_factor: 0.333  # 起點是峰值的 1/3（1e-6 / 3e-6）
  eta_min: 0.0000008  # 8e-7（Cosine Decay 最終值）

# ==================== 訓練配置 ====================
train:
  # 【調整】縮短訓練期（20 → 12）
  # 理由：ChatGPT 建議 12，避免後期過擬合
  epochs: 12
  accumulate_steps: 1

  # 【調整】早停策略（更寬容）
  early_stop:
    metric: "val.f1_macro_weighted"
    patience: 3  # 降低耐心（5→3），Cosine Decay 下更快收斂
    mode: "max"
    min_delta: 0.0003  # 降低閾值（0.0005→0.0003）

# ==================== 評估配置 ====================
eval:
  group_split:
    by_stock: true

  metrics:
    unweighted: ["acc", "f1_macro", "per_class_pr_rc", "confusion"]
    weighted: ["f1_macro", "loss"]

  breakdowns: ["reasons", "t_hit_bucket", "ret_quantile", "stock"]
  thit_buckets: [0, 5, 10, 20, 100]
  ret_quantiles: [0.8, 0.9, 0.95]

# ==================== 校準配置 ====================
calibration:
  enabled: true
  type: "temperature"
  opt_metric: "weighted_nll"
  lr: 0.01
  max_iter: 50

# ==================== 推理配置 ====================
inference:
  decision: "argmax"
  cost_matrix:
    path: null
  hold_threshold: 0.0

# ==================== 日誌配置 ====================
logging:
  dir: "logs/deeplob_v5_exp4"
  save_best_by: "val.f1_macro_weighted"

  artifacts:
    - "model"
    - "norm_meta"
    - "label_mapping"
    - "calibration"
    - "cost_matrix"
    - "metrics_json"
    - "logs_csv"
    - "confmat_png"

  wandb:
    enabled: false
    project: "deeplob-v5-exp4"
    entity: null

# ==================== 安全檢查 ====================
safety_checks:
  validate_label_set: true
  validate_weights: true
  validate_norm_meta: true
  print_data_summary: true

# ==================== 硬體配置 ====================
hardware:
  device: "cuda"
  num_workers: 12
  pin_memory: true
  prefetch_factor: 4
  persistent_workers: true
  enable_tf32: true

# ==================== 續訓配置 ====================
resume:
  enabled: false
  checkpoint_path: "checkpoints/v5_exp4/deeplob_v5_best.pth"
  load_optimizer: true

# ==================== 其他配置 ====================
misc:
  save_last: true
  output_dir: "checkpoints/v5_exp4"

# ===================================================================
# 實驗 4 核心策略總結
# ===================================================================
#
# 【問題診斷】（基於訓練日誌）：
# 1. Epoch 5 後驗證集 Loss 發散（1.0411 → 1.1604）
# 2. 梯度爆炸（Epoch 6-10: 4.93 → 17.34）
# 3. Train-Val Gap 過大（Epoch 10: 21%）
# 4. 驗證準確率停滯（43-44%）
#
# 【核心改動】（對比實驗 3）：
#
# 1. **學習率策略**（最關鍵）：
#    - 舊：固定 5e-6 + Cosine（warmup_ratio=0.40）
#    - 新：Warmup + Cosine Decay
#      * Epoch 0-2: 1e-6 → 3e-6（warmup）
#      * Epoch 3-4: 3e-6（峰值）
#      * Epoch 5-12: 3e-6 → 8e-7（Cosine 衰減）
#    - 理由：避免 Epoch 6-10 的學習率過高導致發散
#
# 2. **梯度控制**：
#    - 舊：grad_clip = 0.6（不夠嚴格）
#    - 新：grad_clip = 2.0（ChatGPT 建議）
#    - 理由：控制梯度爆炸（最高 17.34 → 目標 <5.0）
#
# 3. **正則化調整**：
#    - Dropout: 0.7 → 0.75（+0.05，ChatGPT 建議）
#    - Weight Decay: 0.005 → 0.0001（降低，配合 Dropout 提升）
#    - 理由：輕度增強正則化，不過度抑制學習
#
# 4. **類別不平衡處理**：
#    - 舊：balance_sampler=true（過度擬合少數類）
#    - 新：class_weights="auto"（溫和處理）
#    - 理由：減少少數類過擬合，保持整體性能
#
# 5. **標籤平滑**：
#    - 舊：label_smoothing=0.0
#    - 新：label_smoothing=0.02（ChatGPT 建議）
#    - 理由：抑制過度自信，改善泛化
#
# 6. **訓練期限制**：
#    - 舊：20 epochs
#    - 新：12 epochs（ChatGPT 建議）
#    - 理由：避免後期過擬合（實驗 1 的 Epoch 7 後即過擬合）
#
# 7. **模型容量**：
#    - 舊：lstm/fc = 28（實驗 3 欠擬合）
#    - 新：lstm/fc = 32（回到標準）
#    - 理由：實驗 3 容量過小（Train Acc < Val Acc）
#
# 【預期目標】：
# - Train Acc: 50-55%（不過高）
# - Val Acc: 47-50%（提升 3-6%）
# - Train-Val Gap: 3-5%（大幅縮小）
# - Grad Norm: <5.0（穩定）
# - Val Loss: 持續下降或穩定（不發散）
# - Best Epoch: 8-10（後期仍保持性能）
#
# 【訓練指令】：
# conda activate deeplob-pro
# python scripts/train_deeplob_v5.py --config configs/train_v5_experiment4.yaml
#
# 【監控重點】：
# 1. Epoch 6-8 的梯度範數（應 <5.0，不再爆炸）
# 2. Val Loss 曲線（應持續下降或穩定，不發散）
# 3. Train-Val Gap（應 <5%，不過擬合）
# 4. Class 1 (持平) 召回率（應 >40%，不崩潰）
#
# ===================================================================
