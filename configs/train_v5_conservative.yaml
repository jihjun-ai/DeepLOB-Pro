# ===================================================================
# DeepLOB V5 實驗 5b：保守版（不增加容量）
# 核心理念：實驗 4b 不是容量問題，是訓練不足！
# 目標：從 51.14% → 55-58% (保守目標)
# 日期：2025-10-24
# ===================================================================

# ==================== 運行配置 ====================
run:
  seed: 42

# ==================== 標籤配置 ====================
labels:
  num_classes: 3
  class_names: ["下跌", "持平", "上漲"]
  class_names_en: ["down", "stationary", "up"]
  class_ids: [0, 1, 2]
  format: "v5"

  weight_calculation:
    epsilon: 1.0e-6
    normalize: true

# ==================== 可視化配置 ====================
visualization:
  confusion_matrix:
    normalize: true
    dpi: 150
    figsize: [8, 6]
    cmap: "Blues"

# ==================== 數據配置 ====================
data:
  train: "data/processed_v7/npz/stock_embedding_train.npz"
  val: "data/processed_v7/npz/stock_embedding_val.npz"
  test: "data/processed_v7/npz/stock_embedding_test.npz"
  norm_meta: "data/processed_v7/npz/normalization_meta.json"

  v5_labels: true

  y_raw_validation:
    enabled: true
    expected_values: [-1, 0, 1]

  # 【改動 1】啟用樣本權重（改善 Class 0/2）
  use_sample_weights: true
  weights_normalize: "mean_to_1"

  use_stock_ids: false
  fail_on_missing_keys: true

# ==================== DataLoader 配置 ====================
dataloader:
  batch_size: 384  # 保持不變
  num_workers: 12
  pin_memory: true

  balance_sampler:
    enabled: false
    strategy: "inv_freq"

# ==================== 模型配置 ====================
model:
  arch: "DeepLOB"

  input:
    shape: [100, 20]

  num_classes: 3

  # 【核心】容量保持不變（證明不是容量問題）
  conv1_filters: 32
  conv2_filters: 32
  conv3_filters: 32
  lstm_hidden_size: 32
  fc_hidden_size: 32

  # 【改動 2】降低 Dropout（0.75 → 0.70）
  dropout: 0.70  # 輕度降低，避免欠擬合

  embeddings:
    enabled: false
    num_stocks: 367
    dim: 16
    dropout: 0.1
    weight_decay: 0.0001
    max_norm: null
    unk_strategy: "explicit_unk"
    feature_dropout:
      p: 0.3
    conditioning: "film"

# ==================== 損失函數配置 ====================
loss:
  type: "ce"

  # 【改動 3】啟用類別權重（改善 Class 0/2）
  class_weights: "auto"

  # 標籤平滑保持輕度
  label_smoothing:
    global: 0.02
    flat_bonus: 0.00

# ==================== 優化器配置 ====================
optim:
  name: "adamw"

  # 【改動 4】提高學習率（2.5e-6 → 3.5e-6，+40%）
  lr: 0.0000035  # 3.5e-6

  # Weight Decay 保持適度
  weight_decay: 0.001

  # Grad Clip 保持
  grad_clip: 1.5

  amp: false
  use_bf16: false

  betas: [0.9, 0.999]
  eps: 1.0e-8
  amsgrad: false

# ==================== 學習率調度器 ====================
sched:
  name: "cosine"

  # 【改動 5】適度延長 Warmup（16.7% → 20%）
  warmup_ratio: 0.20  # 3/15 epochs
  warmup_start_factor: 0.286  # 1e-6 / 3.5e-6

  # eta_min 保持
  eta_min: 0.000001  # 1e-6

# ==================== 訓練配置 ====================
train:
  # 【改動 6】延長訓練期（10 → 15）
  epochs: 15  # 給模型更多時間
  accumulate_steps: 1

  # 【改動 7】大幅放寬早停（只防止嚴重惡化）
  early_stop:
    metric: "val.f1_macro_weighted"
    patience: 5  # 1 → 5（非常寬容，允許波動）
    mode: "max"
    min_delta: 0.0001  # 0.0005 → 0.0001（極低閾值）

# ==================== 評估配置 ====================
eval:
  group_split:
    by_stock: true

  metrics:
    unweighted: ["acc", "f1_macro", "per_class_pr_rc", "confusion"]
    weighted: ["f1_macro", "loss"]

  breakdowns: ["reasons", "t_hit_bucket", "ret_quantile", "stock"]
  thit_buckets: [0, 5, 10, 20, 100]
  ret_quantiles: [0.8, 0.9, 0.95]

# ==================== 校準配置 ====================
calibration:
  enabled: true
  type: "temperature"
  opt_metric: "weighted_nll"
  lr: 0.01
  max_iter: 50

# ==================== 推理配置 ====================
inference:
  decision: "argmax"
  cost_matrix:
    path: null
  hold_threshold: 0.0

# ==================== 日誌配置 ====================
logging:
  dir: "logs/deeplob_v5_exp5b"
  save_best_by: "val.f1_macro_weighted"

  artifacts:
    - "model"
    - "norm_meta"
    - "label_mapping"
    - "calibration"
    - "cost_matrix"
    - "metrics_json"
    - "logs_csv"
    - "confmat_png"

  wandb:
    enabled: false
    project: "deeplob-v5-exp5b"
    entity: null

# ==================== 安全檢查 ====================
safety_checks:
  validate_label_set: true
  validate_weights: true
  validate_norm_meta: true
  print_data_summary: true

# ==================== 硬體配置 ====================
hardware:
  device: "cuda"
  num_workers: 12
  pin_memory: true
  prefetch_factor: 4
  persistent_workers: true
  enable_tf32: true

# ==================== 續訓配置 ====================
resume:
  enabled: false
  checkpoint_path: "checkpoints/v5/deeplob_v5_best.pth"
  load_optimizer: true

# ==================== 其他配置 ====================
misc:
  save_last: true
  output_dir: "checkpoints/v5"

# ===================================================================
# 實驗 5b：保守版策略（不增加容量）
# ===================================================================
#
# 【核心洞察】：
# 實驗 4b 的問題不是"容量不足"，而是"訓練不足"！
#
# 證據：
# - Train Acc 只有 50.08%（訓練集上也沒學好）
# - Train-Val Gap 接近 0%（沒有過擬合）
# - 只訓練 3 個 epoch 就停止（太早）
# - 學習率 2.5e-6 很保守（可能過低）
#
# 【核心策略】（不增加容量，僅優化訓練）：
#
# 1. **延長訓練時間**
#    - Epochs: 10 → 15 (+50%)
#    - Patience: 1 → 2 (+100%)
#    理由：給模型更多時間學習
#
# 2. **提高學習率**
#    - LR: 2.5e-6 → 3.5e-6 (+40%)
#    理由：更快學習，突破 50% 瓶頸
#
# 3. **減弱正則化**
#    - Dropout: 0.75 → 0.70 (-7%)
#    理由：Train Acc 50% 表示欠擬合
#
# 4. **改善 Class 0/2**
#    - Class Weights: none → auto
#    - Sample Weights: false → true
#    理由：針對性改善召回率低的類別
#
# 5. **容量保持不變**
#    - Conv/LSTM/FC 保持 32
#    理由：證明不是容量問題
#
# 【預期目標】（保守估計）：
# - Val Acc: 54-57%（提升 4-7%）
# - Test Acc: 55-58%（提升 4-7%）
# - Train Acc: 55-60%（訓練集也要提升）
# - Class 0/2 Recall: >45%（改善 5%+）
# - Train-Val Gap: 3-5%（健康範圍）
# - Best Epoch: 5-10（中期達到最佳）
# - 訓練時間: 5-7 分鐘（比實驗 5 快）
#
# 【對比實驗 5（激進版）】：
# | 項目 | 實驗 5 (激進) | 實驗 5b (保守) | 差異 |
# |------|--------------|---------------|------|
# | Conv filters | 48 | **32** | 保持 |
# | LSTM/FC | 64 | **32** | 保持 |
# | Dropout | 0.65 | **0.70** | 更強 |
# | LR | 3e-6 | **3.5e-6** | 更高 |
# | Epochs | 20 | **15** | 更短 |
# | Batch Size | 512 | **384** | 更小 |
# | 參數量 | 600K | **250K** | 保持 |
# | 訓練時間 | 10-15 分鐘 | **5-7 分鐘** | 快 2x |
# | 預期 Test Acc | >60% | **55-58%** | 保守 |
#
# 【優勢】：
# - ✅ 訓練時間短（5-7 分鐘 vs 10-15 分鐘）
# - ✅ 顯存占用少（250K vs 600K 參數）
# - ✅ 風險低（不增加容量，避免過擬合）
# - ✅ 證明假設（51% 是否真的接近容量上限）
#
# 【訓練指令】：
# conda activate deeplob-pro
# python scripts/train_deeplob_v5.py --config configs/train_v5_conservative.yaml --data-dir data/processed_v7/npz
#
# 【監控重點】：
# 1. Train Acc 是否超過 55%（證明模型能學到更多）
# 2. Best Epoch 是否在 5-10（不要太晚）
# 3. Train-Val Gap 是否 3-5%（健康範圍）
# 4. Class 0/2 Recall 是否改善
#
# ===================================================================
