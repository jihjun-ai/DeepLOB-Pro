# ===================================================================
# DeepLOB V5 實驗 5c：完全關閉早停版
# 核心理念：讓模型充分學習，不受早停限制
# 目標：從 51.14% → >60%
# 日期：2025-10-24
# ===================================================================

# ==================== 運行配置 ====================
run:
  seed: 42

# ==================== 標籤配置 ====================
labels:
  num_classes: 3
  class_names: ["下跌", "持平", "上漲"]
  class_names_en: ["down", "stationary", "up"]
  class_ids: [0, 1, 2]
  format: "v5"

  weight_calculation:
    epsilon: 1.0e-6
    normalize: true

# ==================== 可視化配置 ====================
visualization:
  confusion_matrix:
    normalize: true
    dpi: 150
    figsize: [8, 6]
    cmap: "Blues"

# ==================== 數據配置 ====================
data:
  train: "data/processed_v7/npz/stock_embedding_train.npz"
  val: "data/processed_v7/npz/stock_embedding_val.npz"
  test: "data/processed_v7/npz/stock_embedding_test.npz"
  norm_meta: "data/processed_v7/npz/normalization_meta.json"

  v5_labels: true

  y_raw_validation:
    enabled: true
    expected_values: [-1, 0, 1]

  # 啟用樣本權重
  use_sample_weights: true
  weights_normalize: "mean_to_1"

  use_stock_ids: false
  fail_on_missing_keys: true

# ==================== DataLoader 配置 ====================
dataloader:
  batch_size: 512
  num_workers: 12
  pin_memory: true

  balance_sampler:
    enabled: false
    strategy: "inv_freq"

# ==================== 模型配置 ====================
model:
  arch: "DeepLOB"

  input:
    shape: [100, 20]

  num_classes: 3

  # 增大容量
  conv1_filters: 48
  conv2_filters: 48
  conv3_filters: 48
  lstm_hidden_size: 64
  fc_hidden_size: 64

  # 降低 Dropout
  dropout: 0.65

  embeddings:
    enabled: false
    num_stocks: 367
    dim: 16
    dropout: 0.1
    weight_decay: 0.0001
    max_norm: null
    unk_strategy: "explicit_unk"
    feature_dropout:
      p: 0.3
    conditioning: "film"

# ==================== 損失函數配置 ====================
loss:
  type: "ce"
  class_weights: "auto"
  label_smoothing:
    global: 0.01
    flat_bonus: 0.00

# ==================== 優化器配置 ====================
optim:
  name: "adamw"
  lr: 0.000003  # 3e-6
  weight_decay: 0.0005
  grad_clip: 2.0
  amp: false
  use_bf16: false
  betas: [0.9, 0.999]
  eps: 1.0e-8
  amsgrad: false

# ==================== 學習率調度器 ====================
sched:
  name: "cosine"
  warmup_ratio: 0.25
  warmup_start_factor: 0.333
  eta_min: 0.0000005

# ==================== 訓練配置 ====================
train:
  # 固定訓練 15 個 epoch（不受早停影響）
  epochs: 15
  accumulate_steps: 1

  # 【核心】關閉早停（Patience 設為 999，實際上不會觸發）
  early_stop:
    metric: "val.f1_macro_weighted"
    patience: 999  # 設為極大值，實際上關閉早停
    mode: "max"
    min_delta: 0.0  # 設為 0，任何改善都算

# ==================== 評估配置 ====================
eval:
  group_split:
    by_stock: true

  metrics:
    unweighted: ["acc", "f1_macro", "per_class_pr_rc", "confusion"]
    weighted: ["f1_macro", "loss"]

  breakdowns: ["reasons", "t_hit_bucket", "ret_quantile", "stock"]
  thit_buckets: [0, 5, 10, 20, 100]
  ret_quantiles: [0.8, 0.9, 0.95]

# ==================== 校準配置 ====================
calibration:
  enabled: true
  type: "temperature"
  opt_metric: "weighted_nll"
  lr: 0.01
  max_iter: 50

# ==================== 推理配置 ====================
inference:
  decision: "argmax"
  cost_matrix:
    path: null
  hold_threshold: 0.0

# ==================== 日誌配置 ====================
logging:
  dir: "logs/deeplob_v5_exp5c"
  save_best_by: "val.f1_macro_weighted"

  artifacts:
    - "model"
    - "norm_meta"
    - "label_mapping"
    - "calibration"
    - "cost_matrix"
    - "metrics_json"
    - "logs_csv"
    - "confmat_png"

  wandb:
    enabled: false
    project: "deeplob-v5-exp5c"
    entity: null

# ==================== 安全檢查 ====================
safety_checks:
  validate_label_set: true
  validate_weights: true
  validate_norm_meta: true
  print_data_summary: true

# ==================== 硬體配置 ====================
hardware:
  device: "cuda"
  num_workers: 12
  pin_memory: true
  prefetch_factor: 4
  persistent_workers: true
  enable_tf32: true

# ==================== 續訓配置 ====================
resume:
  enabled: false
  checkpoint_path: "checkpoints/v5/deeplob_v5_best.pth"
  load_optimizer: true

# ==================== 其他配置 ====================
misc:
  save_last: true
  output_dir: "checkpoints/v5"

# ===================================================================
# 實驗 5c：完全關閉早停版
# ===================================================================
#
# 【核心理念】：
# 早停不應該阻止正常學習，應該讓模型跑完所有 epoch
#
# 【早停策略】：
# - Patience = 999（極大值，實際上關閉）
# - min_delta = 0.0（任何改善都算）
# - 固定訓練 15 個 epoch
#
# 【為什麼 15 個 epoch？】：
# - 實驗 4b 只訓練 3 個 epoch（太短）
# - 容量增大後，需要更多時間收斂
# - 15 個 epoch 約 10 分鐘（可接受）
#
# 【監控策略】（人工判斷）：
# 1. 觀察 Train-Val Gap：
#    - <5%：健康，繼續訓練
#    - 5-10%：警告，可能過擬合
#    - >10%：嚴重過擬合，提前手動停止
#
# 2. 觀察 Val Loss 趨勢：
#    - 持續下降：健康
#    - 連續 5 個 epoch 上升：可能過擬合
#
# 3. 觀察 Grad Norm：
#    - <5.0：穩定
#    - >5.0：梯度爆炸，提前停止
#
# 【優勢】：
# - ✅ 不受早停限制，充分學習
# - ✅ 避免"正常波動"被誤判為"性能下降"
# - ✅ 可以觀察完整的學習曲線
# - ✅ 找到真正的最佳點
#
# 【劣勢】：
# - ⚠️ 可能訓練過久（浪費時間）
# - ⚠️ 可能過擬合（需要人工監控）
#
# 【訓練指令】：
# conda activate deeplob-pro
# python scripts/train_deeplob_v5.py --config configs/train_v5_no_earlystop.yaml --data-dir data/processed_v7/npz
#
# 【實時監控】：
# # 另開終端，即時查看
# tail -f logs/deeplob_v5_exp5c/*/train.log | grep "Epoch"
#
# # 如果發現過擬合，手動停止
# Ctrl+C
#
# ===================================================================
