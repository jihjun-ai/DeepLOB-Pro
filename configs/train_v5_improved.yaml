# ===================================================================
# DeepLOB V5 改進版配置 - LayerNorm + Attention Pooling
# 目標：突破 45-46% 訓練平台，達到 65% 準確率
# 改進：
#   1. 使用改進版 DeepLOB 架構（LayerNorm + Attention）
#   2. 增加模型容量（lstm_hidden: 64）
#   3. 調整 early stop patience（15→20）
# ===================================================================

# ==================== 基礎配置 ====================
run:
  seed: 42

labels:
  num_classes: 3
  class_names: ["下跌", "持平", "上漲"]
  class_names_en: ["down", "stationary", "up"]
  class_ids: [0, 1, 2]
  format: "v5"

  weight_calculation:
    epsilon: 1.0e-6
    normalize: true

visualization:
  confusion_matrix:
    normalize: true
    dpi: 150
    figsize: [8, 6]
    cmap: "Blues"

data:
  train: "data/processed_v5_balanced/npz/stock_embedding_train.npz"
  val: "data/processed_v5_balanced/npz/stock_embedding_val.npz"
  test: "data/processed_v5_balanced/npz/stock_embedding_test.npz"
  norm_meta: "data/processed_v5_balanced/npz/normalization_meta.json"

  v5_labels: true
  y_raw_validation:
    enabled: true
    expected_values: [-1, 0, 1]

  use_sample_weights: false  # ✅ 保持關閉（避免權重異常）
  weights_clip: [0.1, 10.0]  # ✨ 新增：未來啟用 sample_weights 時的安全裁剪
  weights_normalize: "mean_to_1"
  use_stock_ids: false
  fail_on_missing_keys: true

# ==================== DataLoader 配置 ====================
dataloader:
  batch_size: 512  # 大 batch 穩定梯度
  num_workers: 4
  pin_memory: true

  balance_sampler:
    enabled: false  # ✅ 避免與 class_weights 雙重加權
    strategy: "inv_freq"

# ==================== 模型配置 ====================
model:
  type: "deeplob_improved"  # ✨ 使用改進版 DeepLOB
  arch: "DeepLOB"

  input:
    shape: [100, 20]
  num_classes: 3

  # 🔧 CNN 配置
  conv1_filters: 32
  conv2_filters: 32
  conv3_filters: 32

  # ✨ 增加容量（原 48 → 64）
  lstm_hidden_size: 64
  fc_hidden_size: 64
  dropout: 0.6

  # ✨ 改進版特有配置
  use_layer_norm: true      # 啟用 LayerNorm（穩定訓練）
  use_attention: true       # 啟用 Attention Pooling（更好的時序建模）

  embeddings:
    enabled: false

# ==================== 損失函數配置 ====================
loss:
  type: "ce"
  class_weights: "auto"  # PyTorch 自動計算逆頻率權重

  label_smoothing:
    global: 0.15      # 防止過度自信
    flat_bonus: 0.0

# ==================== 優化器配置 ====================
optim:
  name: "adamw"
  lr: 0.00008        # 保守的學習率
  weight_decay: 0.0005  # L2 正則化
  grad_clip: 0.5     # 梯度裁剪
  amp: false
  use_bf16: false
  betas: [0.9, 0.999]
  eps: 1.0e-8
  amsgrad: false

# ==================== 學習率調度器 ====================
sched:
  name: "cosine"
  warmup_ratio: 0.15
  warmup_start_factor: 0.01  # ✨ 新增：warmup 起始學習率係數（0.01 = 1% of lr）
  eta_min: 5.0e-6

# ==================== 訓練配置 ====================
train:
  epochs: 50
  accumulate_steps: 1

  early_stop:
    metric: "val.f1_macro_unweighted"
    patience: 20     # ✨ 增加耐心（15→20）給模型更多探索時間
    mode: "max"

# ==================== 評估配置 ====================
eval:
  group_split:
    by_stock: true

  metrics:
    unweighted: ["acc", "f1_macro", "per_class_pr_rc", "confusion"]
    weighted: ["f1_macro", "loss"]

  breakdowns: ["reasons", "t_hit_bucket", "ret_quantile", "stock"]
  thit_buckets: [0, 5, 10, 20, 100]
  ret_quantiles: [0.8, 0.9, 0.95]

# ==================== 校準配置 ====================
calibration:
  enabled: true
  type: "temperature"
  opt_metric: "weighted_nll"
  lr: 0.01
  max_iter: 50

# ==================== 推理配置 ====================
inference:
  decision: "argmax"
  cost_matrix:
    path: null
  hold_threshold: 0.0

# ==================== 日誌配置 ====================
logging:
  dir: "logs/deeplob_v5_improved"  # ✨ 新的日誌目錄
  save_best_by: "val.f1_macro_unweighted"

  artifacts:
    - "model"
    - "norm_meta"
    - "label_mapping"
    - "calibration"
    - "cost_matrix"
    - "metrics_json"
    - "logs_csv"
    - "confmat_png"

  wandb:
    enabled: false

# ==================== 安全檢查 ====================
safety_checks:
  validate_label_set: true
  validate_weights: true
  validate_norm_meta: true
  print_data_summary: true

# ==================== 硬件配置 ====================
hardware:
  device: "cuda"
  num_workers: 4
  pin_memory: true
  prefetch_factor: 2
  persistent_workers: true

# ==================== 續訓配置 ====================
resume:
  enabled: false
  checkpoint_path: "checkpoints/v5_improved/deeplob_v5_best.pth"
  load_optimizer: true

# ==================== 其他配置 ====================
misc:
  save_last: true
  output_dir: "checkpoints/v5_improved"  # ✨ 新的檢查點目錄

# ===================================================================
# 改進總結（V5 Improved）
# ===================================================================
# 🎯 目標：突破 45-46% 訓練平台，達到 65% 準確率
#
# 核心改進：
# 1. ✨ 使用改進版 DeepLOB 架構
#    - LayerNorm: 穩定訓練、改善梯度流
#    - Attention Pooling: 智能選擇重要時間步（不再只用最後一步）
#
# 2. ✨ 增加模型容量
#    - lstm_hidden_size: 48 → 64
#    - fc_hidden_size: 48 → 64
#
# 3. ✨ 調整訓練策略
#    - early_stop.patience: 15 → 20（給模型更多探索時間）
#
# 4. ✅ 保持良好配置
#    - use_sample_weights: false（避免權重異常）
#    - class_weights: auto（PyTorch 自動平衡）
#    - label_smoothing: 0.15（防止過度自信）
#
# 預期效果：
#   - LayerNorm: +1-2% 穩定性改善
#   - Attention Pooling: +3-5% 時序建模改善
#   - 增加容量: +2-3% 表達能力改善
#   - 累計預期: 45-46% → 52-58%
#
# 使用方法：
#   conda activate deeplob-pro
#   python scripts/train_deeplob_v5.py \
#       --config configs/train_v5_improved.yaml \
#       --epochs 50
#
# 快速測試（10 epochs）：
#   python scripts/train_deeplob_v5.py \
#       --config configs/train_v5_improved.yaml \
#       --epochs 10
# ===================================================================
