# ===================================================================
# DeepLOB V5 æ”¹é€²ç‰ˆé…ç½® - LayerNorm + Attention Pooling
# ç›®æ¨™ï¼šçªç ´ 45-46% è¨“ç·´å¹³å°ï¼Œé”åˆ° 65% æº–ç¢ºç‡
# æ”¹é€²ï¼š
#   1. ä½¿ç”¨æ”¹é€²ç‰ˆ DeepLOB æ¶æ§‹ï¼ˆLayerNorm + Attentionï¼‰
#   2. å¢åŠ æ¨¡å‹å®¹é‡ï¼ˆlstm_hidden: 64ï¼‰
#   3. èª¿æ•´ early stop patienceï¼ˆ15â†’20ï¼‰
# ===================================================================

# ==================== åŸºç¤é…ç½® ====================
run:
  seed: 42

labels:
  num_classes: 3
  class_names: ["ä¸‹è·Œ", "æŒå¹³", "ä¸Šæ¼²"]
  class_names_en: ["down", "stationary", "up"]
  class_ids: [0, 1, 2]
  format: "v5"

  weight_calculation:
    epsilon: 1.0e-6
    normalize: true

visualization:
  confusion_matrix:
    normalize: true
    dpi: 150
    figsize: [8, 6]
    cmap: "Blues"

data:
  train: "data/processed_v5_balanced/npz/stock_embedding_train.npz"
  val: "data/processed_v5_balanced/npz/stock_embedding_val.npz"
  test: "data/processed_v5_balanced/npz/stock_embedding_test.npz"
  norm_meta: "data/processed_v5_balanced/npz/normalization_meta.json"

  v5_labels: true
  y_raw_validation:
    enabled: true
    expected_values: [-1, 0, 1]

  use_sample_weights: false  # âœ… ä¿æŒé—œé–‰ï¼ˆé¿å…æ¬Šé‡ç•°å¸¸ï¼‰
  weights_clip: [0.1, 10.0]  # âœ¨ æ–°å¢ï¼šæœªä¾†å•Ÿç”¨ sample_weights æ™‚çš„å®‰å…¨è£å‰ª
  weights_normalize: "mean_to_1"
  use_stock_ids: false
  fail_on_missing_keys: true

# ==================== DataLoader é…ç½® ====================
dataloader:
  batch_size: 512  # å¤§ batch ç©©å®šæ¢¯åº¦
  num_workers: 4
  pin_memory: true

  balance_sampler:
    enabled: false  # âœ… é¿å…èˆ‡ class_weights é›™é‡åŠ æ¬Š
    strategy: "inv_freq"

# ==================== æ¨¡å‹é…ç½® ====================
model:
  type: "deeplob_improved"  # âœ¨ ä½¿ç”¨æ”¹é€²ç‰ˆ DeepLOB
  arch: "DeepLOB"

  input:
    shape: [100, 20]
  num_classes: 3

  # ğŸ”§ CNN é…ç½®
  conv1_filters: 32
  conv2_filters: 32
  conv3_filters: 32

  # âœ¨ å¢åŠ å®¹é‡ï¼ˆåŸ 48 â†’ 64ï¼‰
  lstm_hidden_size: 64
  fc_hidden_size: 64
  dropout: 0.6

  # âœ¨ æ”¹é€²ç‰ˆç‰¹æœ‰é…ç½®
  use_layer_norm: true      # å•Ÿç”¨ LayerNormï¼ˆç©©å®šè¨“ç·´ï¼‰
  use_attention: true       # å•Ÿç”¨ Attention Poolingï¼ˆæ›´å¥½çš„æ™‚åºå»ºæ¨¡ï¼‰

  embeddings:
    enabled: false

# ==================== æå¤±å‡½æ•¸é…ç½® ====================
loss:
  type: "ce"
  class_weights: "auto"  # PyTorch è‡ªå‹•è¨ˆç®—é€†é »ç‡æ¬Šé‡

  label_smoothing:
    global: 0.15      # é˜²æ­¢éåº¦è‡ªä¿¡
    flat_bonus: 0.0

# ==================== å„ªåŒ–å™¨é…ç½® ====================
optim:
  name: "adamw"
  lr: 0.00008        # ä¿å®ˆçš„å­¸ç¿’ç‡
  weight_decay: 0.0005  # L2 æ­£å‰‡åŒ–
  grad_clip: 0.5     # æ¢¯åº¦è£å‰ª
  amp: false
  use_bf16: false
  betas: [0.9, 0.999]
  eps: 1.0e-8
  amsgrad: false

# ==================== å­¸ç¿’ç‡èª¿åº¦å™¨ ====================
sched:
  name: "cosine"
  warmup_ratio: 0.15
  warmup_start_factor: 0.01  # âœ¨ æ–°å¢ï¼šwarmup èµ·å§‹å­¸ç¿’ç‡ä¿‚æ•¸ï¼ˆ0.01 = 1% of lrï¼‰
  eta_min: 5.0e-6

# ==================== è¨“ç·´é…ç½® ====================
train:
  epochs: 50
  accumulate_steps: 1

  early_stop:
    metric: "val.f1_macro_unweighted"
    patience: 20     # âœ¨ å¢åŠ è€å¿ƒï¼ˆ15â†’20ï¼‰çµ¦æ¨¡å‹æ›´å¤šæ¢ç´¢æ™‚é–“
    mode: "max"

# ==================== è©•ä¼°é…ç½® ====================
eval:
  group_split:
    by_stock: true

  metrics:
    unweighted: ["acc", "f1_macro", "per_class_pr_rc", "confusion"]
    weighted: ["f1_macro", "loss"]

  breakdowns: ["reasons", "t_hit_bucket", "ret_quantile", "stock"]
  thit_buckets: [0, 5, 10, 20, 100]
  ret_quantiles: [0.8, 0.9, 0.95]

# ==================== æ ¡æº–é…ç½® ====================
calibration:
  enabled: true
  type: "temperature"
  opt_metric: "weighted_nll"
  lr: 0.01
  max_iter: 50

# ==================== æ¨ç†é…ç½® ====================
inference:
  decision: "argmax"
  cost_matrix:
    path: null
  hold_threshold: 0.0

# ==================== æ—¥èªŒé…ç½® ====================
logging:
  dir: "logs/deeplob_v5_improved"  # âœ¨ æ–°çš„æ—¥èªŒç›®éŒ„
  save_best_by: "val.f1_macro_unweighted"

  artifacts:
    - "model"
    - "norm_meta"
    - "label_mapping"
    - "calibration"
    - "cost_matrix"
    - "metrics_json"
    - "logs_csv"
    - "confmat_png"

  wandb:
    enabled: false

# ==================== å®‰å…¨æª¢æŸ¥ ====================
safety_checks:
  validate_label_set: true
  validate_weights: true
  validate_norm_meta: true
  print_data_summary: true

# ==================== ç¡¬ä»¶é…ç½® ====================
hardware:
  device: "cuda"
  num_workers: 4
  pin_memory: true
  prefetch_factor: 2
  persistent_workers: true

# ==================== çºŒè¨“é…ç½® ====================
resume:
  enabled: false
  checkpoint_path: "checkpoints/v5_improved/deeplob_v5_best.pth"
  load_optimizer: true

# ==================== å…¶ä»–é…ç½® ====================
misc:
  save_last: true
  output_dir: "checkpoints/v5_improved"  # âœ¨ æ–°çš„æª¢æŸ¥é»ç›®éŒ„

# ===================================================================
# æ”¹é€²ç¸½çµï¼ˆV5 Improvedï¼‰
# ===================================================================
# ğŸ¯ ç›®æ¨™ï¼šçªç ´ 45-46% è¨“ç·´å¹³å°ï¼Œé”åˆ° 65% æº–ç¢ºç‡
#
# æ ¸å¿ƒæ”¹é€²ï¼š
# 1. âœ¨ ä½¿ç”¨æ”¹é€²ç‰ˆ DeepLOB æ¶æ§‹
#    - LayerNorm: ç©©å®šè¨“ç·´ã€æ”¹å–„æ¢¯åº¦æµ
#    - Attention Pooling: æ™ºèƒ½é¸æ“‡é‡è¦æ™‚é–“æ­¥ï¼ˆä¸å†åªç”¨æœ€å¾Œä¸€æ­¥ï¼‰
#
# 2. âœ¨ å¢åŠ æ¨¡å‹å®¹é‡
#    - lstm_hidden_size: 48 â†’ 64
#    - fc_hidden_size: 48 â†’ 64
#
# 3. âœ¨ èª¿æ•´è¨“ç·´ç­–ç•¥
#    - early_stop.patience: 15 â†’ 20ï¼ˆçµ¦æ¨¡å‹æ›´å¤šæ¢ç´¢æ™‚é–“ï¼‰
#
# 4. âœ… ä¿æŒè‰¯å¥½é…ç½®
#    - use_sample_weights: falseï¼ˆé¿å…æ¬Šé‡ç•°å¸¸ï¼‰
#    - class_weights: autoï¼ˆPyTorch è‡ªå‹•å¹³è¡¡ï¼‰
#    - label_smoothing: 0.15ï¼ˆé˜²æ­¢éåº¦è‡ªä¿¡ï¼‰
#
# é æœŸæ•ˆæœï¼š
#   - LayerNorm: +1-2% ç©©å®šæ€§æ”¹å–„
#   - Attention Pooling: +3-5% æ™‚åºå»ºæ¨¡æ”¹å–„
#   - å¢åŠ å®¹é‡: +2-3% è¡¨é”èƒ½åŠ›æ”¹å–„
#   - ç´¯è¨ˆé æœŸ: 45-46% â†’ 52-58%
#
# ä½¿ç”¨æ–¹æ³•ï¼š
#   conda activate deeplob-pro
#   python scripts/train_deeplob_v5.py \
#       --config configs/train_v5_improved.yaml \
#       --epochs 50
#
# å¿«é€Ÿæ¸¬è©¦ï¼ˆ10 epochsï¼‰ï¼š
#   python scripts/train_deeplob_v5.py \
#       --config configs/train_v5_improved.yaml \
#       --epochs 10
# ===================================================================
