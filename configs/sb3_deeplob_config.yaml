# Stable-Baselines3 PPO + DeepLOB 完整訓練配置
#
# 此配置文件用於 train_sb3_deeplob.py 訓練腳本
# 所有參數統一由 YAML 管理，無硬編碼
#
# 作者: SB3-DeepLOB 專案團隊
# 日期: 2025-10-26
# 版本: v3.0 (Long Only 策略)

# ===== 專案資訊 =====
project:
  name: "SB3-DeepLOB"
  version: "3.0"
  description: "PPO + DeepLOB 雙層學習架構 - 台股高頻交易 (Long Only 策略)"
  author: "SB3-DeepLOB Team"
  strategy: "Long Only"  # 只做多，不做空 (PPO_6 起)

# ===== 環境配置 =====
env_config:
  # 數據路徑
  data_dir: "data/processed_v7/npz"

  # DeepLOB 檢查點（預設路徑，可被命令行參數覆蓋）
  deeplob_checkpoint: "checkpoints/v5/deeplob_v5_best.pth"

  # Episode 配置
  max_steps: 500
  initial_balance: 10000.0

  # 交易參數 (Long Only)
  max_position: 1               # 最大持倉 [0, 1] | PPO_6: 改為只做多 (不允許負倉位)

  # ===== 台股交易成本配置 (2025 最新) =====
  # 重要：台股報價為"每股價格"，交易單位為"張"（1 張 = 1000 股）
  #       計算成本時：交易價值 = 報價 × 1000 × 持倉張數
  transaction_cost:
    # 台股交易單位
    shares_per_lot: 1000        # 每張 = 1000 股（固定）

    # 券商手續費（買賣雙邊收取，折扣相同）
    commission:
      base_rate: 0.001425       # 法定上限 0.1425%
      discount: 0.3             # 折扣 (0.1-1.0)
                                # 1.0 = 無折扣 (0.1425%)
                                # 0.6 = 6折 (0.0855%) ← 電子券商
                                # 0.3 = 3折 (0.04275%) ← 量化交易
      min_fee: 20.0             # 單筆最低手續費（台幣）

                                # 計算範例：
                                # 報價 100 元/股，買 1 張
                                # 交易價值 = 100 × 1000 = 100,000 元
                                # 手續費 = 100,000 × 0.04275% = 42.75 元

    # 證券交易稅（僅賣出收取）
    securities_tax:
      rate: 0.0015              # 稅率 0.15%（當沖減半後，延長至 2027/12/31）
                                # 一般賣出: 0.003 (0.3%)
                                # 當沖賣出: 0.0015 (0.15%) ← 推薦使用

    # 滑點成本（可選）
    slippage:
      enabled: false            # 是否啟用
      rate: 0.0001              # 滑點率 0.01%

  # 獎勵塑形 (PPO_6 簡化版)
  reward_config:
    pnl_scale: 1.0              # PnL 權重（核心獎勵）
    cost_penalty: 1.0           # 交易成本懲罰
    inventory_penalty: 0.0      # ❌ 已移除 | PPO_6: 不懲罰長時間持倉
    risk_penalty: 0.0           # ❌ 已移除 | PPO_6: 簡化獎勵函數

  # 數據模式
  data_mode: "train"            # train/val/test

# ===== PPO 超參數 =====
ppo:
  # 學習率
  learning_rate: 0.0001         # 1e-4 (降低) | PPO_5: 3e-4 (KL散度過高0.129，訓練不穩定)

  # 折扣因子
  gamma: 0.99                   # 長期收益權重
  gae_lambda: 0.95              # GAE 優勢估計

  # PPO 特有參數
  clip_range: 0.1               # PPO clip 參數 (降低) | PPO_5: 0.2 (策略更新步長過大)
  ent_coef: 0.01                # Entropy 係數（探索）
  vf_coef: 0.7                  # Value function 係數 (提高) | PPO_5: 0.5 (解釋方差-0.524，價值函數擬合不佳)
  max_grad_norm: 0.5            # 梯度裁剪

  # 訓練配置
  n_steps: 2048                 # Rollout buffer size (每次收集步數)
  batch_size: 64                # Mini-batch size
  n_epochs: 10                  # 每次更新的 epoch 數

  # 網絡架構
  net_arch:
    pi: [512, 256]              # Actor 網絡 (策略) | PPO_5: [256,128] (解釋方差低，需增加容量)
    vf: [512, 256]              # Critic 網絡 (價值函數) | PPO_5: [256,128] (解釋方差-0.524，擬合不佳)

  # 激活函數
  activation_fn: "ReLU"         # ReLU / Tanh

  # 其他
  verbose: 1                    # 日誌詳細程度 (0/1/2)
  seed: 42                      # 隨機種子

# ===== DeepLOB 特徵提取器配置 =====
deeplob_extractor:
  # 是否使用 DeepLOB 特徵提取器
  use_deeplob: true

  # 特徵維度
  features_dim: 128             # 提取器輸出維度

  # 是否使用 LSTM 隱藏層特徵
  use_lstm_hidden: false        # false=使用預測概率, true=使用LSTM hidden

  # 是否凍結 DeepLOB 權重
  freeze_deeplob: true          # 凍結 DeepLOB（不訓練）

  # MLP 提取器網絡架構
  extractor_net_arch: [512, 256] # PPO_5: [256,128] (配合 net_arch 增加容量)

# ===== 訓練配置 =====
training:
  # 訓練步數
  total_timesteps: 200000       # 200K steps (調參階段) | PPO_5: 500K (調參OK後再增加至1M)

  # 續訓配置
  resume:
    enabled: false              # 是否啟用續訓模式
    checkpoint_path: null       # 續訓的 PPO 模型路徑（.zip 文件）
                                # 範例: "checkpoints/sb3/ppo_deeplob/ppo_model_500000_steps.zip"
    reset_timesteps: false      # 是否重置時間步數計數器
                                # false: 繼續累加步數（推薦）
                                # true: 從 0 開始重新計數

  # 日誌配置
  log_interval: 10              # 每 N 次更新記錄一次
  tensorboard_log: "logs/sb3_deeplob"

  # 檢查點配置
  checkpoint_dir: "checkpoints/sb3/ppo_deeplob"
  final_model_name: "ppo_deeplob_final"

  # 進度條
  progress_bar: true

# ===== 設備配置 =====
device:
  # 預設設備（可被命令行參數覆蓋）
  default: "cuda"               # cuda / cpu

  # 自動回退到 CPU
  auto_fallback: true           # CUDA 不可用時自動使用 CPU

# ===== 向量化環境配置 =====
vec_env:
  # 預設環境數量（可被命令行參數覆蓋）
  n_envs: 1                     # 並行環境數（1=不並行）

  # 向量化類型（可被命令行參數覆蓋）
  vec_type: "dummy"             # dummy / subproc

# ===== 評估配置 =====
evaluation:
  # 評估頻率
  eval_freq: 10000              # 每 10K steps 評估一次
  n_eval_episodes: 10           # 評估 episode 數

  # 評估環境配置（使用驗證集）
  eval_env_config:
    data_mode: "val"
    max_steps: 500

  # 最佳模型選擇
  deterministic: true           # 評估時使用確定性策略
  render: false                 # 是否渲染

# ===== 回調配置 =====
callbacks:
  # Checkpoint Callback
  checkpoint:
    enabled: true
    save_freq: 50000            # 每 50K steps 保存一次
    save_path: "checkpoints/sb3/ppo_deeplob"
    name_prefix: "ppo_model"
    save_replay_buffer: false
    save_vecnormalize: false

  # Eval Callback
  eval:
    enabled: true
    eval_freq: 10000            # 每 10K steps 評估一次
    n_eval_episodes: 10
    best_model_save_path: "checkpoints/sb3/ppo_deeplob"
    log_path: "logs/sb3_eval"
    deterministic: true

  # TensorBoard
  tensorboard:
    enabled: true
    log_dir: "logs/sb3_deeplob"

# ===== 測試模式配置 =====
test_mode:
  # 快速測試配置（用於驗證流程）
  enabled: false                # 由命令行 --test 參數控制

  total_timesteps: 10000        # 10K steps
  save_freq: 5000
  eval_freq: 5000
  n_eval_episodes: 3
  n_steps: 512
  batch_size: 32

# ===== 驗證配置 =====
validation:
  # DeepLOB 檢查點驗證
  verify_checkpoint: true       # 是否驗證檢查點存在

  # 日誌級別
  log_level: "INFO"             # DEBUG / INFO / WARNING / ERROR

# ===== 輸出配置 =====
output:
  # 訓練完成後的提示信息
  show_next_steps: true

  # 保存路徑
  final_model_path: "checkpoints/sb3/ppo_deeplob/ppo_deeplob_final"
  best_model_path: "checkpoints/sb3/ppo_deeplob/best_model"

# ===== 高級配置 =====
advanced:
  # 混合精度訓練（需要 GPU）
  use_mixed_precision: false    # 實驗性功能

  # 梯度累積
  gradient_accumulation_steps: 1

  # 學習率調度
  lr_schedule: "constant"       # constant / linear / exponential

  # 提前停止
  early_stopping:
    enabled: false
    patience: 10
    min_delta: 0.01

# ===== 註釋說明 =====
#
# 使用範例：
#
# 1. 完整訓練（1M steps）：
#    python scripts/train_sb3_deeplob.py --config configs/sb3_deeplob_config.yaml
#
# 2. 快速測試（10K steps）：
#    python scripts/train_sb3_deeplob.py --config configs/sb3_deeplob_config.yaml --test
#
# 3. 指定 DeepLOB 檢查點：
#    python scripts/train_sb3_deeplob.py \
#        --config configs/sb3_deeplob_config.yaml \
#        --deeplob-checkpoint checkpoints/v5/deeplob_v5_best.pth
#
# 4. 高性能訓練（大 batch + 並行環境）：
#    python scripts/train_sb3_deeplob.py \
#        --config configs/sb3_deeplob_config.yaml \
#        --n-envs 4 \
#        --device cuda
#
# 5. 續訓模式（命令行指定）：
#    python scripts/train_sb3_deeplob.py \
#        --config configs/sb3_deeplob_config.yaml \
#        --resume checkpoints/sb3/ppo_deeplob/ppo_model_500000_steps.zip
#
# 6. 續訓模式（YAML 配置，修改 training.resume 部分）：
#    training:
#      resume:
#        enabled: true
#        checkpoint_path: "checkpoints/sb3/ppo_deeplob/ppo_model_500000_steps.zip"
#        reset_timesteps: false
#    然後運行: python scripts/train_sb3_deeplob.py --config configs/sb3_deeplob_config.yaml
#
# 7. 監控訓練：
#    tensorboard --logdir logs/sb3_deeplob/
#
# 關鍵參數調整建議：
#
# 1. 學習率 (learning_rate):
#    - 預設: 3e-4
#    - 建議範圍: 1e-4 ~ 1e-3
#    - 調整: 訓練不穩定時降低，收斂太慢時提高
#
# 2. Gamma (折扣因子):
#    - 預設: 0.99
#    - 建議範圍: 0.95 ~ 0.995
#    - 調整: 高頻交易可降低（更重視短期收益）
#
# 3. Entropy 係數 (ent_coef):
#    - 預設: 0.01
#    - 建議範圍: 0.001 ~ 0.05
#    - 調整: 探索不足時提高，過度探索時降低
#
# 4. N Steps (rollout buffer):
#    - 預設: 2048
#    - 建議範圍: 1024 ~ 4096
#    - 調整: 更大值更穩定但慢，更小值更快但不穩定
#
# 5. Batch Size:
#    - 預設: 64
#    - 建議範圍: 32 ~ 256
#    - 調整: 根據 GPU 顯存調整，越大越穩定
#
# GPU 利用率優化：
#    - 增加 batch_size (64 -> 128 -> 256)
#    - 增加 n_envs (1 -> 4 -> 8)
#    - 增加 n_steps (2048 -> 4096)
#
# RTX 5090 (32GB VRAM) 建議配置：
#    - batch_size: 256
#    - n_envs: 8
#    - n_steps: 4096
#    - 預期 GPU 利用率: 80-95%
