# ===================================================================
# DeepLOB V5 實驗 5：基於 ChatGPT 建議的保守微調 ⭐⭐⭐⭐⭐
# 目標：突破 50% 天花板 (batch 160 關鍵改進)
# 核心策略：Batch 160 + 微調學習率 (7.3e-7) + 微放寬正則
# 日期：2025-10-25
# V5 實驗 4 結果: Val Acc 50.24% (Epoch 9), F1 0.4929
# 問題：卡在 50% 天花板, Ep9 後過擬合 (梯度 4.14→6.65)
# ChatGPT 建議：batch 160 (關鍵) + lr 7.5e-7 + 微放寬正則
# 本實驗：採用更保守 lr 7.3e-7 (避免 Exp-3 過擬合)
# 預期: Val Acc 50.4-50.6%, 梯度 < 4.0, Gap < 3%
# ===================================================================

# ==================== 運行配置 ====================
run:
  seed: 42

# ==================== 標籤配置 ====================
labels:
  num_classes: 3
  class_names: ["下跌", "持平", "上漲"]
  class_names_en: ["down", "stationary", "up"]
  class_ids: [0, 1, 2]
  format: "v5"

  weight_calculation:
    epsilon: 1.0e-6
    normalize: true

# ==================== 可視化配置 ====================
visualization:
  confusion_matrix:
    normalize: true
    dpi: 150
    figsize: [8, 6]
    cmap: "Blues"

# ==================== 數據配置 ====================
data:
  train: "data/processed_v7/npz/stock_embedding_train.npz"
  val: "data/processed_v7/npz/stock_embedding_val.npz"
  test: "data/processed_v7/npz/stock_embedding_test.npz"
  norm_meta: "data/processed_v7/npz/normalization_meta.json"

  v5_labels: true

  y_raw_validation:
    enabled: true
    expected_values: [-1, 0, 1]

  # V7 實驗 1: 保持關閉樣本權重
  use_sample_weights: false
  weights_normalize: "mean_to_1"

  use_stock_ids: false
  fail_on_missing_keys: true

# ==================== DataLoader 配置 ====================
dataloader:
  # V5 實驗 5: ChatGPT 建議關鍵改進 ⭐⭐⭐⭐⭐
  batch_size: 160  # 實驗4: 128→160 (+25%, 降低梯度噪音, ChatGPT核心建議)
  num_workers: 12
  pin_memory: true

  # 關閉平衡採樣器
  balance_sampler:
    enabled: false
    strategy: "inv_freq"

# ==================== 模型配置 ====================
model:
  arch: "DeepLOB"

  input:
    shape: [100, 20]

  num_classes: 3

  # V7 實驗 1: 減小模型容量 (降低過擬合風險)
  conv1_filters: 32  # 48 → 32 (降回基準)
  conv2_filters: 32  # 48 → 32
  conv3_filters: 32  # 48 → 32

  # V7 實驗 1: 減小 LSTM/FC 容量
  lstm_hidden_size: 48  # 64 → 48 (介於 32-64)
  fc_hidden_size: 48    # 64 → 48

  # V5 實驗 5: 微放寬 Dropout (配合 LR 微升) ⭐
  dropout: 0.78  # 實驗4: 0.80→0.78 (-2.5%, ChatGPT建議, 配合batch 160)

  embeddings:
    enabled: false
    num_stocks: 367
    dim: 16
    dropout: 0.1
    weight_decay: 0.0001
    max_norm: null
    unk_strategy: "explicit_unk"
    feature_dropout:
      p: 0.3
    conditioning: "film"

# ==================== 損失函數配置 ====================
loss:
  type: "ce"

  # V7 實驗 1: 保持關閉類別權重
  class_weights: none

  # V5 實驗 5: 微調標籤平滑 (配合正則微放寬)
  label_smoothing:
    global: 0.028  # 實驗4: 0.03→0.028 (-7%, ChatGPT建議)
    flat_bonus: 0.00

# ==================== 優化器配置 ====================
optim:
  name: "adamw"

  # V5 實驗 5: 保守微調學習率 ⭐⭐⭐⭐ 核心修改
  # ChatGPT建議 7.5e-7, 本實驗更保守 7.3e-7 (避免Exp-3過擬合)
  lr: 0.00000073  # 實驗4: 7e-7→7.3e-7 (+4%, 配合batch 160加速學習)

  # V5 實驗 5: 微放寬 Weight Decay (配合 LR 微升) ⭐⭐
  weight_decay: 0.0029  # 實驗4: 0.003→0.0029 (-3%, 平衡正則化)

  # V5 實驗 5: 微調梯度裁剪 (配合 batch 160) ⭐
  grad_clip: 1.6  # 實驗4: 1.5→1.6 (+7%, ChatGPT建議, batch變大需放寬)

  amp: false
  use_bf16: false

  betas: [0.9, 0.999]
  eps: 1.0e-8
  amsgrad: false

# ==================== 學習率調度器 ====================
sched:
  name: "cosine"

  # V5 實驗 5: 微調 Warmup 策略 ⭐
  warmup_ratio: 0.38  # 實驗4: 0.40→0.38 (-5%, ChatGPT建議)
  warmup_start_factor: 0.16  # 實驗4: 0.15→0.16 (+7%, 改善LR顯示, ChatGPT建議)

  # V5 實驗 5: 微調最小學習率 (配合 LR 提升) ⭐
  eta_min: 0.00000021  # 實驗4: 2e-7→2.1e-7 (+5%, 保持比例)

# ==================== 訓練配置 ====================
train:
  # V5 實驗 5: 延長訓練期 (慢學習需更長時間) ⭐
  epochs: 22  # 實驗4: 20→22 (預期最佳點延後到Ep10-12, ChatGPT建議)
  accumulate_steps: 1

  # V5 實驗 5: 保持早停策略 ⭐
  early_stop:
    metric: "val.f1_macro_weighted"
    patience: 2  # 保持 (ChatGPT建議, 對準Ep9附近)
    mode: "max"
    min_delta: 0.0003  # 保持

# ==================== 評估配置 ====================
eval:
  group_split:
    by_stock: true

  metrics:
    unweighted: ["acc", "f1_macro", "per_class_pr_rc", "confusion"]
    weighted: ["f1_macro", "loss"]

  breakdowns: ["reasons", "t_hit_bucket", "ret_quantile", "stock"]
  thit_buckets: [0, 5, 10, 20, 100]
  ret_quantiles: [0.8, 0.9, 0.95]

# ==================== 校準配置 ====================
calibration:
  enabled: true
  type: "temperature"
  opt_metric: "weighted_nll"
  lr: 0.01
  max_iter: 50

# ==================== 推理配置 ====================
inference:
  decision: "argmax"
  cost_matrix:
    path: null
  hold_threshold: 0.0

# ==================== 日誌配置 ====================
logging:
  dir: "logs/deeplob_v5_exp5"
  save_best_by: "val.f1_macro_weighted"  # ChatGPT建議 (已實現)

  artifacts:
    - "model"
    - "norm_meta"
    - "label_mapping"
    - "calibration"
    - "cost_matrix"
    - "metrics_json"
    - "logs_csv"
    - "confmat_png"

  wandb:
    enabled: false
    project: "deeplob-v5-exp5"
    entity: null

# ==================== 安全檢查 ====================
safety_checks:
  validate_label_set: true
  validate_weights: true
  validate_norm_meta: true
  print_data_summary: true

# ==================== 硬體配置 ====================
hardware:
  device: "cuda"
  num_workers: 12
  pin_memory: true
  prefetch_factor: 4
  persistent_workers: true
  enable_tf32: true

# ==================== 續訓配置 ====================
resume:
  enabled: false
  checkpoint_path: "checkpoints/v5/deeplob_v5_best.pth"
  load_optimizer: true

# ==================== 其他配置 ====================
misc:
  save_last: true
  output_dir: "checkpoints/v5"

# ===================================================================
# V5 實驗 5 配置總結 (基於 ChatGPT 建議的保守微調) ⭐⭐⭐⭐⭐
# ===================================================================
# 背景:
# - V5 實驗 4: Val Acc 50.24% (Epoch 9), F1 0.4929
# - 問題診斷: 卡在 50% 天花板, Ep9 後過擬合 (梯度 4.14→6.65)
# - V7 實驗對比: V5 Exp-4 與 V7 Exp-2 幾乎一致 (說明配置最優)
#
# ChatGPT 建議 (2025-10-25):
# A. 早停對準 Ep9 ✅ (已實現)
# B. V7 Exp-4 建議參數:
#    - batch 160 ⭐⭐⭐⭐⭐ (關鍵改進, 降低梯度噪音)
#    - lr 7.5e-7 (本實驗更保守 7.3e-7)
#    - dropout 0.78, weight_decay 0.0028
# C. 保存策略: save_best_by F1 ✅ (已實現)
# D. 診斷輸出: 每類別 F1 + 混淆矩陣 ⭐
#
# V5 實驗 5 策略 (保守微調版):
# 採用 ChatGPT 建議, 但學習率更保守:
#
# 1. batch_size: 160 (+25%, ChatGPT核心建議) ⭐⭐⭐⭐⭐
# 2. lr: 7.3e-7 (+4%, 更保守, 避免Exp-3過擬合) ⭐⭐⭐⭐
# 3. dropout: 0.78 (-2.5%, 配合LR微升) ⭐⭐
# 4. weight_decay: 0.0029 (-3%, 平衡正則化) ⭐
# 5. grad_clip: 1.6 (+7%, 配合batch變大) ⭐
# 6. warmup_start_factor: 0.16 (+7%, 改善LR顯示) ⭐
# 7. eta_min: 2.1e-7 (+5%, 保持比例)
# 8. epochs: 22 (+10%, 預期最佳點延後)
#
# 50% 天花板根本原因 (專業判斷):
# 1. 模型容量不足 ⭐⭐⭐⭐⭐ (conv 32, lstm 48 vs 原版 64)
# 2. 數據標籤質量 ⭐⭐⭐⭐ (三分類高頻困難)
# 3. 優化策略過保守 ⭐⭐⭐ (極限正則化抑制學習)
#
# 實驗對比:
# | 配置      | V5 Exp-4    | V5 Exp-5 (本實驗) | 變化     |
# |-----------|-------------|-------------------|----------|
# | lr        | 7e-7        | 7.3e-7            | +4%      |
# | batch     | 128         | 160               | +25% ⭐   |
# | dropout   | 0.80        | 0.78              | -2.5%    |
# | weight_decay | 0.003    | 0.0029            | -3%      |
# | grad_clip | 1.5         | 1.6               | +7%      |
# | epochs    | 20          | 22                | +10%     |
#
# 預期效果:
# - Val Acc: 50.4-50.6% (+0.2-0.4% vs Exp-4)
# - 最佳點: Epoch 10-12 (vs Exp-4 Epoch 9)
# - 梯度範數: < 4.0 (vs Exp-4 的 4.14)
# - Train-Val Gap: < 3% (vs Exp-4 的 3.5%)
#
# 訓練指令:
# conda activate deeplob-pro
# python scripts/train_deeplob_v5.py --config configs/train_v5.yaml
#
# 監控重點:
# - Epoch 10-12 是否 Val Acc > 50.4%
# - 梯度範數是否 < 4.0
# - 是否仍卡在 50% 天花板 (若是, 改用激進方案B)
#
# 備選方案 B (激進突破, 高風險):
# 若 Exp-5 仍卡 50.5%, 下次改為:
# - 增加模型容量: conv 48, lstm 64
# - 適中學習率: lr 1.2e-6
# - 適中正則化: dropout 0.72, weight_decay 0.002
# - 大 Batch: 256
# ===================================================================
