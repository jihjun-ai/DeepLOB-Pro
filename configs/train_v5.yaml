# ===================================================================
# DeepLOB V7 實驗 3：平衡策略 (基於實驗 1/2 經驗) ❌ 失敗
# 目標：在學習速度與過擬合控制間找到最佳平衡點
# 核心策略：中等學習率 + 適中正則化 + 中等 Batch Size
# 日期：2025-10-25
# V7 實驗 1: Val Acc 50.18% (Epoch 7), 學習率 1e-6 過高
# V7 實驗 2: Val Acc 50.25% (Epoch 9), 學習率 7e-7 最優 ⭐
# V7 實驗 3: Val Acc 49.92% (Epoch 8), 學習率 8.5e-7 退步 -0.33% ❌
# 結論: 實驗 2 配置 (7e-7) 為當前最優
# ===================================================================

# ==================== 運行配置 ====================
run:
  seed: 42

# ==================== 標籤配置 ====================
labels:
  num_classes: 3
  class_names: ["下跌", "持平", "上漲"]
  class_names_en: ["down", "stationary", "up"]
  class_ids: [0, 1, 2]
  format: "v5"

  weight_calculation:
    epsilon: 1.0e-6
    normalize: true

# ==================== 可視化配置 ====================
visualization:
  confusion_matrix:
    normalize: true
    dpi: 150
    figsize: [8, 6]
    cmap: "Blues"

# ==================== 數據配置 ====================
data:
  train: "data/processed_v7/npz/stock_embedding_train.npz"
  val: "data/processed_v7/npz/stock_embedding_val.npz"
  test: "data/processed_v7/npz/stock_embedding_test.npz"
  norm_meta: "data/processed_v7/npz/normalization_meta.json"

  v5_labels: true

  y_raw_validation:
    enabled: true
    expected_values: [-1, 0, 1]

  # V7 實驗 1: 保持關閉樣本權重
  use_sample_weights: false
  weights_normalize: "mean_to_1"

  use_stock_ids: false
  fail_on_missing_keys: true

# ==================== DataLoader 配置 ====================
dataloader:
  # V7 實驗 3: 中等 Batch Size (平衡穩定性與正則化)
  batch_size: 192  # 實驗2: 128 → 192 (介於 128-256, 降噪同時保留穩定性)
  num_workers: 12
  pin_memory: true

  # 關閉平衡採樣器
  balance_sampler:
    enabled: false
    strategy: "inv_freq"

# ==================== 模型配置 ====================
model:
  arch: "DeepLOB"

  input:
    shape: [100, 20]

  num_classes: 3

  # V7 實驗 1: 減小模型容量 (降低過擬合風險)
  conv1_filters: 32  # 48 → 32 (降回基準)
  conv2_filters: 32  # 48 → 32
  conv3_filters: 32  # 48 → 32

  # V7 實驗 1: 減小 LSTM/FC 容量
  lstm_hidden_size: 48  # 64 → 48 (介於 32-64)
  fc_hidden_size: 48    # 64 → 48

  # V7 實驗 3: 適中 Dropout (避免過強抑制學習)
  dropout: 0.77  # 實驗2: 0.80 → 0.77 (稍微放寬, 實驗2 正則化過強)

  embeddings:
    enabled: false
    num_stocks: 367
    dim: 16
    dropout: 0.1
    weight_decay: 0.0001
    max_norm: null
    unk_strategy: "explicit_unk"
    feature_dropout:
      p: 0.3
    conditioning: "film"

# ==================== 損失函數配置 ====================
loss:
  type: "ce"

  # V7 實驗 1: 保持關閉類別權重
  class_weights: none

  # V7 實驗 3: 適中標籤平滑
  label_smoothing:
    global: 0.025  # 實驗2: 0.03 → 0.025 (介於實驗1/2 之間)
    flat_bonus: 0.00

# ==================== 優化器配置 ====================
optim:
  name: "adamw"

  # V7 實驗 3: 平衡學習率 (介於實驗1/2) ⭐⭐⭐ 核心修改
  # 實驗1: 1e-6 過高 → 過擬合快 | 實驗2: 7e-7 過低 → 學習太慢
  lr: 0.00000085  # 實驗2: 7e-7 → 8.5e-7 (+21%, 尋找平衡點)

  # V7 實驗 3: 適中 Weight Decay ⭐⭐⭐ 核心修改
  weight_decay: 0.0025  # 實驗2: 0.003 → 0.0025 (-17%, 稍微放寬)

  # V7 實驗 3: 適中梯度裁剪
  grad_clip: 1.7  # 實驗2: 1.5 → 1.7 (稍微放寬)

  amp: false
  use_bf16: false

  betas: [0.9, 0.999]
  eps: 1.0e-8
  amsgrad: false

# ==================== 學習率調度器 ====================
sched:
  name: "cosine"

  # V7 實驗 3: 適中 Warmup (介於實驗1/2)
  warmup_ratio: 0.35  # 實驗2: 0.40 → 0.35 (約 6/18 epochs warmup)
  warmup_start_factor: 0.17  # 實驗2: 0.15 → 0.17 (起點約 1.4e-7)

  # V7 實驗 3: 適中最小學習率
  eta_min: 0.00000025  # 實驗2: 2e-7 → 2.5e-7 (配合 8.5e-7 基礎 LR)

# ==================== 訓練配置 ====================
train:
  # V7 實驗 3: 適中訓練期 (預期 Epoch 8-10 達最佳)
  epochs: 18  # 實驗2: 20 → 18 (縮短 2 epochs, 學習率提升後更快)
  accumulate_steps: 1

  # V7 實驗 3: 保持適中早停策略
  early_stop:
    metric: "val.f1_macro_weighted"
    patience: 2  # 保持 (實驗1/2 均為 2)
    mode: "max"
    min_delta: 0.0003  # 保持

# ==================== 評估配置 ====================
eval:
  group_split:
    by_stock: true

  metrics:
    unweighted: ["acc", "f1_macro", "per_class_pr_rc", "confusion"]
    weighted: ["f1_macro", "loss"]

  breakdowns: ["reasons", "t_hit_bucket", "ret_quantile", "stock"]
  thit_buckets: [0, 5, 10, 20, 100]
  ret_quantiles: [0.8, 0.9, 0.95]

# ==================== 校準配置 ====================
calibration:
  enabled: true
  type: "temperature"
  opt_metric: "weighted_nll"
  lr: 0.01
  max_iter: 50

# ==================== 推理配置 ====================
inference:
  decision: "argmax"
  cost_matrix:
    path: null
  hold_threshold: 0.0

# ==================== 日誌配置 ====================
logging:
  dir: "logs/deeplob_v7_exp3"
  save_best_by: "val.f1_macro_weighted"

  artifacts:
    - "model"
    - "norm_meta"
    - "label_mapping"
    - "calibration"
    - "cost_matrix"
    - "metrics_json"
    - "logs_csv"
    - "confmat_png"

  wandb:
    enabled: false
    project: "deeplob-v7-exp3"
    entity: null

# ==================== 安全檢查 ====================
safety_checks:
  validate_label_set: true
  validate_weights: true
  validate_norm_meta: true
  print_data_summary: true

# ==================== 硬體配置 ====================
hardware:
  device: "cuda"
  num_workers: 12
  pin_memory: true
  prefetch_factor: 4
  persistent_workers: true
  enable_tf32: true

# ==================== 續訓配置 ====================
resume:
  enabled: false
  checkpoint_path: "checkpoints/v5/deeplob_v5_best.pth"
  load_optimizer: true

# ==================== 其他配置 ====================
misc:
  save_last: true
  output_dir: "checkpoints/v5"

# ===================================================================
# V7 實驗 3 修改總結 (基於實驗 1/2 結果尋找平衡點) ❌ 失敗
# ===================================================================
# 實驗 1 成果: Val Acc 50.18% (Epoch 7), Val F1 0.4949 (Epoch 8)
# 實驗 1 問題: 學習率 1e-6 過高, Epoch 7-9 過擬合加劇
#
# 實驗 2 成果: Val Acc 50.25% (Epoch 9), Val F1 0.4929 ⭐ 當前最優
# 實驗 2 特點: 學習率 7e-7 穩定, 過擬合控制最佳
#
# 實驗 3 假設 (錯誤): 平衡學習速度與過擬合控制 (介於實驗1/2)
#
# 核心改動 (基於實驗 2, 10 個參數)：
# 1. batch_size: 128 → 192 (+50%, 介於 128-256, 平衡穩定與噪音)
# 2. dropout: 0.80 → 0.77 (-4%, 稍微放寬, 避免抑制學習)
# 3. label_smoothing: 0.03 → 0.025 (-17%, 回調)
# 4. lr: 7e-7 → 8.5e-7 (+21%, 介於 7e-7 和 1e-6) ❌ 反而更差
# 5. weight_decay: 0.003 → 0.0025 (-17%, 稍微放寬)
# 6. grad_clip: 1.5 → 1.7 (+13%, 稍微放寬)
# 7. warmup_ratio: 0.40 → 0.35 (-13%, 介於實驗1/2)
# 8. warmup_start_factor: 0.15 → 0.17 (+13%, 起點約 1.4e-7)
# 9. eta_min: 2e-7 → 2.5e-7 (+25%)
# 10. epochs: 20 → 18 (-2, 學習率提升後預期更快達最佳)
#
# 實際結果 (實驗 3): ❌ 未達預期
# - Val Acc: 49.92% (Epoch 8), 比實驗 2 退步 -0.33%
# - Val F1: 0.4949 (與實驗 1 持平)
# - 梯度範數: 5.06 (最高, 過擬合最嚴重)
#
# 實驗對比總結:
# | 指標      | 實驗 1       | 實驗 2 ⭐    | 實驗 3       |
# |-----------|--------------|--------------|--------------|
# | Val Acc   | 50.18%       | 50.25% (最佳) | 49.92%       |
# | Val F1    | 0.4949       | 0.4929       | 0.4949       |
# | 學習率    | 1e-6 (過高)  | 7e-7 (合適)  | 8.5e-7 (偏高) |
# | 梯度範數  | 4.46         | 4.13 (最低)  | 5.06 (最高)  |
#
# 結論:
# - 實驗 3 的"平衡假設"錯誤, 導致性能退步
# - 實驗 2 (學習率 7e-7) 為當前最優配置
# - 下一步: 基於實驗 2 做極微幅調整 (+7% LR → 7.5e-7)
#
# 訓練指令：
# conda activate deeplob-pro
# python scripts/train_deeplob_v5.py --config configs/train_v5.yaml
# ===================================================================
