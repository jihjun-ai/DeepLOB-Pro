# ===================================================================
# DeepLOB V5 實驗 5：突破 60% 準確率挑戰
# 目標：從 51.14% → >60% (Test Acc)
# 核心策略：增強模型容量 + 延長訓練 + 改善 Class 0/2 學習
# 日期：2025-10-24
# ===================================================================

# ==================== 運行配置 ====================
run:
  seed: 42

# ==================== 標籤配置 ====================
labels:
  num_classes: 3
  class_names: ["下跌", "持平", "上漲"]
  class_names_en: ["down", "stationary", "up"]
  class_ids: [0, 1, 2]
  format: "v5"

  weight_calculation:
    epsilon: 1.0e-6
    normalize: true

# ==================== 可視化配置 ====================
visualization:
  confusion_matrix:
    normalize: true
    dpi: 150
    figsize: [8, 6]
    cmap: "Blues"

# ==================== 數據配置 ====================
data:
  train: "data/processed_v7/npz/stock_embedding_train.npz"
  val: "data/processed_v7/npz/stock_embedding_val.npz"
  test: "data/processed_v7/npz/stock_embedding_test.npz"
  norm_meta: "data/processed_v7/npz/normalization_meta.json"

  v5_labels: true

  y_raw_validation:
    enabled: true
    expected_values: [-1, 0, 1]

  # 【改動 1】啟用樣本權重（改善 Class 0/2 學習）
  use_sample_weights: true
  weights_normalize: "mean_to_1"

  use_stock_ids: false
  fail_on_missing_keys: true

# ==================== DataLoader 配置 ====================
dataloader:
  # 【改動 2】增加 Batch Size（提升訓練穩定性）
  batch_size: 512  # 384 → 512
  num_workers: 12
  pin_memory: true

  # 關閉平衡採樣器（避免過擬合）
  balance_sampler:
    enabled: false
    strategy: "inv_freq"

# ==================== 模型配置 ====================
model:
  arch: "DeepLOB"

  input:
    shape: [100, 20]

  num_classes: 3

  # 【改動 3】增大卷積容量（提升特徵提取能力）
  conv1_filters: 48  # 32 → 48
  conv2_filters: 48  # 32 → 48
  conv3_filters: 48  # 32 → 48

  # 【改動 4】增大 LSTM/FC 容量（提升時序建模能力）
  lstm_hidden_size: 64  # 32 → 64
  fc_hidden_size: 64    # 32 → 64

  # 【改動 5】降低 Dropout（避免欠擬合）
  dropout: 0.65  # 0.75 → 0.65（容量增大後，正則化要減弱）

  embeddings:
    enabled: false
    num_stocks: 367
    dim: 16
    dropout: 0.1
    weight_decay: 0.0001
    max_norm: null
    unk_strategy: "explicit_unk"
    feature_dropout:
      p: 0.3
    conditioning: "film"

# ==================== 損失函數配置 ====================
loss:
  type: "ce"

  # 【改動 6】啟用溫和類別權重（改善 Class 0/2）
  class_weights: "auto"  # none → auto

  # 【改動 7】降低標籤平滑（避免過度模糊邊界）
  label_smoothing:
    global: 0.01  # 0.02 → 0.01（輕度降低）
    flat_bonus: 0.00

# ==================== 優化器配置 ====================
optim:
  name: "adamw"

  # 【改動 8】提高學習率（容量增大需要更快學習）
  lr: 0.000003  # 2.5e-6 → 3e-6（提高 20%）

  # 【改動 9】降低 Weight Decay（容量增大後，減弱正則化）
  weight_decay: 0.0005  # 0.001 → 0.0005（減半）

  # 【改動 10】放寬梯度裁剪（容量增大後，允許更大梯度）
  grad_clip: 2.0  # 1.5 → 2.0

  amp: false
  use_bf16: false

  betas: [0.9, 0.999]
  eps: 1.0e-8
  amsgrad: false

# ==================== 學習率調度器 ====================
sched:
  name: "cosine"

  # 【改動 11】延長 Warmup（給模型更多穩定期）
  warmup_ratio: 0.25  # 0.167 → 0.25（5/20 epochs）
  warmup_start_factor: 0.333  # 0.4 → 0.333（起點 1e-6）

  # 【改動 12】降低最小學習率（更長衰減期）
  eta_min: 0.0000005  # 1e-6 → 5e-7（更低）

# ==================== 訓練配置 ====================
train:
  # 【改動 13】延長訓練期（給模型更多學習時間）
  epochs: 20  # 10 → 20（容量增大需要更多訓練）
  accumulate_steps: 1

  # 【改動 14】大幅放寬早停（只防止嚴重惡化）
  early_stop:
    metric: "val.f1_macro_weighted"
    patience: 5  # 1 → 5（非常寬容，允許 5 個 epoch 波動）
    mode: "max"
    min_delta: 0.0001  # 0.0005 → 0.0001（極低閾值，幾乎不限制）

# ==================== 評估配置 ====================
eval:
  group_split:
    by_stock: true

  metrics:
    unweighted: ["acc", "f1_macro", "per_class_pr_rc", "confusion"]
    weighted: ["f1_macro", "loss"]

  breakdowns: ["reasons", "t_hit_bucket", "ret_quantile", "stock"]
  thit_buckets: [0, 5, 10, 20, 100]
  ret_quantiles: [0.8, 0.9, 0.95]

# ==================== 校準配置 ====================
calibration:
  enabled: true
  type: "temperature"
  opt_metric: "weighted_nll"
  lr: 0.01
  max_iter: 50

# ==================== 推理配置 ====================
inference:
  decision: "argmax"
  cost_matrix:
    path: null
  hold_threshold: 0.0

# ==================== 日誌配置 ====================
logging:
  dir: "logs/deeplob_v5_exp5"
  save_best_by: "val.f1_macro_weighted"

  artifacts:
    - "model"
    - "norm_meta"
    - "label_mapping"
    - "calibration"
    - "cost_matrix"
    - "metrics_json"
    - "logs_csv"
    - "confmat_png"

  wandb:
    enabled: false
    project: "deeplob-v5-exp5"
    entity: null

# ==================== 安全檢查 ====================
safety_checks:
  validate_label_set: true
  validate_weights: true
  validate_norm_meta: true
  print_data_summary: true

# ==================== 硬體配置 ====================
hardware:
  device: "cuda"
  num_workers: 12
  pin_memory: true
  prefetch_factor: 4
  persistent_workers: true
  enable_tf32: true

# ==================== 續訓配置 ====================
resume:
  enabled: false
  checkpoint_path: "checkpoints/v5/deeplob_v5_best.pth"
  load_optimizer: true

# ==================== 其他配置 ====================
misc:
  save_last: true
  output_dir: "checkpoints/v5"

# ===================================================================
# 實驗 5：突破 60% 準確率的 14 個核心改動
# ===================================================================
#
# 【問題診斷】（實驗 4b 結果）：
# - 測試準確率：51.14%（目標 >60%，差距 8.86%）
# - Class 0 召回率：40.72%（偏低）
# - Class 2 召回率：36.13%（偏低）
# - 模型過於保守（偏好預測 Class 1）
#
# 【核心策略】（14 個改動）：
#
# 1️⃣ **增強模型容量**（最關鍵，6 個改動）
#    - Conv filters: 32 → 48 (+50%)
#    - LSTM hidden: 32 → 64 (+100%)
#    - FC hidden: 32 → 64 (+100%)
#    理由：51% 可能接近模型容量上限，需要更大網絡
#
# 2️⃣ **延長訓練時間**（2 個改動）
#    - Epochs: 10 → 20 (+100%)
#    - Patience: 1 → 3 (+200%)
#    理由：容量增大後，需要更多時間收斂
#
# 3️⃣ **減弱正則化**（3 個改動）
#    - Dropout: 0.75 → 0.65 (-13%)
#    - Weight Decay: 0.001 → 0.0005 (-50%)
#    - Label Smoothing: 0.02 → 0.01 (-50%)
#    理由：容量增大後，過強正則化會欠擬合
#
# 4️⃣ **改善 Class 0/2 學習**（2 個改動）
#    - Sample Weights: false → true
#    - Class Weights: none → auto
#    理由：針對性改善召回率低的類別
#
# 5️⃣ **優化學習率策略**（3 個改動）
#    - LR: 2.5e-6 → 3e-6 (+20%)
#    - Warmup: 16.7% → 25%
#    - eta_min: 1e-6 → 5e-7
#    理由：容量增大需要更高學習率和更長 warmup
#
# 6️⃣ **提升訓練穩定性**（2 個改動）
#    - Batch Size: 384 → 512 (+33%)
#    - Grad Clip: 1.5 → 2.0 (+33%)
#    理由：大模型需要更大 batch 和更寬容梯度控制
#
# 【風險控制】：
# - ⚠️ 容量翻倍可能導致過擬合 → 監控 Train-Val Gap
# - ⚠️ 學習率提高可能梯度爆炸 → 監控 Grad Norm (<5.0)
# - ⚠️ 訓練時間延長 → 預計 10-15 分鐘（原 2 分鐘）
#
# 【預期目標】：
# - Val Acc: 55-60%（提升 5-9%）
# - Test Acc: >60%（突破目標）
# - Class 0/2 Recall: >50%（改善 10%+）
# - Train-Val Gap: <8%（可接受範圍）
# - Grad Norm: <5.0（穩定）
# - Best Epoch: 8-15（後期達到最佳）
#
# 【訓練指令】：
# conda activate deeplob-pro
# python scripts/train_deeplob_v5.py --config configs/train_v5.yaml --data-dir data/processed_v7/npz
#
# 【監控重點】：
# 1. Train-Val Gap（應 <10%，避免過擬合）
# 2. Grad Norm（應 <5.0，避免爆炸）
# 3. Class 0/2 Recall（應逐步提升）
# 4. Best Epoch（應在 8-15 之間，不要太晚）
#
# 【備用策略】（如果失敗）：
# - Plan B: 僅增大 LSTM/FC（64 → 96），保持 Conv=32
# - Plan C: 啟用數據增強（需要新腳本）
# - Plan D: 使用更長時間窗口（100 → 150 timesteps，需重新生成數據）
#
# ===================================================================
