"""
Triple-Barrier æ¨™ç±¤è¨ºæ–·å·¥å…·
==================================================
åˆ†æç•¶å‰ tb_labels çš„è§¸ç™¼æ¨¡å¼å’Œæ¨™ç±¤åˆ†å¸ƒ

ä½¿ç”¨æ–¹å¼ï¼š
    python scripts/analyze_tb_labels.py --data-dir ./data/temp --sample-days 3
"""

import os
import sys
import argparse
import logging
from pathlib import Path
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from collections import Counter

# æ·»åŠ é …ç›®æ ¹ç›®éŒ„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

# è¨­å®šæ—¥èªŒ
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)

# å¾ extract_tw_stock_data_v5.py å°å…¥å¿…è¦å‡½æ•¸
from scripts.extract_tw_stock_data_v5 import (
    parse_line, dedup_by_timestamp_keep_last, aggregate_chunks_of_10,
    ewma_vol, tb_labels, load_config,
    TRADING_START, TRADING_END
)


def analyze_single_day(file_path: str, config: dict) -> dict:
    """åˆ†æå–®æ—¥æ•¸æ“šçš„ tb_labels åˆ†å¸ƒ"""

    logging.info(f"\n{'='*60}")
    logging.info(f"åˆ†ææ–‡ä»¶: {os.path.basename(file_path)}")
    logging.info(f"{'='*60}")

    # è®€å–ä¸¦è§£ææ•¸æ“š
    symbol_data = {}

    with open(file_path, 'r', encoding='utf-8') as f:
        for line in f:
            sym, t, rec = parse_line(line)
            if rec is None:
                continue
            if sym not in symbol_data:
                symbol_data[sym] = []
            symbol_data[sym].append((t, rec))

    if not symbol_data:
        logging.warning("æ²’æœ‰æœ‰æ•ˆæ•¸æ“š")
        return None

    # åˆ†æç¬¬ä¸€å€‹è‚¡ç¥¨ï¼ˆç¤ºç¯„ï¼‰
    sample_symbol = list(symbol_data.keys())[0]
    rows = symbol_data[sample_symbol]

    logging.info(f"ç¤ºç¯„è‚¡ç¥¨: {sample_symbol}, åŸå§‹äº‹ä»¶æ•¸: {len(rows)}")

    # å»é‡å’Œèšåˆ
    rows_dedup = dedup_by_timestamp_keep_last(rows)
    Xd, mids = aggregate_chunks_of_10(rows_dedup)

    if len(mids) < 100:
        logging.warning(f"èšåˆå¾Œæ•¸æ“šé»å¤ªå°‘: {len(mids)}")
        return None

    logging.info(f"èšåˆå¾Œæ•¸æ“šé»: {len(mids)}")

    # è¨ˆç®—æ³¢å‹•ç‡å’Œæ¨™ç±¤
    close = pd.Series(mids, name='close')
    vol = ewma_vol(close, halflife=config['volatility']['halflife'])

    tb_cfg = config['triple_barrier']

    try:
        tb_df = tb_labels(
            close=close,
            vol=vol,
            pt_mult=tb_cfg['pt_multiplier'],
            sl_mult=tb_cfg['sl_multiplier'],
            max_holding=tb_cfg['max_holding'],
            min_return=tb_cfg['min_return'],
            day_end_idx=len(close) - 1
        )
    except Exception as e:
        logging.error(f"tb_labels å¤±æ•—: {e}")
        return None

    # çµ±è¨ˆåˆ†æ
    stats = {
        'symbol': sample_symbol,
        'n_points': len(close),
        'n_labels': len(tb_df),
    }

    # è§¸ç™¼åŸå› åˆ†å¸ƒ
    why_counts = tb_df['why'].value_counts().to_dict()
    stats['trigger_counts'] = why_counts
    stats['trigger_pct'] = {k: v/len(tb_df)*100 for k, v in why_counts.items()}

    # æ¨™ç±¤åˆ†å¸ƒ
    label_counts = tb_df['y'].value_counts().to_dict()
    stats['label_counts'] = label_counts
    stats['label_pct'] = {k: v/len(tb_df)*100 for k, v in label_counts.items()}

    # æ¯ç¨®è§¸ç™¼æ–¹å¼çš„æ¨™ç±¤åˆ†å¸ƒ
    stats['trigger_label_breakdown'] = {}
    for trigger in ['up', 'down', 'time']:
        if trigger in tb_df['why'].values:
            subset = tb_df[tb_df['why'] == trigger]
            label_dist = subset['y'].value_counts().to_dict()
            stats['trigger_label_breakdown'][trigger] = label_dist

    # time trigger çš„å ±é…¬åˆ†å¸ƒ
    time_subset = tb_df[tb_df['why'] == 'time']
    if len(time_subset) > 0:
        ret_abs = time_subset['ret'].abs()
        stats['time_ret_stats'] = {
            'mean': float(ret_abs.mean()),
            'median': float(ret_abs.median()),
            'p25': float(ret_abs.quantile(0.25)),
            'p75': float(ret_abs.quantile(0.75)),
            'p90': float(ret_abs.quantile(0.90)),
            'below_min_return': int((ret_abs < tb_cfg['min_return']).sum()),
            'total': len(time_subset)
        }

    # æ³¢å‹•ç‡çµ±è¨ˆ
    vol_clean = vol.replace([np.inf, -np.inf], np.nan).dropna()
    stats['vol_stats'] = {
        'mean': float(vol_clean.mean()),
        'median': float(vol_clean.median()),
        'min': float(vol_clean.min()),
        'max': float(vol_clean.max()),
        'near_zero': int((vol_clean < 1e-6).sum()),
        'total': len(vol_clean)
    }

    # åˆ—å°è©³ç´°å ±å‘Š
    print_analysis_report(stats, tb_cfg)

    return stats


def print_analysis_report(stats: dict, config: dict):
    """åˆ—å°åˆ†æå ±å‘Š"""

    print(f"\n{'='*60}")
    print(f"ğŸ“Š Triple-Barrier æ¨™ç±¤è¨ºæ–·å ±å‘Š")
    print(f"{'='*60}")

    print(f"\nã€åŸºæœ¬è³‡è¨Šã€‘")
    print(f"  è‚¡ç¥¨ä»£ç¢¼: {stats['symbol']}")
    print(f"  æ•¸æ“šé»æ•¸: {stats['n_points']}")
    print(f"  æ¨™ç±¤æ•¸é‡: {stats['n_labels']}")

    print(f"\nã€ç•¶å‰åƒæ•¸ã€‘")
    print(f"  PT å€æ•¸: {config['pt_multiplier']}")
    print(f"  SL å€æ•¸: {config['sl_multiplier']}")
    print(f"  æœ€å¤§æŒæœ‰: {config['max_holding']} bars")
    print(f"  æœ€å°å ±é…¬: {config['min_return']} ({config['min_return']*100:.3f}%)")

    print(f"\nã€è§¸ç™¼åŸå› åˆ†å¸ƒã€‘")
    for trigger, count in stats['trigger_counts'].items():
        pct = stats['trigger_pct'][trigger]
        print(f"  {trigger:>6}: {count:>6} ({pct:>5.1f}%)")

    print(f"\nã€æ¨™ç±¤åˆ†å¸ƒï¼ˆæ•´é«”ï¼‰ã€‘")
    for label, count in sorted(stats['label_counts'].items()):
        pct = stats['label_pct'][label]
        label_name = {-1: 'ä¸‹è·Œ', 0: 'æŒå¹³', 1: 'ä¸Šæ¼²'}[label]
        print(f"  {label_name} ({label:+2}): {count:>6} ({pct:>5.1f}%)")

    print(f"\nã€å„è§¸ç™¼æ–¹å¼çš„æ¨™ç±¤åˆ†å¸ƒã€‘")
    for trigger, label_dist in stats['trigger_label_breakdown'].items():
        total = sum(label_dist.values())
        print(f"  {trigger} (n={total}):")
        for label in sorted(label_dist.keys()):
            count = label_dist[label]
            pct = count / total * 100
            label_name = {-1: 'ä¸‹è·Œ', 0: 'æŒå¹³', 1: 'ä¸Šæ¼²'}[label]
            print(f"    {label_name} ({label:+2}): {count:>5} ({pct:>5.1f}%)")

    if 'time_ret_stats' in stats:
        print(f"\nã€time trigger å ±é…¬çµ±è¨ˆï¼ˆçµ•å°å€¼ï¼‰ã€‘")
        trs = stats['time_ret_stats']
        print(f"  å¹³å‡å€¼: {trs['mean']*100:.4f}%")
        print(f"  ä¸­ä½æ•¸: {trs['median']*100:.4f}%")
        print(f"  25% åˆ†ä½: {trs['p25']*100:.4f}%")
        print(f"  75% åˆ†ä½: {trs['p75']*100:.4f}%")
        print(f"  90% åˆ†ä½: {trs['p90']*100:.4f}%")
        print(f"  ä½æ–¼ min_return: {trs['below_min_return']}/{trs['total']} ({trs['below_min_return']/trs['total']*100:.1f}%)")

    print(f"\nã€æ³¢å‹•ç‡çµ±è¨ˆã€‘")
    vs = stats['vol_stats']
    print(f"  å¹³å‡å€¼: {vs['mean']*100:.4f}%")
    print(f"  ä¸­ä½æ•¸: {vs['median']*100:.4f}%")
    print(f"  æœ€å°å€¼: {vs['min']*100:.6f}%")
    print(f"  æœ€å¤§å€¼: {vs['max']*100:.4f}%")
    print(f"  æ¥è¿‘é›¶ (<1e-6): {vs['near_zero']}/{vs['total']}")

    print(f"\n{'='*60}")
    print(f"ğŸ” é—œéµç™¼ç¾ï¼š")
    print(f"{'='*60}")

    # è‡ªå‹•è¨ºæ–·
    issues = []

    # æª¢æŸ¥ "å¹³" æ¯”ä¾‹
    if 0 in stats['label_pct']:
        zero_pct = stats['label_pct'][0]
        if zero_pct < 10:
            issues.append(f"âŒ 'æŒå¹³' æ¨™ç±¤æ¯”ä¾‹éä½ ({zero_pct:.1f}%)")
        elif zero_pct < 20:
            issues.append(f"âš ï¸  'æŒå¹³' æ¨™ç±¤æ¯”ä¾‹åä½ ({zero_pct:.1f}%)")
        else:
            issues.append(f"âœ… 'æŒå¹³' æ¨™ç±¤æ¯”ä¾‹åˆç† ({zero_pct:.1f}%)")
    else:
        issues.append(f"âŒ æ²’æœ‰ 'æŒå¹³' æ¨™ç±¤ï¼")

    # æª¢æŸ¥ PT/SL è§¸ç™¼ç‡
    pt_sl_pct = stats['trigger_pct'].get('up', 0) + stats['trigger_pct'].get('down', 0)
    if pt_sl_pct > 80:
        issues.append(f"âš ï¸  PT/SL è§¸ç™¼ç‡éé«˜ ({pt_sl_pct:.1f}%)ï¼Œå¯èƒ½é–€æª»å¤ªçª„")

    # æª¢æŸ¥æ³¢å‹•ç‡
    if vs['near_zero'] > vs['total'] * 0.1:
        issues.append(f"âš ï¸  {vs['near_zero']/vs['total']*100:.1f}% çš„æ³¢å‹•ç‡æ¥è¿‘é›¶ï¼Œå¯èƒ½å°è‡´é–€æª»éå°")

    # æª¢æŸ¥ time trigger çš„åˆ©ç”¨
    if 'time_ret_stats' in stats:
        trs = stats['time_ret_stats']
        if trs['below_min_return'] / trs['total'] < 0.3:
            issues.append(f"âš ï¸  åªæœ‰ {trs['below_min_return']/trs['total']*100:.1f}% çš„ time trigger ä½æ–¼ min_return")
            issues.append(f"    å»ºè­°ï¼šæé«˜ min_return åˆ° {trs['p75']*100:.4f}% (75åˆ†ä½) æˆ– {trs['p90']*100:.4f}% (90åˆ†ä½)")

    for issue in issues:
        print(f"  {issue}")

    print(f"\n{'='*60}\n")


def main():
    parser = argparse.ArgumentParser(description='Triple-Barrier æ¨™ç±¤è¨ºæ–·å·¥å…·')
    parser.add_argument('--data-dir', default='./data/temp', help='åŸå§‹æ•¸æ“šç›®éŒ„')
    parser.add_argument('--config', default='./configs/config_pro_v5.yaml', help='é…ç½®æ–‡ä»¶')
    parser.add_argument('--sample-days', type=int, default=3, help='åˆ†æå¤©æ•¸')
    args = parser.parse_args()

    # è¼‰å…¥é…ç½®
    config = load_config(args.config)

    # æŸ¥æ‰¾æ•¸æ“šæ–‡ä»¶
    data_files = sorted(Path(args.data_dir).glob('*.txt'))[:args.sample_days]

    if not data_files:
        logging.error(f"åœ¨ {args.data_dir} æ‰¾ä¸åˆ° .txt æ–‡ä»¶")
        return

    logging.info(f"æ‰¾åˆ° {len(data_files)} å€‹æ•¸æ“šæ–‡ä»¶ï¼Œå°‡åˆ†æå‰ {args.sample_days} å¤©")

    # åˆ†ææ¯ä¸€å¤©
    all_stats = []
    for file_path in data_files:
        stats = analyze_single_day(str(file_path), config)
        if stats:
            all_stats.append(stats)

    if not all_stats:
        logging.error("æ²’æœ‰æˆåŠŸåˆ†æçš„æ•¸æ“š")
        return

    # å½™ç¸½å ±å‘Š
    print(f"\n{'='*60}")
    print(f"ğŸ“ˆ å¤šæ—¥å½™ç¸½çµ±è¨ˆ")
    print(f"{'='*60}")

    # å¹³å‡æ¨™ç±¤åˆ†å¸ƒ
    avg_labels = {-1: 0, 0: 0, 1: 0}
    for stats in all_stats:
        for label, pct in stats['label_pct'].items():
            avg_labels[label] += pct

    n = len(all_stats)
    print(f"\nã€å¹³å‡æ¨™ç±¤åˆ†å¸ƒã€‘(åŸºæ–¼ {n} å¤©)")
    for label in sorted(avg_labels.keys()):
        pct = avg_labels[label] / n
        label_name = {-1: 'ä¸‹è·Œ', 0: 'æŒå¹³', 1: 'ä¸Šæ¼²'}[label]
        print(f"  {label_name} ({label:+2}): {pct:>5.1f}%")

    print(f"\n{'='*60}\n")


if __name__ == '__main__':
    main()

