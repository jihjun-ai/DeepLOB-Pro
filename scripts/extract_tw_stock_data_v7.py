# -*- coding: utf-8 -*-
"""
extract_tw_stock_data_v7.py - V7 ç°¡åŒ–ç‰ˆè³‡æ–™æµæ°´ç·šï¼ˆå°ˆæ³¨æ•¸æ“šçµ„ç¹”ï¼‰
=============================================================================
ã€æ›´æ–°æ—¥æœŸã€‘2025-10-25
ã€ç‰ˆæœ¬èªªæ˜ã€‘v7.1.0-no-cross-file - ä¿®å¾©æ»‘å‹•çª—å£è·¨æ–‡ä»¶å•é¡Œ

æ ¸å¿ƒç†å¿µï¼š
  "é è™•ç†å·²å®Œæˆï¼ŒV7 åªåšæ•¸æ“šçµ„ç¹”"

V7 ç°¡åŒ–æµç¨‹ï¼š
  é è™•ç†éšæ®µï¼ˆpreprocess_single_day.pyï¼‰:
    âœ… æ•¸æ“šæ¸…æ´—èˆ‡èšåˆ
    âœ… Z-Score æ¨™æº–åŒ–
    âœ… æ¨™ç±¤è¨ˆç®—ï¼ˆTriple-Barrier / Trendï¼‰
    âœ… æ¬Šé‡ç­–ç•¥è¨ˆç®—ï¼ˆ11 ç¨®ï¼‰
    âœ… çµ±è¨ˆä¿¡æ¯è¨˜éŒ„

  V7 éšæ®µï¼ˆextract_tw_stock_data_v7.pyï¼‰:
    âœ… è®€å–é è™•ç† NPZï¼ˆç›´æ¥ä½¿ç”¨ features, labelsï¼‰
    âœ… æ•¸æ“šé¸æ“‡ï¼ˆdataset_selection.json æˆ–é…ç½®éæ¿¾ï¼‰
    âœ… æ»‘å‹•çª—å£ç”Ÿæˆï¼ˆ100 timestepsï¼‰- ã€V7.1 ä¿®å¾©ï¼šé€æ–‡ä»¶è™•ç†ï¼Œé¿å…è·¨æ–‡ä»¶ã€‘
    âœ… æŒ‰è‚¡ç¥¨åŠƒåˆ†ï¼ˆtrain/val/test = 70/15/15ï¼‰
    âœ… è¼¸å‡º NPZï¼ˆèˆ‡ V6 æ ¼å¼å…¼å®¹ï¼‰

    âŒ ä¸é‡æ–°è¨ˆç®—æ¨™ç±¤
    âŒ ä¸é‡æ–°è¨ˆç®—æ³¢å‹•ç‡
    âŒ ä¸é‡æ–°è¨ˆç®—æ¬Šé‡
    âŒ ä¸é‡æ–°æ¨™æº–åŒ–

V7.1 ä¿®å¾©å…§å®¹ï¼ˆ2025-10-25ï¼‰:
  âœ… ä¿®å¾©æ»‘å‹•çª—å£è·¨æ–‡ä»¶å•é¡Œ
  âœ… æ”¹ç‚ºé€å€‹ NPZ æ–‡ä»¶ç¨ç«‹è™•ç†
  âœ… ç¢ºä¿æ¯å€‹çª—å£çš„ 100 å€‹ timesteps ä¾†è‡ªåŒä¸€å¤©ï¼ˆæ™‚é–“é€£çºŒï¼‰
  âœ… é¿å…ä¸åŒæ—¥æœŸæ•¸æ“šæ··åˆï¼ˆä¾‹å¦‚ 9/1 æœ€å¾Œ 50 ç­† + 9/2 å‰ 50 ç­†ï¼‰

ä½¿ç”¨æ–¹å¼ï¼š
  # é¸é … 1: ä½¿ç”¨ dataset_selection.jsonï¼ˆæ¨è–¦ï¼‰
  python scripts/analyze_label_distribution.py       --preprocessed-dir data/preprocessed_v5       --mode smart_recommend       --target-dist "0.30,0.40,0.30"       --output results/dataset_selection_auto.json

  python scripts/extract_tw_stock_data_v7.py       --preprocessed-dir ./data/preprocessed_v5       --output-dir ./data/processed_v7       --config configs/config_pro_v7_optimal.yaml

  # é¸é … 2: ä½¿ç”¨é…ç½®éæ¿¾
  python scripts/extract_tw_stock_data_v7.py       --preprocessed-dir ./data/preprocessed_v5       --output-dir ./data/processed_v7       --config configs/config_v7_test.yaml

è¼¸å‡ºï¼š
  - ./data/processed_v7/npz/stock_embedding_train.npz
  - ./data/processed_v7/npz/stock_embedding_val.npz
  - ./data/processed_v7/npz/stock_embedding_test.npz
  - ./data/processed_v7/npz/normalization_meta.json

ç‰ˆæœ¬ï¼šv7.1.0-no-cross-file
æ›´æ–°ï¼š2025-10-25
"""
import os
import json
import argparse
import glob
import logging
import warnings
from collections import defaultdict
from datetime import datetime
from typing import List, Tuple, Dict, Any, Optional

import numpy as np
from tqdm import tqdm
import pandas as pd
import sys
from pathlib import Path

# æ·»åŠ é …ç›®æ ¹ç›®éŒ„
project_root = Path(__file__).parent.parent
if str(project_root) not in sys.path:
    sys.path.insert(0, str(project_root))

from src.utils.yaml_manager import YAMLManager

# ã€æ–°å¢ã€‘å°å…¥å°ˆæ¥­é‡‘èå·¥ç¨‹å‡½æ•¸åº«ï¼ˆ2025-10-23ï¼‰
from src.utils.financial_engineering import trend_labels_adaptive

# è¨­å®šç‰ˆæœ¬è™Ÿ
VERSION = "7.1.0-no-cross-file"

# è¨­å®šæ—¥èªŒ
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)

warnings.filterwarnings('ignore', category=FutureWarning)
warnings.filterwarnings('ignore', category=RuntimeWarning)

# å›ºå®šå¸¸æ•¸
SEQ_LEN = 100

# å…¨åŸŸçµ±è¨ˆ
global_stats = {
    "loaded_npz_files": 0,
    "symbols_passed_filter": 0,
    "symbols_filtered_out": 0,
    "data_quality_errors": 0,
    "valid_windows": 0,
    "labels_from_npz": 0,  # V7: å¾ NPZ è®€å–æ¨™ç±¤
    "weights_from_metadata": 0,  # V7: å¾ metadata è®€å–æ¬Šé‡
}


# ============================================================
# ============================================================
# V7 æ–°å¢ï¼šJSON è®€å–èˆ‡æ•¸æ“šé¸æ“‡
# ============================================================

def read_dataset_selection_json(json_path: str) -> Optional[Dict]:
    """
    è®€å– dataset_selection.json

    Args:
        json_path: JSON æ–‡ä»¶è·¯å¾‘

    Returns:
        Dict with 'metadata' and 'file_list', or None if error
    """
    try:
        if not os.path.exists(json_path):
            logging.error(f"âŒ JSON æ–‡ä»¶ä¸å­˜åœ¨: {json_path}")
            return None

        with open(json_path, 'r', encoding='utf-8') as f:
            data = json.load(f)

        # é©—è­‰å¿…è¦å­—æ®µ
        if 'file_list' not in data:
            logging.error(f"âŒ JSON ç¼ºå°‘ 'file_list' å­—æ®µ")
            return None

        if not isinstance(data['file_list'], list):
            logging.error(f"âŒ 'file_list' å¿…é ˆæ˜¯åˆ—è¡¨")
            return None

        logging.info(f"âœ… æˆåŠŸè®€å– JSON: {len(data['file_list'])} å€‹æ–‡ä»¶")
        if 'metadata' in data:
            logging.info(f"   ç¸½æ¨£æœ¬æ•¸: {data['metadata'].get('total_samples', 'N/A')}")

        return data

    except json.JSONDecodeError as e:
        logging.error(f"âŒ JSON æ ¼å¼éŒ¯èª¤: {e}")
        return None
    except Exception as e:
        logging.error(f"âŒ è®€å– JSON å¤±æ•—: {e}")
        return None


def filter_data_by_selection(
    all_data: List[Tuple[str, str, np.ndarray, np.ndarray, Dict]],
    config: Dict,
    json_file_override: Optional[str] = None
) -> List[Tuple[str, str, np.ndarray, np.ndarray, Dict]]:
    """
    æ ¹æ“šé…ç½®éæ¿¾æ•¸æ“š

    â­ æ³¨æ„ï¼šå¦‚æœæ•¸æ“šå·²ç¶“åœ¨ load_all_preprocessed_data ä¸­æŒ‰ JSON è¼‰å…¥ï¼Œ
             å‰‡æ­¤å‡½æ•¸æœƒè·³é JSON éæ¿¾ï¼ˆé¿å…é‡è¤‡éæ¿¾ï¼‰

    å„ªå…ˆç´š:
      1. ä½¿ç”¨ --json å‘½ä»¤åƒæ•¸ï¼ˆæœ€é«˜å„ªå…ˆç´šï¼‰
      2. ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„ dataset_selection.json
      3. ä½¿ç”¨é…ç½®éæ¿¾ï¼ˆstart_date, num_days, symbolsï¼‰

    Args:
        all_data: [(date, symbol, features, labels, meta), ...]
        config: é…ç½®å­—å…¸
        json_file_override: å‘½ä»¤åƒæ•¸æŒ‡å®šçš„ JSON æª”æ¡ˆï¼ˆå„ªå…ˆæ–¼é…ç½®æ–‡ä»¶ï¼‰

    Returns:
        éæ¿¾å¾Œçš„æ•¸æ“šåˆ—è¡¨
    """
    data_selection = config.get('data_selection', {})

    # å„ªå…ˆç´š 1: ä½¿ç”¨å‘½ä»¤åƒæ•¸æŒ‡å®šçš„ JSON æ–‡ä»¶
    json_file = json_file_override or data_selection.get('json_file')
    if json_file:
        # â­ æª¢æŸ¥æ•¸æ“šæ˜¯å¦å·²ç¶“æŒ‰ JSON è¼‰å…¥ï¼ˆé¿å…é‡è¤‡éæ¿¾ï¼‰
        # å¦‚æœæ•¸æ“šé‡å·²ç¶“ç­‰æ–¼ JSON ä¸­çš„æ–‡ä»¶æ•¸ï¼Œèªªæ˜å·²ç¶“éæ¿¾éäº†
        json_data = read_dataset_selection_json(json_file)

        if json_data is None:
            logging.warning(f"âš ï¸ JSON è®€å–å¤±æ•—ï¼Œå›é€€åˆ°é…ç½®éæ¿¾")
        else:
            json_file_count = len(json_data['file_list'])

            # â­ å¦‚æœæ•¸æ“šé‡åŒ¹é…ï¼Œèªªæ˜å·²ç¶“åœ¨è¼‰å…¥æ™‚éæ¿¾éäº†
            if len(all_data) == json_file_count:
                logging.info(f"âœ… æ•¸æ“šå·²æŒ‰ JSON è¼‰å…¥ï¼ˆ{len(all_data)} å€‹æ–‡ä»¶ï¼‰ï¼Œè·³éé‡è¤‡éæ¿¾")
                return all_data

            # å¦å‰‡ï¼Œé€²è¡Œ JSON éæ¿¾ï¼ˆèˆŠæ¨¡å¼å…¼å®¹ï¼‰
            logging.info(f"ğŸ“‹ ä½¿ç”¨ dataset_selection.json éæ¿¾: {json_file}")
            selected_files = {(item['date'], item['symbol']) for item in json_data['file_list']}

            filtered_data = [
                item for item in all_data
                if (item[0], item[1]) in selected_files
            ]

            logging.info(f"âœ… JSON éæ¿¾: {len(all_data)} â†’ {len(filtered_data)} å€‹æ–‡ä»¶")
            return filtered_data

    # å„ªå…ˆç´š 2: ä½¿ç”¨é…ç½®éæ¿¾
    logging.info(f"ğŸ“‹ ä½¿ç”¨é…ç½®éæ¿¾")

    # 1. æ—¥æœŸéæ¿¾
    start_date = data_selection.get('start_date')
    end_date = data_selection.get('end_date')
    num_days = data_selection.get('num_days')

    if start_date or end_date or num_days:
        # æå–æ‰€æœ‰æ—¥æœŸä¸¦æ’åº
        all_dates = sorted(set(item[0] for item in all_data))

        if start_date:
            all_dates = [d for d in all_dates if d >= start_date]
        if end_date:
            all_dates = [d for d in all_dates if d <= end_date]
        if num_days and num_days > 0:
            all_dates = all_dates[:num_days]

        selected_dates = set(all_dates)
        all_data = [item for item in all_data if item[0] in selected_dates]

        logging.info(f"   æ—¥æœŸéæ¿¾: ä¿ç•™ {len(selected_dates)} å¤©")

    # 2. è‚¡ç¥¨éæ¿¾
    symbols = data_selection.get('symbols')
    if symbols:
        all_data = [item for item in all_data if item[1] in symbols]
        logging.info(f"   è‚¡ç¥¨éæ¿¾: ä¿ç•™ {len(symbols)} æª”")

    # 3. éš¨æ©Ÿæ¡æ¨£
    sample_ratio = data_selection.get('sample_ratio', 1.0)
    if sample_ratio < 1.0:
        np.random.seed(data_selection.get('random_seed', 42))
        n_samples = int(len(all_data) * sample_ratio)
        indices = np.random.choice(len(all_data), n_samples, replace=False)
        all_data = [all_data[i] for i in sorted(indices)]
        logging.info(f"   éš¨æ©Ÿæ¡æ¨£: {sample_ratio:.1%} â†’ {n_samples} å€‹æ–‡ä»¶")

    logging.info(f"âœ… é…ç½®éæ¿¾å®Œæˆ: {len(all_data)} å€‹æ–‡ä»¶")
    return all_data


def make_sample_weight(ret: pd.Series,
                      tt: pd.Series,
                      y: pd.Series,
                      tau: float = 100.0,
                      scale: float = 10.0,
                      balance: bool = True,
                      use_log_scale: bool = True) -> pd.Series:
    """æ¨£æœ¬æ¬Šé‡è¨ˆç®—"""
    from sklearn.utils.class_weight import compute_class_weight

    ret_array = np.array(ret.values, dtype=np.float64)
    tt_array = np.array(tt.values, dtype=np.float64)

    if use_log_scale:
        ret_weight = np.log1p(np.abs(ret_array) * 1000) * scale
        ret_weight = np.maximum(ret_weight, 0.1)
    else:
        ret_weight = np.abs(ret_array) * scale

    time_decay = np.exp(-tt_array / float(tau))
    # C. æ™‚é–“è¡°æ¸›æ¨™æº–åŒ–ï¼šé¿å…æ™šæœŸæ¨£æœ¬æ•´é«”è¢«å£“ä½
    time_decay = time_decay / (time_decay.mean() + 1e-12)

    base = ret_weight * time_decay
    base = np.clip(base, 0.05, None)

    if balance:
        classes = np.array(sorted(y.unique()))
        y_array = np.array(y.values, dtype=np.int64)
        cls_w = compute_class_weight('balanced', classes=classes, y=y_array)

        cls_w = np.clip(cls_w, 0.5, 3.0)
        cls_w = cls_w / cls_w.mean()

        w_map = dict(zip(classes, cls_w))
        cw = np.array(y.map(w_map).values, dtype=np.float64)
        w = base * cw
    else:
        w = base

    w = w / np.mean(w)
    w = np.clip(w, 0.1, 5.0)

    # è£å‰ªå¾Œé‡æ–°æ­¸ä¸€åŒ–ï¼Œç¢ºä¿å‡å€¼=1.0
    w = w / np.mean(w)

    return pd.Series(w, index=y.index)


def zscore_fit(X: np.ndarray, method: str = 'global', window: int = 100, min_periods: int = 20) -> Tuple[np.ndarray, np.ndarray]:
    """
    è¨ˆç®— Z-Score åƒæ•¸ï¼ˆæ”¯æŒå¤šç¨®æ¨™æº–åŒ–æ–¹æ³•ï¼‰

    Parameters:
    -----------
    X : np.ndarray, shape (n_samples, n_features)
        è¼¸å…¥ç‰¹å¾µ
    method : str
        æ¨™æº–åŒ–æ–¹æ³•ï¼š
        - 'global': å…¨å±€çµ±è¨ˆé‡ï¼ˆèˆŠæ–¹æ³•ï¼Œå¯èƒ½å°è‡´åˆ†å¸ƒæ¼‚ç§»ï¼‰
        - 'rolling_zscore': æ»¾å‹•çª—å£ Z-Scoreï¼ˆæ¨è–¦ï¼Œé©æ‡‰å¸‚å ´è®ŠåŒ–ï¼‰
    window : int
        æ»¾å‹•çª—å£å¤§å°ï¼ˆåƒ… rolling_zscore ä½¿ç”¨ï¼‰
    min_periods : int
        æœ€å°æœ‰æ•ˆæ¨£æœ¬æ•¸ï¼ˆåƒ… rolling_zscore ä½¿ç”¨ï¼‰

    Returns:
    --------
    mu, sd : å‡å€¼å’Œæ¨™æº–å·®ï¼ˆglobal æ–¹æ³•ï¼‰æˆ– None, Noneï¼ˆrolling æ–¹æ³•ï¼‰

    Notes:
    ------
    - å…¨å±€æ–¹æ³•ï¼šä½¿ç”¨æ•´å€‹è¨“ç·´é›†çµ±è¨ˆé‡ï¼ˆå¿«é€Ÿä½†å¯èƒ½æ¼‚ç§»ï¼‰
    - æ»¾å‹•æ–¹æ³•ï¼šä½¿ç”¨æœ€è¿‘ N å€‹æ¨£æœ¬çµ±è¨ˆé‡ï¼ˆç©©å®šä½†ç¨æ…¢ï¼‰
    """
    if method == 'global':
        # åŸæœ‰çš„å…¨å±€æ¨™æº–åŒ–
        mu = X.mean(axis=0)
        sd = X.std(axis=0, ddof=0)
        sd = np.where(sd < 1e-8, 1.0, sd)

        if np.any(np.abs(mu) > 1e6):
            logging.warning(f"åµæ¸¬åˆ°ç•°å¸¸å¤§çš„å‡å€¼: max|Î¼|={np.max(np.abs(mu)):.2f}")

        return mu, sd

    elif method == 'rolling_zscore':
        # æ»¾å‹•çª—å£æ¨™æº–åŒ–ä¸éœ€è¦é å…ˆè¨ˆç®—çµ±è¨ˆé‡
        # çµ±è¨ˆé‡åœ¨ zscore_apply ä¸­å¯¦æ™‚è¨ˆç®—
        logging.info(f"ä½¿ç”¨æ»¾å‹•çª—å£æ¨™æº–åŒ–: window={window}, min_periods={min_periods}")
        return None, None

    else:
        raise ValueError(f"æœªçŸ¥çš„æ¨™æº–åŒ–æ–¹æ³•: {method}")


def zscore_apply(X: np.ndarray, mu: np.ndarray, sd: np.ndarray,
                 method: str = 'global', window: int = 100, min_periods: int = 20) -> np.ndarray:
    """
    æ‡‰ç”¨ Z-Score æ­£è¦åŒ–

    Parameters:
    -----------
    X : np.ndarray, shape (n_samples, n_features)
        è¼¸å…¥ç‰¹å¾µ
    mu : np.ndarray or None
        å‡å€¼ï¼ˆglobal æ–¹æ³•ï¼‰æˆ– Noneï¼ˆrolling æ–¹æ³•ï¼‰
    sd : np.ndarray or None
        æ¨™æº–å·®ï¼ˆglobal æ–¹æ³•ï¼‰æˆ– Noneï¼ˆrolling æ–¹æ³•ï¼‰
    method : str
        æ¨™æº–åŒ–æ–¹æ³•
    window : int
        æ»¾å‹•çª—å£å¤§å°
    min_periods : int
        æœ€å°æœ‰æ•ˆæ¨£æœ¬æ•¸

    Returns:
    --------
    normalized : np.ndarray
        æ¨™æº–åŒ–å¾Œçš„ç‰¹å¾µ
    """
    if method == 'global':
        # å…¨å±€æ¨™æº–åŒ–
        return (X - mu.reshape(1, -1)) / sd.reshape(1, -1)

    elif method == 'rolling_zscore':
        # æ»¾å‹•çª—å£æ¨™æº–åŒ–
        n_samples, n_features = X.shape
        normalized = np.zeros_like(X)

        for i in range(n_samples):
            # ç¢ºå®šæ»¾å‹•çª—å£ç¯„åœ
            start_idx = max(0, i - window + 1)
            window_data = X[start_idx:i+1, :]

            # è¨ˆç®—ç•¶å‰çª—å£çš„çµ±è¨ˆé‡
            if len(window_data) >= min_periods:
                mu_rolling = window_data.mean(axis=0)
                sd_rolling = window_data.std(axis=0, ddof=0)
                sd_rolling = np.where(sd_rolling < 1e-8, 1.0, sd_rolling)
            else:
                # warm-up æœŸï¼šä½¿ç”¨ expanding window
                mu_rolling = window_data.mean(axis=0)
                sd_rolling = window_data.std(axis=0, ddof=0)
                sd_rolling = np.where(sd_rolling < 1e-8, 1.0, sd_rolling)

            # æ¨™æº–åŒ–ç•¶å‰æ¨£æœ¬
            normalized[i, :] = (X[i, :] - mu_rolling) / sd_rolling

        return normalized

    else:
        raise ValueError(f"æœªçŸ¥çš„æ¨™æº–åŒ–æ–¹æ³•: {method}")


# ============================================================
# V6 æ ¸å¿ƒï¼šè¼‰å…¥é è™•ç†æ•¸æ“š
# ============================================================

def validate_preprocessed_data(features: np.ndarray, mids: np.ndarray, meta: Dict, npz_path: str) -> bool:
    """
    é©—è­‰é è™•ç†æ•¸æ“šè³ªé‡ï¼ˆä¸ä¿®è£œï¼Œåªæª¢æŸ¥ï¼‰

    Returns:
        True if valid, False if invalid (æœƒè¨˜éŒ„è­¦å‘Š)
    """
    symbol = meta.get('symbol', 'unknown')
    date = meta.get('date', 'unknown')

    # æª¢æŸ¥ 1: mids ä¸èƒ½ç‚º 0
    if (mids == 0).any():
        zero_count = (mids == 0).sum()
        zero_pct = zero_count / len(mids) * 100
        logging.error(
            f"âŒ æ•¸æ“šè³ªé‡éŒ¯èª¤ [{symbol} @ {date}]\n"
            f"   ç™¼ç¾ {zero_count} å€‹ mids=0 ({zero_pct:.1f}%)\n"
            f"   æª”æ¡ˆ: {npz_path}\n"
            f"   â†’ é è™•ç†éšæ®µ (preprocess_single_day.py) æ‡‰è©²ç§»é™¤é€™äº›é»ï¼"
        )
        return False

    # æª¢æŸ¥ 2: mids ä¸èƒ½ç‚º NaN
    if np.isnan(mids).any():
        nan_count = np.isnan(mids).sum()
        nan_pct = nan_count / len(mids) * 100
        logging.error(
            f"âŒ æ•¸æ“šè³ªé‡éŒ¯èª¤ [{symbol} @ {date}]\n"
            f"   ç™¼ç¾ {nan_count} å€‹ NaN ({nan_pct:.1f}%)\n"
            f"   æª”æ¡ˆ: {npz_path}"
        )
        return False

    # æª¢æŸ¥ 3: mids ä¸èƒ½ç‚ºè² æ•¸
    if (mids < 0).any():
        neg_count = (mids < 0).sum()
        logging.error(
            f"âŒ æ•¸æ“šè³ªé‡éŒ¯èª¤ [{symbol} @ {date}]\n"
            f"   ç™¼ç¾ {neg_count} å€‹è² åƒ¹æ ¼\n"
            f"   æª”æ¡ˆ: {npz_path}"
        )
        return False

    # æª¢æŸ¥ 4: features ä¸èƒ½ç‚º NaN
    if np.isnan(features).any():
        nan_count = np.isnan(features).sum()
        logging.error(
            f"âŒ æ•¸æ“šè³ªé‡éŒ¯èª¤ [{symbol} @ {date}]\n"
            f"   ç™¼ç¾ {nan_count} å€‹ NaN ç‰¹å¾µå€¼\n"
            f"   æª”æ¡ˆ: {npz_path}"
        )
        return False

    # æª¢æŸ¥ 5: å½¢ç‹€åŒ¹é…
    if features.shape[0] != len(mids):
        logging.error(
            f"âŒ æ•¸æ“šè³ªé‡éŒ¯èª¤ [{symbol} @ {date}]\n"
            f"   features å’Œ mids é•·åº¦ä¸åŒ¹é…: {features.shape[0]} vs {len(mids)}\n"
            f"   æª”æ¡ˆ: {npz_path}"
        )
        return False

    return True


def load_preprocessed_npz(npz_path: str) -> Optional[Tuple[np.ndarray, np.ndarray, Dict, np.ndarray, np.ndarray, np.ndarray]]:
    """
    è¼‰å…¥é è™•ç†å¾Œçš„ NPZ æª”æ¡ˆï¼ˆV7 ç‰ˆæœ¬ï¼šè¿”å› labels + åƒ¹æ ¼/æˆäº¤é‡ï¼‰

    Returns:
        (features, labels, metadata, last_prices, last_volumes, total_volumes) or None if filtered/invalid

    V7 æ”¹å‹•:
        - è¿”å› labels è€Œé mids/bucket_mask
        - å¼·åˆ¶è¦æ±‚ NPZ v2.0+ï¼ˆå¿…é ˆæœ‰ labels å­—æ®µï¼‰
        - â­ NEW: è¿”å›åƒ¹æ ¼å’Œæˆäº¤é‡æ•¸æ“šï¼ˆå‘å¾Œå…¼å®¹ï¼Œå¯ç‚º Noneï¼‰
    """
    try:
        # â­ é¡¯ç¤ºæ­£åœ¨è®€å–çš„æª”æ¡ˆ
        logging.debug(f"è®€å– NPZ: {npz_path}")

        data = np.load(npz_path, allow_pickle=True)

        # V7 ç‰ˆæœ¬æª¢æŸ¥ï¼šå¿…é ˆæœ‰ labels å­—æ®µ
        if 'labels' not in data:
            logging.error(
                f"âŒ NPZ ç‰ˆæœ¬éèˆŠï¼ˆv1.0ï¼‰: {npz_path}\n"
                f"   V7 è¦æ±‚ v2.0+ NPZï¼ˆå« labels å­—æ®µï¼‰\n"
                f"   è§£æ±ºæ–¹æ³•:\n"
                f"   1. é‹è¡Œ: scripts\\batch_preprocess.bat\n"
                f"   2. ç¢ºä¿ NPZ å«æœ‰ 'labels' å­—æ®µ"
            )
            return None

        features = data['features']  # (T, 20)
        labels = data['labels']      # (T,) with values {-1, 0, 1} or {0, 1}
        mids = data.get('mids', np.zeros(len(labels)))  # åƒ…ç”¨æ–¼é©—è­‰

        # â­ NEW: è®€å–åƒ¹æ ¼å’Œæˆäº¤é‡å­—æ®µï¼ˆå‘å¾Œå…¼å®¹ï¼‰
        last_prices = data.get('last_prices', None)
        last_volumes = data.get('last_volumes', None)
        total_volumes = data.get('total_volumes', None)

        # â­ æª¢æŸ¥ä¸¦å ±å‘Šç¼ºå°‘çš„æ¬„ä½
        if last_prices is None or last_volumes is None or total_volumes is None:
            logging.warning(
                f"âš ï¸ ç¼ºå°‘åƒ¹æ ¼/æˆäº¤é‡æ¬„ä½: {npz_path}\n"
                f"   last_prices: {'âœ“' if last_prices is not None else 'âœ—'}\n"
                f"   last_volumes: {'âœ“' if last_volumes is not None else 'âœ—'}\n"
                f"   total_volumes: {'âœ“' if total_volumes is not None else 'âœ—'}\n"
                f"   â†’ æ­¤æª”æ¡ˆéœ€è¦é‡æ–°é è™•ç†"
            )

        meta = json.loads(str(data['metadata']))

        global_stats["loaded_npz_files"] += 1
        global_stats["labels_from_npz"] += 1  # V7 çµ±è¨ˆ

        # æª¢æŸ¥éæ¿¾ç‹€æ…‹
        if not meta['pass_filter']:
            global_stats["symbols_filtered_out"] += 1
            return None

        # æ•¸æ“šè³ªé‡é©—è­‰ï¼ˆV7 ç°¡åŒ–ç‰ˆï¼‰
        if not validate_preprocessed_data(features, mids, meta, npz_path):
            logging.warning(f"âš ï¸ è·³éæœ‰å•é¡Œçš„æ•¸æ“š: {npz_path}")
            global_stats["data_quality_errors"] += 1
            global_stats["symbols_filtered_out"] += 1
            return None

        # V7 é¡å¤–é©—è­‰ï¼šæª¢æŸ¥ labels
        if len(labels) != len(features):
            logging.error(f"âŒ labels é•·åº¦ä¸åŒ¹é…: {npz_path}")
            return None

        unique_labels = np.unique(labels)
        if not all(label in [-1, 0, 1, 0.0, 1.0] for label in unique_labels):
            logging.error(f"âŒ labels åŒ…å«ç•°å¸¸å€¼ {unique_labels}: {npz_path}")
            return None

        global_stats["symbols_passed_filter"] += 1

        # â­ NEW: è¿”å›å¢å¼·æ•¸æ“šï¼ˆåŒ…å«åƒ¹æ ¼å’Œæˆäº¤é‡ï¼‰
        return features, labels, meta, last_prices, last_volumes, total_volumes

    except Exception as e:
        logging.warning(f"ç„¡æ³•è¼‰å…¥ {npz_path}: {e}")
        return None


def load_all_preprocessed_data(
    preprocessed_dir: str,
    config: Dict = None,
    json_file_override: Optional[str] = None
) -> List[Tuple[str, str, np.ndarray, np.ndarray, Dict, np.ndarray, np.ndarray, np.ndarray]]:
    """
    è¼‰å…¥é è™•ç†æ•¸æ“šï¼ˆV7 ç‰ˆæœ¬ï¼šè¿”å› labels + åƒ¹æ ¼/æˆäº¤é‡ï¼‰

    â­ å„ªåŒ–ï¼šå¦‚æœæŒ‡å®š JSONï¼Œåªè¼‰å…¥ JSON ä¸­çš„æª”æ¡ˆï¼ˆé¿å…æƒæå…¨éƒ¨æª”æ¡ˆï¼‰

    Returns:
        List[(date, symbol, features, labels, metadata, last_prices, last_volumes, total_volumes)]

    V7 æ”¹å‹•:
        - è¿”å› labels è€Œé mids/bucket_mask
        - â­ NEW: è¿”å›åƒ¹æ ¼å’Œæˆäº¤é‡æ•¸æ“š
        - â­ NEW: æ”¯æŒå¾ JSON ç›´æ¥è¼‰å…¥æŒ‡å®šæª”æ¡ˆ
    """
    daily_dir = os.path.join(preprocessed_dir, "daily")

    if not os.path.exists(daily_dir):
        logging.error(f"é è™•ç†ç›®éŒ„ä¸å­˜åœ¨: {daily_dir}")
        return []

    all_data = []

    # â­ NEW: å„ªå…ˆæª¢æŸ¥æ˜¯å¦æœ‰ JSON æ–‡ä»¶ï¼ˆé«˜æ•ˆæ¨¡å¼ï¼‰
    file_list_to_load = None
    if config:
        data_selection = config.get('data_selection', {})
        json_file = json_file_override or data_selection.get('json_file')

        if json_file:
            logging.info(f"ğŸ“‹ ä½¿ç”¨ JSON æ–‡ä»¶ç›´æ¥è¼‰å…¥: {json_file}")
            json_data = read_dataset_selection_json(json_file)

            if json_data:
                file_list_to_load = [(item['date'], item['symbol']) for item in json_data['file_list']]
                logging.info(f"âœ… JSON æŒ‡å®š {len(file_list_to_load)} å€‹æª”æ¡ˆï¼ˆæŒ‰æ—¥æœŸ+è‚¡ç¥¨ä»£ç¢¼æ’åºï¼‰")

    # æ±ºå®šè¼‰å…¥æ–¹å¼
    if file_list_to_load:
        # â­ é«˜æ•ˆæ¨¡å¼ï¼šåªè¼‰å…¥ JSON æŒ‡å®šçš„æª”æ¡ˆ
        npz_files = []
        for date, symbol in sorted(file_list_to_load):  # æŒ‰æ—¥æœŸ+è‚¡ç¥¨ä»£ç¢¼æ’åº
            npz_path = os.path.join(daily_dir, date, f"{symbol}.npz")
            if os.path.exists(npz_path):
                npz_files.append(npz_path)
            else:
                logging.warning(f"âš ï¸ JSON æŒ‡å®šçš„æª”æ¡ˆä¸å­˜åœ¨: {npz_path}")

        logging.info(f"é–‹å§‹è¼‰å…¥ {len(npz_files)} å€‹ JSON æŒ‡å®šçš„ NPZ æª”æ¡ˆ...")
    else:
        # âš ï¸ èˆŠæ¨¡å¼ï¼šæƒææ‰€æœ‰ NPZ æª”æ¡ˆï¼ˆä½æ•ˆï¼‰
        npz_files = sorted(glob.glob(os.path.join(daily_dir, "*", "*.npz")))
        logging.info(f"âš ï¸ æœªä½¿ç”¨ JSONï¼Œæƒææ‰€æœ‰ NPZ æª”æ¡ˆ: {len(npz_files)} å€‹")
        logging.info(f"   æç¤ºï¼šä½¿ç”¨ dataset_selection.json å¯å¤§å¹…æå‡è¼‰å…¥é€Ÿåº¦")

    for npz_file in tqdm(npz_files, desc="è¼‰å…¥ NPZ", unit="æª”"):
        result = load_preprocessed_npz(npz_file)

        if result is None:
            continue

        # â­ NEW: è§£åŒ…å¢å¼·æ•¸æ“šï¼ˆåŒ…å«åƒ¹æ ¼å’Œæˆäº¤é‡ï¼‰
        features, labels, meta, last_prices, last_volumes, total_volumes = result
        date = meta['date']
        symbol = meta['symbol']

        # â­ NEW: å„²å­˜å®Œæ•´æ•¸æ“šï¼ˆåŒ…å«åƒ¹æ ¼å’Œæˆäº¤é‡ï¼‰
        all_data.append((date, symbol, features, labels, meta, last_prices, last_volumes, total_volumes))

    # G.1: ç©ºæ•¸æ“šå ±å‘Šå¢å¼·
    if len(all_data) == 0:
        logging.error(f"\nâŒ æ²’æœ‰å¯ç”¨çš„é è™•ç†æ•¸æ“šï¼")
        logging.error(f"å¯èƒ½åŸå› :")
        logging.error(f"  1. preprocessed_dir è·¯å¾‘éŒ¯èª¤: {preprocessed_dir}")
        logging.error(f"  2. æ‰€æœ‰è‚¡ç¥¨è¢«éæ¿¾ï¼ˆpass_filter=falseï¼‰")
        logging.error(f"  3. æ•¸æ“šè³ªé‡éŒ¯èª¤ï¼ˆmids=0, NaN ç­‰ï¼‰")
        logging.error(f"  4. é è™•ç†å°šæœªåŸ·è¡Œ")
        logging.error(f"\nå»ºè­°æª¢æŸ¥:")
        logging.error(f"  - ç¢ºèªé è™•ç†ç›®éŒ„å­˜åœ¨: {os.path.exists(daily_dir)}")
        logging.error(f"  - æª¢æŸ¥ NPZ æª”æ¡ˆæ•¸é‡: {len(list(glob.glob(os.path.join(daily_dir, '*', '*.npz'))))}")
        logging.error(f"  - æŸ¥çœ‹é è™•ç†æ—¥èªŒæˆ– summary.json")
    else:
        logging.info(f"è¼‰å…¥äº† {len(all_data)} å€‹ symbol-day çµ„åˆï¼ˆé€šééæ¿¾ï¼‰")
        logging.info(f"éæ¿¾æ‰: {global_stats['symbols_filtered_out']} å€‹")
        if global_stats['data_quality_errors'] > 0:
            logging.warning(f"æ•¸æ“šè³ªé‡éŒ¯èª¤: {global_stats['data_quality_errors']} å€‹ï¼ˆå·²è·³éï¼‰")

    return all_data


# ============================================================
# V6 æ»‘çª—æµç¨‹ï¼ˆç°¡åŒ–ç‰ˆï¼Œæ•¸æ“šå·²æ¸…æ´—ï¼‰
# ============================================================

def sliding_windows_v7(
    preprocessed_data: List[Tuple[str, str, np.ndarray, np.ndarray, Dict, np.ndarray, np.ndarray, np.ndarray]],
    out_dir: str,
    config: Dict[str, Any],
    json_file: Optional[str] = None
):
    """
    V7 ç°¡åŒ–ç‰ˆæ»‘çª—æµç¨‹ï¼ˆå°ˆæ³¨æ•¸æ“šçµ„ç¹”ï¼Œä¸é‡è¤‡è¨ˆç®—ï¼‰

    V7 ç°¡åŒ–æ”¹å‹•ï¼š
    - è¼¸å…¥: (date, symbol, features, labels, meta, last_prices, last_volumes, total_volumes) â­ NEW
    - ä¸é‡æ–°è¨ˆç®—æ¨™ç±¤ï¼ˆç›´æ¥ä½¿ç”¨ NPZ çš„ labelsï¼‰
    - ä¸é‡æ–°è¨ˆç®—æ³¢å‹•ç‡ï¼ˆä¸éœ€è¦ï¼‰
    - ä¸é‡æ–°è¨ˆç®—æ¬Šé‡ï¼ˆå¾ metadata è®€å–ï¼‰
    - å°ˆæ³¨æ–¼æ»‘å‹•çª—å£ç”Ÿæˆå’Œæ•¸æ“šåŠƒåˆ†

    Args:
        json_file: å‘½ä»¤åƒæ•¸æŒ‡å®šçš„ JSON æª”æ¡ˆè·¯å¾‘ï¼ˆå„ªå…ˆæ–¼é…ç½®æ–‡ä»¶ï¼‰
    """
    global global_stats
    
    logging.info("=" * 80)
    logging.info("V7 ç°¡åŒ–ç‰ˆæ»‘çª—æµç¨‹é–‹å§‹")
    logging.info("=" * 80)

    if not preprocessed_data:
        logging.warning("æ²’æœ‰è³‡æ–™å¯ä¾›ç”¢ç”Ÿ .npz æª”æ¡ˆ")
        return

    # æ­¥é©Ÿ 0: æ‡‰ç”¨æ•¸æ“šé¸æ“‡éæ¿¾
    preprocessed_data = filter_data_by_selection(preprocessed_data, config, json_file)
    
    if not preprocessed_data:
        logging.error("âŒ éæ¿¾å¾Œç„¡æ•¸æ“š")
        return

    logging.info(f"éæ¿¾å¾Œæ•¸æ“š: {len(preprocessed_data)} å€‹ symbol-day")

    # æ­¥é©Ÿ 1: æŒ‰è‚¡ç¥¨åˆ†çµ„ï¼ˆä¿å­˜ metadata ç”¨æ–¼æ¬Šé‡æå–ï¼‰
    stock_data = defaultdict(list)
    stock_metadata = {}  # æ¯å€‹è‚¡ç¥¨çš„ç¬¬ä¸€å€‹ metadataï¼ˆç”¨æ–¼æ¬Šé‡ç­–ç•¥ï¼‰
    # â­ NEW: è§£åŒ…å¢å¼·æ•¸æ“šï¼ˆåŒ…å«åƒ¹æ ¼å’Œæˆäº¤é‡ï¼‰
    for date, sym, features, labels, meta, last_prices, last_volumes, total_volumes in preprocessed_data:
        # â­ NEW: åŒæ™‚ä¿å­˜åƒ¹æ ¼å’Œæˆäº¤é‡
        stock_data[sym].append((date, features, labels, last_prices, last_volumes, total_volumes))
        if sym not in stock_metadata:
            stock_metadata[sym] = meta  # ä¿å­˜ç¬¬ä¸€å€‹ metadata

    logging.info(f"å…± {len(stock_data)} å€‹è‚¡ç¥¨")

    # æ­¥é©Ÿ 2: æŒ‰è‚¡ç¥¨åŠƒåˆ† train/val/test (70/15/15)
    symbols = sorted(stock_data.keys())
    n_symbols = len(symbols)
    
    n_train = int(n_symbols * 0.70)
    n_val = int(n_symbols * 0.15)
    
    train_symbols = set(symbols[:n_train])
    val_symbols = set(symbols[n_train:n_train + n_val])
    test_symbols = set(symbols[n_train + n_val:])
    
    logging.info(f"åŠƒåˆ†: train={len(train_symbols)}, val={len(val_symbols)}, test={len(test_symbols)}")

    # æ­¥é©Ÿ 3: æå–æ¬Šé‡ç­–ç•¥é…ç½®
    weight_strategy_name = config.get('sample_weights', {}).get('strategy', 'uniform')
    weight_enabled = config.get('sample_weights', {}).get('enabled', True)
    logging.info(f"æ¨£æœ¬æ¬Šé‡: {'enabled' if weight_enabled else 'disabled'}, strategy='{weight_strategy_name}'")

    # â­ NEW: å…¨å±€æª¢æŸ¥ - æ˜¯å¦æ‰€æœ‰è‚¡ç¥¨éƒ½æœ‰åƒ¹æ ¼/æˆäº¤é‡æ•¸æ“š
    all_have_price_data = True
    files_without_prices = []  # è¨˜éŒ„ç¼ºå°‘æ•¸æ“šçš„æª”æ¡ˆ

    for date, sym, features, labels, meta, last_prices, last_volumes, total_volumes in preprocessed_data:
        if last_prices is None or last_volumes is None or total_volumes is None:
            all_have_price_data = False
            files_without_prices.append((date, sym))

    if all_have_price_data:
        logging.info("âœ… æ‰€æœ‰è‚¡ç¥¨éƒ½åŒ…å«åƒ¹æ ¼å’Œæˆäº¤é‡æ•¸æ“š")
    else:
        logging.error(
            f"\n{'='*80}\n"
            f"âŒ ç™¼ç¾ {len(files_without_prices)} å€‹æª”æ¡ˆç¼ºå°‘åƒ¹æ ¼/æˆäº¤é‡æ•¸æ“šï¼\n"
            f"{'='*80}"
        )

        # æŒ‰æ—¥æœŸåˆ†çµ„é¡¯ç¤º
        dates_missing = defaultdict(list)
        for date, sym in files_without_prices:
            dates_missing[date].append(sym)

        logging.error(f"\nç¼ºå°‘æ•¸æ“šçš„æ—¥æœŸå’Œè‚¡ç¥¨:")
        for date in sorted(dates_missing.keys())[:10]:  # åªé¡¯ç¤ºå‰ 10 å€‹æ—¥æœŸ
            symbols = dates_missing[date]
            logging.error(f"  æ—¥æœŸ {date}: {len(symbols)} æª”è‚¡ç¥¨")
            logging.error(f"    è‚¡ç¥¨: {', '.join(symbols[:5])}")
            if len(symbols) > 5:
                logging.error(f"    ... é‚„æœ‰ {len(symbols) - 5} å€‹")

        if len(dates_missing) > 10:
            logging.error(f"  ... é‚„æœ‰ {len(dates_missing) - 10} å€‹æ—¥æœŸ")

        logging.error(
            f"\n{'='*80}\n"
            f"è§£æ±ºæ–¹æ¡ˆï¼š\n"
            f"  1. é‡æ–°é‹è¡Œé è™•ç†ï¼ˆåŒ…å«åƒ¹æ ¼/æˆäº¤é‡æ¬„ä½ï¼‰ï¼š\n"
            f"     scripts\\batch_preprocess.bat\n"
            f"\n"
            f"  2. æˆ–è€…åƒ…é‡æ–°è™•ç†ç¼ºå°‘çš„æ—¥æœŸï¼š\n"
        )
        for date in sorted(dates_missing.keys())[:3]:
            logging.error(
                f"     python scripts/preprocess_single_day.py "
                f"--input data/raw/tw_stock/{date}.txt "
                f"--output-dir data/preprocessed_v5 "
                f"--config configs/config_pro_v5_ml_optimal.yaml"
            )

        logging.error(f"\n{'='*80}\n")
        logging.warning("âš ï¸ å°‡ä¸ä¿å­˜ prices/volumes åˆ°æœ€çµ‚ NPZï¼ˆå‘å¾Œå…¼å®¹æ¨¡å¼ï¼‰")

    # æ­¥é©Ÿ 4: ç”Ÿæˆæ»‘å‹•çª—å£ï¼ˆåŒ…å« weights å’Œ stock_idsï¼‰
    train_X, train_y, train_weights, train_stock_ids = [], [], [], []
    val_X, val_y, val_weights, val_stock_ids = [], [], [], []
    test_X, test_y, test_weights, test_stock_ids = [], [], [], []

    # â­ NEW: åƒ¹æ ¼å’Œæˆäº¤é‡çª—å£ï¼ˆåªæœ‰åœ¨æ‰€æœ‰è‚¡ç¥¨éƒ½æœ‰æ™‚æ‰ä½¿ç”¨ï¼‰
    train_prices, train_volumes = [], []
    val_prices, val_volumes = [], []
    test_prices, test_volumes = [], []

    # ğŸ†• æ¨™æº–åŒ–é…ç½®æ—¥èªŒ
    norm_config = config.get('normalization', {})
    norm_method = norm_config.get('method', 'rolling_zscore')
    logging.info(f"æ¨™æº–åŒ–æ–¹æ³•: {norm_method}")
    if norm_method == 'rolling_zscore':
        logging.info(f"  - window: {norm_config.get('window', 100)}")
        logging.info(f"  - min_periods: {norm_config.get('min_periods', 20)}")

    logging.info(f"é–‹å§‹ç”Ÿæˆæ»‘å‹•çª—å£ï¼ˆ{len(symbols)} æª”è‚¡ç¥¨ï¼‰...")
    for sym in tqdm(symbols, desc="ç”Ÿæˆæ»‘çª—", unit="è‚¡"):
        # æå–è©²è‚¡ç¥¨çš„æ¬Šé‡ç­–ç•¥
        meta = stock_metadata.get(sym, {})
        weight_strategies = meta.get('weight_strategies', {})

        # ç²å–æŒ‡å®šç­–ç•¥çš„æ¬Šé‡
        if weight_enabled and weight_strategy_name in weight_strategies:
            class_weights = weight_strategies[weight_strategy_name].get('class_weights', {})
            weight_down = class_weights.get('-1', 1.0)
            weight_neutral = class_weights.get('0', 1.0)
            weight_up = class_weights.get('1', 1.0)
        else:
            # é»˜èªç„¡æ¬Šé‡
            weight_down = weight_neutral = weight_up = 1.0

        # â­â­â­ V7.1 ä¿®å¾©ï¼šé€æ–‡ä»¶è™•ç†ï¼Œé¿å…æ»‘å‹•çª—å£è·¨è¶Šæ–‡ä»¶é‚Šç•Œï¼ˆ2025-10-25ï¼‰
        # ã€é—œéµä¿®æ”¹ã€‘ä¸å†åˆä½µæ‰€æœ‰å¤©çš„æ•¸æ“šï¼Œè€Œæ˜¯é€å€‹æ–‡ä»¶è™•ç†

        # è®€å–æ¨™æº–åŒ–é…ç½®
        norm_config = config.get('normalization', {})
        norm_method = norm_config.get('method', 'rolling_zscore')
        norm_window = norm_config.get('window', 100)
        norm_min_periods = norm_config.get('min_periods', 20)

        # é€å€‹æ—¥æœŸæ–‡ä»¶è™•ç†ï¼ˆé¿å…è·¨æ–‡ä»¶çª—å£ï¼‰
        file_data_list = sorted(stock_data[sym], key=lambda x: x[0])

        # èª¿è©¦ï¼šé¡¯ç¤ºè©²è‚¡ç¥¨çš„æ–‡ä»¶æ•¸é‡ï¼ˆå¯é¸ï¼‰
        if len(file_data_list) > 0:
            logging.debug(f"{sym}: è™•ç† {len(file_data_list)} å€‹æ—¥æœŸæ–‡ä»¶")

        for date, features, labels, last_prices, last_volumes, total_volumes in file_data_list:
            # æª¢æŸ¥è©²æ–‡ä»¶æ˜¯å¦æœ‰åƒ¹æ ¼/æˆäº¤é‡æ•¸æ“š
            has_price_data = (last_prices is not None and
                            last_volumes is not None and
                            total_volumes is not None)

            # æ‡‰ç”¨æ¨™æº–åŒ–åˆ°ç•¶å‰æ–‡ä»¶
            if norm_method == 'rolling_zscore':
                features_norm = zscore_apply(
                    features,
                    mu=None,
                    sd=None,
                    method='rolling_zscore',
                    window=norm_window,
                    min_periods=norm_min_periods
                )
            elif norm_method == 'global':
                mu, sd = zscore_fit(features, method='global')
                features_norm = zscore_apply(features, mu, sd, method='global')
            else:
                features_norm = features

            # æª¢æŸ¥æ–‡ä»¶é•·åº¦æ˜¯å¦è¶³å¤ ç”Ÿæˆçª—å£
            T = len(features_norm)
            if T < SEQ_LEN:
                # æ•¸æ“šä¸è¶³ 100ï¼Œè·³éæ­¤æ–‡ä»¶
                continue

            # ã€é—œéµã€‘åœ¨å–®å€‹æ–‡ä»¶å…§ç”Ÿæˆæ»‘å‹•çª—å£ï¼ˆç¢ºä¿æ™‚é–“é€£çºŒæ€§ï¼‰
            for i in range(T - SEQ_LEN + 1):  # æ³¨æ„ï¼š+1 ç¢ºä¿æœ€å¾Œä¸€å€‹çª—å£ä¹Ÿè¢«åŒ…å«
                X_window = features_norm[i:i+SEQ_LEN]  # (100, 20)
                y_label = labels[i+SEQ_LEN-1]          # æœ€å¾Œä¸€å€‹æ™‚é–“æ­¥çš„æ¨™ç±¤

                # æå–åƒ¹æ ¼å’Œæˆäº¤é‡çª—å£
                if all_have_price_data and has_price_data:
                    price_window = last_prices[i:i+SEQ_LEN]  # (100,)
                    volume_window = np.column_stack([
                        last_volumes[i:i+SEQ_LEN],
                        total_volumes[i:i+SEQ_LEN]
                    ])  # (100, 2)
                else:
                    price_window = None
                    volume_window = None

                # æ¨™ç±¤è½‰æ› {-1, 0, 1} â†’ {0, 1, 2}ï¼Œä¸¦åˆ†é…æ¬Šé‡
                if y_label == -1:
                    y_label = 0
                    sample_weight = weight_down
                elif y_label == 0:
                    y_label = 1
                    sample_weight = weight_neutral
                elif y_label == 1:
                    y_label = 2
                    sample_weight = weight_up
                else:
                    # è™•ç†ç•°å¸¸æ¨™ç±¤
                    if y_label == 0.0:
                        y_label = 1
                        sample_weight = weight_neutral
                    elif y_label == 1.0:
                        y_label = 2
                        sample_weight = weight_up
                    else:
                        continue

                # åˆ†é…åˆ°å°æ‡‰é›†åˆ
                if sym in train_symbols:
                    train_X.append(X_window)
                    train_y.append(y_label)
                    train_weights.append(sample_weight)
                    train_stock_ids.append(sym)
                    if price_window is not None:
                        train_prices.append(price_window)
                    if volume_window is not None:
                        train_volumes.append(volume_window)

                elif sym in val_symbols:
                    val_X.append(X_window)
                    val_y.append(y_label)
                    val_weights.append(sample_weight)
                    val_stock_ids.append(sym)
                    if price_window is not None:
                        val_prices.append(price_window)
                    if volume_window is not None:
                        val_volumes.append(volume_window)

                else:  # test_symbols
                    test_X.append(X_window)
                    test_y.append(y_label)
                    test_weights.append(sample_weight)
                    test_stock_ids.append(sym)
                    if price_window is not None:
                        test_prices.append(price_window)
                    if volume_window is not None:
                        test_volumes.append(volume_window)

            # çµ±è¨ˆï¼šç•¶å‰æ–‡ä»¶ç”Ÿæˆçš„çª—å£æ•¸é‡
            windows_in_file = T - SEQ_LEN + 1
            global_stats["valid_windows"] += windows_in_file

    # æ­¥é©Ÿ 5: è½‰æ›ç‚º numpy é™£åˆ—ï¼ˆåŒ…å« weights å’Œ stock_idsï¼‰
    train_X = np.array(train_X, dtype=np.float32)  # (N_train, 100, 20)
    train_y = np.array(train_y, dtype=np.int32)
    train_weights = np.array(train_weights, dtype=np.float32)
    train_stock_ids = np.array(train_stock_ids, dtype='<U10')  # Unicode string

    val_X = np.array(val_X, dtype=np.float32)
    val_y = np.array(val_y, dtype=np.int32)
    val_weights = np.array(val_weights, dtype=np.float32)
    val_stock_ids = np.array(val_stock_ids, dtype='<U10')

    test_X = np.array(test_X, dtype=np.float32)
    test_y = np.array(test_y, dtype=np.int32)
    test_weights = np.array(test_weights, dtype=np.float32)
    test_stock_ids = np.array(test_stock_ids, dtype='<U10')

    # â­ NEW: è½‰æ›åƒ¹æ ¼å’Œæˆäº¤é‡é™£åˆ—ï¼ˆåªåœ¨å…¨å±€æª¢æŸ¥é€šéæ™‚ï¼‰
    if all_have_price_data and train_prices:
        # â­ èª¿è©¦ï¼šæª¢æŸ¥æ•¸é‡æ˜¯å¦åŒ¹é…
        logging.info(f"è½‰æ›å‰æª¢æŸ¥:")
        logging.info(f"  train_X: {len(train_X)} å€‹æ¨£æœ¬")
        logging.info(f"  train_prices: {len(train_prices)} å€‹ price_window")
        logging.info(f"  train_volumes: {len(train_volumes)} å€‹ volume_window")

        if len(train_prices) != len(train_X):
            logging.error(f"âŒ æ•¸é‡ä¸åŒ¹é…ï¼train_prices ({len(train_prices)}) != train_X ({len(train_X)})")
            logging.error(f"   é€™è¡¨ç¤ºæŸäº›çª—å£æœ‰ price_windowï¼ŒæŸäº›æ²’æœ‰")
            logging.error(f"   è«‹æª¢æŸ¥ has_price_data å’Œ concat_last_prices çš„é‚è¼¯")
            raise ValueError(f"train_prices å’Œ train_X æ•¸é‡ä¸åŒ¹é…")

        # â­ èª¿è©¦ï¼šæª¢æŸ¥ train_prices ä¸­çš„å…ƒç´ é•·åº¦
        price_lengths = set()
        for i, p in enumerate(train_prices[:min(100, len(train_prices))]):
            if p is None:
                logging.error(f"âŒ æ¨£æœ¬ {i}: price_window ç‚º Noneï¼")
                raise ValueError(f"ç™¼ç¾ None çš„ price_windowï¼ˆæ¨£æœ¬ {i}ï¼‰")
            price_lengths.add(len(p))

        if len(price_lengths) > 1:
            logging.error(f"âŒ ç™¼ç¾ç•°å¸¸ï¼šprice_window é•·åº¦ä¸ä¸€è‡´ï¼")
            logging.error(f"   ç™¼ç¾çš„é•·åº¦: {sorted(price_lengths)}")
            raise ValueError(f"price_window é•·åº¦ä¸ä¸€è‡´: {price_lengths}")

        try:
            train_prices_arr = np.array(train_prices, dtype=np.float64)  # (N_train, 100)
            train_volumes_arr = np.array(train_volumes, dtype=np.int64)  # (N_train, 100, 2)

            val_prices_arr = np.array(val_prices, dtype=np.float64) if val_prices else None
            val_volumes_arr = np.array(val_volumes, dtype=np.int64) if val_volumes else None

            test_prices_arr = np.array(test_prices, dtype=np.float64) if test_prices else None
            test_volumes_arr = np.array(test_volumes, dtype=np.int64) if test_volumes else None

            logging.info(f"âœ… åƒ¹æ ¼å’Œæˆäº¤é‡æ•¸æ“šå·²è½‰æ›:")
            logging.info(f"  Train prices: {train_prices_arr.shape}")
            logging.info(f"  Train volumes: {train_volumes_arr.shape}")
        except ValueError as e:
            logging.error(f"âŒ è½‰æ›å¤±æ•—: {e}")
            raise
    else:
        train_prices_arr = None
        train_volumes_arr = None
        val_prices_arr = None
        val_volumes_arr = None
        test_prices_arr = None
        test_volumes_arr = None
        logging.warning("âš ï¸ æœªä¿å­˜åƒ¹æ ¼å’Œæˆäº¤é‡æ•¸æ“šï¼ˆéƒ¨åˆ†è‚¡ç¥¨ç¼ºå°‘æ•¸æ“šï¼‰")

    logging.info(f"ç”Ÿæˆæ¨£æœ¬:")
    logging.info(f"  Train: {len(train_X)} æ¨£æœ¬")
    logging.info(f"  Val:   {len(val_X)} æ¨£æœ¬")
    logging.info(f"  Test:  {len(test_X)} æ¨£æœ¬")

    # æ¨™ç±¤åˆ†å¸ƒ
    for name, y in [("Train", train_y), ("Val", val_y), ("Test", test_y)]:
        dist = np.bincount(y, minlength=3)
        pct = dist / len(y) * 100 if len(y) > 0 else [0, 0, 0]
        logging.info(f"  {name} åˆ†å¸ƒ: Down={pct[0]:.1f}%, Neutral={pct[1]:.1f}%, Up={pct[2]:.1f}%")

    # æ­¥é©Ÿ 6: ä¿å­˜ NPZï¼ˆåŒ…å« weights å’Œ stock_idsï¼‰
    out_npz_dir = os.path.join(out_dir, 'npz')
    os.makedirs(out_npz_dir, exist_ok=True)

    train_path = os.path.join(out_npz_dir, 'stock_embedding_train.npz')
    val_path = os.path.join(out_npz_dir, 'stock_embedding_val.npz')
    test_path = os.path.join(out_npz_dir, 'stock_embedding_test.npz')

    logging.info("é–‹å§‹ä¿å­˜ NPZ æª”æ¡ˆï¼ˆåŒ…å« weights, stock_ids, prices, volumesï¼‰...")
    datasets = [
        ("train", train_path, train_X, train_y, train_weights, train_stock_ids, train_prices_arr, train_volumes_arr),
        ("val", val_path, val_X, val_y, val_weights, val_stock_ids, val_prices_arr, val_volumes_arr),
        ("test", test_path, test_X, test_y, test_weights, test_stock_ids, test_prices_arr, test_volumes_arr)
    ]

    for name, path, X, y, weights, stock_ids, prices, volumes in tqdm(datasets, desc="ä¿å­˜ NPZ", unit="æª”"):
        # â­ NEW: æ ¹æ“šæ˜¯å¦æœ‰åƒ¹æ ¼/æˆäº¤é‡æ•¸æ“šæ±ºå®šä¿å­˜å…§å®¹
        save_dict = {
            'X': X,
            'y': y,
            'weights': weights,
            'stock_ids': stock_ids
        }

        # æ·»åŠ åƒ¹æ ¼å’Œæˆäº¤é‡ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        if prices is not None:
            save_dict['prices'] = prices
            logging.info(f"  {name}: åŒ…å«åƒ¹æ ¼æ•¸æ“š {prices.shape}")

        if volumes is not None:
            save_dict['volumes'] = volumes
            logging.info(f"  {name}: åŒ…å«æˆäº¤é‡æ•¸æ“š {volumes.shape}")

        np.savez_compressed(path, **save_dict)

    logging.info(f"âœ… ä¿å­˜å®Œæˆ:")
    logging.info(f"   {train_path}")
    logging.info(f"   {val_path}")
    logging.info(f"   {test_path}")
    logging.info("   â­ NEW æ ¼å¼: X, y, weights, stock_ids, prices (å¯é¸), volumes (å¯é¸)")

    # æ­¥é©Ÿ 7: èšåˆæ‰€æœ‰è‚¡ç¥¨çš„æ¬Šé‡ç­–ç•¥ï¼ˆç”¨æ–¼ metadataï¼‰
    logging.info("èšåˆæ¬Šé‡ç­–ç•¥...")
    aggregated_weight_strategies = {}

    # æ”¶é›†æ‰€æœ‰æ¬Šé‡ç­–ç•¥åç¨±
    all_strategy_names = set()
    for meta in stock_metadata.values():
        ws = meta.get('weight_strategies', {})
        all_strategy_names.update(ws.keys())

    # å°æ¯å€‹ç­–ç•¥è¨ˆç®—å¹³å‡æ¬Šé‡
    for strategy_name in all_strategy_names:
        weights_by_class = {'-1': [], '0': [], '1': []}
        strategy_descriptions = []

        for meta in stock_metadata.values():
            ws = meta.get('weight_strategies', {})
            if strategy_name in ws:
                strategy = ws[strategy_name]
                class_weights = strategy.get('class_weights', {})
                weights_by_class['-1'].append(class_weights.get('-1', 1.0))
                weights_by_class['0'].append(class_weights.get('0', 1.0))
                weights_by_class['1'].append(class_weights.get('1', 1.0))
                if 'description' in strategy and strategy['description'] not in strategy_descriptions:
                    strategy_descriptions.append(strategy['description'])

        # è¨ˆç®—å¹³å‡æ¬Šé‡
        if weights_by_class['-1']:  # ç¢ºä¿æœ‰æ•¸æ“š
            avg_weights = {
                '-1': float(np.mean(weights_by_class['-1'])),
                '0': float(np.mean(weights_by_class['0'])),
                '1': float(np.mean(weights_by_class['1']))
            }

            aggregated_weight_strategies[strategy_name] = {
                'class_weights': avg_weights,
                'type': 'class_weight',
                'description': strategy_descriptions[0] if strategy_descriptions else f'{strategy_name} (aggregated)',
                'aggregation_method': 'mean',
                'n_stocks': len(weights_by_class['-1'])
            }

    logging.info(f"âœ… èšåˆ {len(aggregated_weight_strategies)} ç¨®æ¬Šé‡ç­–ç•¥")

    # æ­¥é©Ÿ 8: ä¿å­˜ metadataï¼ˆåŒ…å«æ¬Šé‡ç­–ç•¥å’Œ normalization ä¿¡æ¯ï¼‰
    meta_path = os.path.join(out_npz_dir, 'normalization_meta.json')

    # V7: æ•¸æ“šä¾†è‡ªé è™•ç†ï¼Œå·²ç¶“æ˜¯åŸå§‹åƒ¹æ ¼ï¼ˆæœªæ¨™æº–åŒ–ï¼‰
    # label_viewer éœ€è¦ normalization æ¬„ä½ä¾†åæ¨™æº–åŒ–ï¼Œä½† V7 æ•¸æ“šä¸éœ€è¦åæ¨™æº–åŒ–
    # å› æ­¤æä¾›ä¸€å€‹ identity transformation (mean=0, std=1)
    normalization_info = {
        "method": "none",
        "note": "V7 æ•¸æ“šä¾†è‡ªé è™•ç† NPZï¼Œfeatures ç‚ºåŸå§‹åƒ¹æ ¼ï¼ˆæœªæ¨™æº–åŒ–ï¼‰",
        "feature_means": [0.0] * 20,  # Identity: mean = 0
        "feature_stds": [1.0] * 20    # Identity: std = 1
    }

    metadata = {
        "format": "deeplob_v7_simplified",
        "version": VERSION,
        "creation_date": datetime.now().isoformat(),
        "normalization": normalization_info,  # æ·»åŠ  normalization æ¬„ä½
        "data_source": {
            "preprocessed_files": len(preprocessed_data),
            "symbols_count": len(symbols)
        },
        "data_split": {
            "method": "by_symbol",
            "train": {
                "symbols": len(train_symbols),
                "samples": len(train_X),
                "label_dist": train_y.tolist() if len(train_y) > 0 else []
            },
            "val": {
                "symbols": len(val_symbols),
                "samples": len(val_X),
                "label_dist": val_y.tolist() if len(val_y) > 0 else []
            },
            "test": {
                "symbols": len(test_symbols),
                "samples": len(test_X),
                "label_dist": test_y.tolist() if len(test_y) > 0 else []
            }
        },
        "label_source": {
            "method": "preprocessed",
            "note": "ç›´æ¥ä½¿ç”¨é è™•ç† NPZ çš„ labels å­—æ®µï¼Œæœªé‡æ–°è¨ˆç®—"
        },
        "sample_weights": {
            "enabled": weight_enabled,
            "strategy_used": weight_strategy_name,
            "available_strategies": list(aggregated_weight_strategies.keys()),
            "note": "æ¬Šé‡ç­–ç•¥å¾é è™•ç†æ•¸æ“šèšåˆè€Œä¾†ï¼ˆå¹³å‡å€¼ï¼‰"
        },
        "weight_strategies": aggregated_weight_strategies
    }

    with open(meta_path, 'w', encoding='utf-8') as f:
        json.dump(metadata, f, indent=2, ensure_ascii=False)

    logging.info(f"âœ… Metadata ä¿å­˜: {meta_path}")
    logging.info("=" * 80)


def parse_args():
    p = argparse.ArgumentParser("extract_tw_stock_data_v6", description="V7 ç°¡åŒ–ç‰ˆè³‡æ–™æµæ°´ç·šï¼ˆéšæ®µ2ï¼‰")
    p.add_argument("--preprocessed-dir", default="./data/preprocessed_v5", help="é è™•ç†çµæœç›®éŒ„")
    p.add_argument("--output-dir", default="./data/processed_v6", help="è¼¸å‡ºç›®éŒ„")
    p.add_argument("--config", default="./configs/config_pro_v5_ml_optimal.yaml", help="é…ç½®æ–‡ä»¶")
    p.add_argument("--json", default=None, help="dataset_selection.json æª”æ¡ˆè·¯å¾‘ï¼ˆå„ªå…ˆæ–¼é…ç½®æ–‡ä»¶ä¸­çš„è¨­å®šï¼‰")
    return p.parse_args()


def main():
    try:
        args = parse_args()

        # è¼‰å…¥é…ç½®
        if not os.path.exists(args.config):
            logging.error(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {args.config}")
            return 1

        yaml_manager = YAMLManager(args.config)
        config = yaml_manager.as_dict()

        # é©—è­‰ç›®éŒ„
        if not os.path.exists(args.preprocessed_dir):
            logging.error(f"é è™•ç†ç›®éŒ„ä¸å­˜åœ¨: {args.preprocessed_dir}")
            return 1

        os.makedirs(args.output_dir, exist_ok=True)

        logging.info(f"{'='*60}")
        logging.info(f"V6 è³‡æ–™æµæ°´ç·šå•Ÿå‹•ï¼ˆéšæ®µ2ï¼šè®€å–é è™•ç†æ•¸æ“šï¼‰")
        logging.info(f"{'='*60}")
        logging.info(f"é è™•ç†ç›®éŒ„: {args.preprocessed_dir}")
        logging.info(f"è¼¸å‡ºç›®éŒ„: {args.output_dir}")
        logging.info(f"é…ç½®ç‰ˆæœ¬: {config['version']}")
        logging.info(f"{'='*60}\n")

        # â­ NEW: è¼‰å…¥é è™•ç†æ•¸æ“šï¼ˆå„ªåŒ–ï¼šæ”¯æŒå¾ JSON ç›´æ¥è¼‰å…¥ï¼‰
        preprocessed_data = load_all_preprocessed_data(
            args.preprocessed_dir,
            config=config,
            json_file_override=args.json  # å‚³å…¥å‘½ä»¤åƒæ•¸æŒ‡å®šçš„ JSON æª”æ¡ˆ
        )

        if not preprocessed_data:
            logging.error("æ²’æœ‰å¯ç”¨çš„é è™•ç†æ•¸æ“šï¼")
            return 1

        # â­ æ³¨æ„ï¼šå¦‚æœä½¿ç”¨ JSON è¼‰å…¥ï¼Œä¸éœ€è¦å†æ¬¡éæ¿¾
        # sliding_windows_v7 ä¸­çš„ filter_data_by_selection æœƒæª¢æ¸¬åˆ°å·²ç¶“æŒ‰ JSON è¼‰å…¥ï¼Œç›´æ¥è·³ééæ¿¾
        sliding_windows_v7(
            preprocessed_data,
            args.output_dir,  # ä¿®æ­£ï¼šç›´æ¥å‚³å…¥ output_dirï¼Œå‡½æ•¸å…§éƒ¨æœƒåŠ ä¸Š npz
            config,
            args.json  # å‚³å…¥å‘½ä»¤åƒæ•¸æŒ‡å®šçš„ JSON æª”æ¡ˆï¼ˆç”¨æ–¼ filter æª¢æ¸¬ï¼‰
        )

        logging.info(f"\n{'='*60}")
        logging.info(f"[å®Œæˆ] V6 è½‰æ›æˆåŠŸï¼Œè¼¸å‡ºè³‡æ–™å¤¾: {args.output_dir}")
        logging.info(f"{'='*60}")
        logging.info(f"çµ±è¨ˆè³‡æ–™:")
        logging.info(f"  è¼‰å…¥ NPZ: {global_stats['loaded_npz_files']:,}")
        logging.info(f"  é€šééæ¿¾: {global_stats['symbols_passed_filter']:,}")
        logging.info(f"  è¢«éæ¿¾: {global_stats['symbols_filtered_out']:,}")

        # æ•¸æ“šè³ªé‡å ±å‘Šï¼ˆé‡è¦ï¼ï¼‰
        if global_stats['data_quality_errors'] > 0:
            logging.warning(f"  âš ï¸ æ•¸æ“šè³ªé‡éŒ¯èª¤: {global_stats['data_quality_errors']:,} å€‹æª”æ¡ˆ")
            logging.warning(f"     â†’ è«‹æª¢æŸ¥ä¸Šæ–¹çš„éŒ¯èª¤è¨Šæ¯ï¼Œä¸¦é‡æ–°é‹è¡Œé è™•ç†ï¼")
        else:
            logging.info(f"  âœ… æ•¸æ“šè³ªé‡æª¢æŸ¥: å…¨éƒ¨é€šé")

        logging.info(f"  æœ‰æ•ˆçª—å£: {global_stats['valid_windows']:,}")
        logging.info(f"  æ¨™ç±¤ä¾†æº: NPZ é è™•ç†æ•¸æ“šï¼ˆ{global_stats['labels_from_npz']:,} å€‹æª”æ¡ˆï¼‰")
        logging.info(f"{'='*60}\n")

        # å¦‚æœæœ‰æ•¸æ“šè³ªé‡éŒ¯èª¤ï¼Œè¿”å›è­¦å‘Šç‹€æ…‹ç¢¼
        if global_stats['data_quality_errors'] > 0:
            logging.warning(f"\nâš ï¸ è­¦å‘Šï¼šç™¼ç¾ {global_stats['data_quality_errors']} å€‹æ•¸æ“šè³ªé‡å•é¡Œ")
            logging.warning(f"å»ºè­°é‡æ–°é‹è¡Œé è™•ç†è…³æœ¬ä»¥ä¿®å¾©å•é¡Œ")
            return 2  # è¿”å› 2 è¡¨ç¤ºæœ‰è­¦å‘Š

        return 0

    except Exception as e:
        logging.error(f"ç¨‹å¼åŸ·è¡Œå¤±æ•—: {e}")
        import traceback
        traceback.print_exc()
        return 1


if __name__ == "__main__":
    import sys
    sys.exit(main())
