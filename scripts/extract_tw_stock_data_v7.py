# -*- coding: utf-8 -*-
"""
extract_tw_stock_data_v7.py - V7 ç°¡åŒ–ç‰ˆè³‡æ–™æµæ°´ç·šï¼ˆå°ˆæ³¨æ•¸æ“šçµ„ç¹”ï¼‰
=============================================================================
ã€æ›´æ–°æ—¥æœŸã€‘2025-10-23
ã€ç‰ˆæœ¬èªªæ˜ã€‘v7.0.0-simplified - é è™•ç†å·²å®Œæˆæ‰€æœ‰è¨ˆç®—ï¼ŒV7 åªåšæ•¸æ“šçµ„ç¹”

æ ¸å¿ƒç†å¿µï¼š
  "é è™•ç†å·²å®Œæˆï¼ŒV7 åªåšæ•¸æ“šçµ„ç¹”"

V7 ç°¡åŒ–æµç¨‹ï¼š
  é è™•ç†éšæ®µï¼ˆpreprocess_single_day.pyï¼‰:
    âœ… æ•¸æ“šæ¸…æ´—èˆ‡èšåˆ
    âœ… Z-Score æ¨™æº–åŒ–
    âœ… æ¨™ç±¤è¨ˆç®—ï¼ˆTriple-Barrier / Trendï¼‰
    âœ… æ¬Šé‡ç­–ç•¥è¨ˆç®—ï¼ˆ11 ç¨®ï¼‰
    âœ… çµ±è¨ˆä¿¡æ¯è¨˜éŒ„

  V7 éšæ®µï¼ˆextract_tw_stock_data_v7.pyï¼‰:
    âœ… è®€å–é è™•ç† NPZï¼ˆç›´æ¥ä½¿ç”¨ features, labelsï¼‰
    âœ… æ•¸æ“šé¸æ“‡ï¼ˆdataset_selection.json æˆ–é…ç½®éæ¿¾ï¼‰
    âœ… æ»‘å‹•çª—å£ç”Ÿæˆï¼ˆ100 timestepsï¼‰
    âœ… æŒ‰è‚¡ç¥¨åŠƒåˆ†ï¼ˆtrain/val/test = 70/15/15ï¼‰
    âœ… è¼¸å‡º NPZï¼ˆèˆ‡ V6 æ ¼å¼å…¼å®¹ï¼‰

    âŒ ä¸é‡æ–°è¨ˆç®—æ¨™ç±¤
    âŒ ä¸é‡æ–°è¨ˆç®—æ³¢å‹•ç‡
    âŒ ä¸é‡æ–°è¨ˆç®—æ¬Šé‡
    âŒ ä¸é‡æ–°æ¨™æº–åŒ–

ä½¿ç”¨æ–¹å¼ï¼š
  # é¸é … 1: ä½¿ç”¨ dataset_selection.jsonï¼ˆæ¨è–¦ï¼‰
  python scripts/analyze_label_distribution.py       --preprocessed-dir data/preprocessed_v5       --mode smart_recommend       --target-dist "0.30,0.40,0.30"       --output results/dataset_selection_auto.json

  python scripts/extract_tw_stock_data_v7.py       --preprocessed-dir ./data/preprocessed_v5       --output-dir ./data/processed_v7       --config configs/config_pro_v7_optimal.yaml

  # é¸é … 2: ä½¿ç”¨é…ç½®éæ¿¾
  python scripts/extract_tw_stock_data_v7.py       --preprocessed-dir ./data/preprocessed_v5       --output-dir ./data/processed_v7       --config configs/config_v7_test.yaml

è¼¸å‡ºï¼š
  - ./data/processed_v7/npz/stock_embedding_train.npz
  - ./data/processed_v7/npz/stock_embedding_val.npz
  - ./data/processed_v7/npz/stock_embedding_test.npz
  - ./data/processed_v7/npz/normalization_meta.json

ç‰ˆæœ¬ï¼šv7.0.0-simplified
æ›´æ–°ï¼š2025-10-23
"""
import os
import json
import argparse
import glob
import logging
import warnings
from collections import defaultdict
from datetime import datetime
from typing import List, Tuple, Dict, Any, Optional

import numpy as np
from tqdm import tqdm
import pandas as pd
import sys
from pathlib import Path

# æ·»åŠ é …ç›®æ ¹ç›®éŒ„
project_root = Path(__file__).parent.parent
if str(project_root) not in sys.path:
    sys.path.insert(0, str(project_root))

from src.utils.yaml_manager import YAMLManager

# ã€æ–°å¢ã€‘å°å…¥å°ˆæ¥­é‡‘èå·¥ç¨‹å‡½æ•¸åº«ï¼ˆ2025-10-23ï¼‰
from src.utils.financial_engineering import trend_labels_adaptive

# è¨­å®šç‰ˆæœ¬è™Ÿ
VERSION = "7.0.0-simplified"

# è¨­å®šæ—¥èªŒ
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)

warnings.filterwarnings('ignore', category=FutureWarning)
warnings.filterwarnings('ignore', category=RuntimeWarning)

# å›ºå®šå¸¸æ•¸
SEQ_LEN = 100

# å…¨åŸŸçµ±è¨ˆ
global_stats = {
    "loaded_npz_files": 0,
    "symbols_passed_filter": 0,
    "symbols_filtered_out": 0,
    "data_quality_errors": 0,
    "valid_windows": 0,
    "labels_from_npz": 0,  # V7: å¾ NPZ è®€å–æ¨™ç±¤
    "weights_from_metadata": 0,  # V7: å¾ metadata è®€å–æ¬Šé‡
}


# ============================================================
# ============================================================
# V7 æ–°å¢ï¼šJSON è®€å–èˆ‡æ•¸æ“šé¸æ“‡
# ============================================================

def read_dataset_selection_json(json_path: str) -> Optional[Dict]:
    """
    è®€å– dataset_selection.json

    Args:
        json_path: JSON æ–‡ä»¶è·¯å¾‘

    Returns:
        Dict with 'metadata' and 'file_list', or None if error
    """
    try:
        if not os.path.exists(json_path):
            logging.error(f"âŒ JSON æ–‡ä»¶ä¸å­˜åœ¨: {json_path}")
            return None

        with open(json_path, 'r', encoding='utf-8') as f:
            data = json.load(f)

        # é©—è­‰å¿…è¦å­—æ®µ
        if 'file_list' not in data:
            logging.error(f"âŒ JSON ç¼ºå°‘ 'file_list' å­—æ®µ")
            return None

        if not isinstance(data['file_list'], list):
            logging.error(f"âŒ 'file_list' å¿…é ˆæ˜¯åˆ—è¡¨")
            return None

        logging.info(f"âœ… æˆåŠŸè®€å– JSON: {len(data['file_list'])} å€‹æ–‡ä»¶")
        if 'metadata' in data:
            logging.info(f"   ç¸½æ¨£æœ¬æ•¸: {data['metadata'].get('total_samples', 'N/A')}")

        return data

    except json.JSONDecodeError as e:
        logging.error(f"âŒ JSON æ ¼å¼éŒ¯èª¤: {e}")
        return None
    except Exception as e:
        logging.error(f"âŒ è®€å– JSON å¤±æ•—: {e}")
        return None


def filter_data_by_selection(
    all_data: List[Tuple[str, str, np.ndarray, np.ndarray, Dict]],
    config: Dict,
    json_file_override: Optional[str] = None
) -> List[Tuple[str, str, np.ndarray, np.ndarray, Dict]]:
    """
    æ ¹æ“šé…ç½®éæ¿¾æ•¸æ“š

    å„ªå…ˆç´š:
      1. ä½¿ç”¨ --json å‘½ä»¤åƒæ•¸ï¼ˆæœ€é«˜å„ªå…ˆç´šï¼‰
      2. ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„ dataset_selection.json
      3. ä½¿ç”¨é…ç½®éæ¿¾ï¼ˆstart_date, num_days, symbolsï¼‰

    Args:
        all_data: [(date, symbol, features, labels, meta), ...]
        config: é…ç½®å­—å…¸
        json_file_override: å‘½ä»¤åƒæ•¸æŒ‡å®šçš„ JSON æª”æ¡ˆï¼ˆå„ªå…ˆæ–¼é…ç½®æ–‡ä»¶ï¼‰

    Returns:
        éæ¿¾å¾Œçš„æ•¸æ“šåˆ—è¡¨
    """
    data_selection = config.get('data_selection', {})

    # å„ªå…ˆç´š 1: ä½¿ç”¨å‘½ä»¤åƒæ•¸æŒ‡å®šçš„ JSON æ–‡ä»¶
    json_file = json_file_override or data_selection.get('json_file')
    if json_file:
        logging.info(f"ğŸ“‹ ä½¿ç”¨ dataset_selection.json: {json_file}")
        json_data = read_dataset_selection_json(json_file)

        if json_data is None:
            logging.warning(f"âš ï¸ JSON è®€å–å¤±æ•—ï¼Œå›é€€åˆ°é…ç½®éæ¿¾")
        else:
            # å¾ JSON æå–éœ€è¦çš„æ–‡ä»¶
            file_list = json_data['file_list']
            selected_files = {(item['date'], item['symbol']) for item in file_list}

            filtered_data = [
                item for item in all_data
                if (item[0], item[1]) in selected_files
            ]

            logging.info(f"âœ… JSON éæ¿¾: {len(all_data)} â†’ {len(filtered_data)} å€‹æ–‡ä»¶")
            return filtered_data

    # å„ªå…ˆç´š 2: ä½¿ç”¨é…ç½®éæ¿¾
    logging.info(f"ğŸ“‹ ä½¿ç”¨é…ç½®éæ¿¾")

    # 1. æ—¥æœŸéæ¿¾
    start_date = data_selection.get('start_date')
    end_date = data_selection.get('end_date')
    num_days = data_selection.get('num_days')

    if start_date or end_date or num_days:
        # æå–æ‰€æœ‰æ—¥æœŸä¸¦æ’åº
        all_dates = sorted(set(item[0] for item in all_data))

        if start_date:
            all_dates = [d for d in all_dates if d >= start_date]
        if end_date:
            all_dates = [d for d in all_dates if d <= end_date]
        if num_days and num_days > 0:
            all_dates = all_dates[:num_days]

        selected_dates = set(all_dates)
        all_data = [item for item in all_data if item[0] in selected_dates]

        logging.info(f"   æ—¥æœŸéæ¿¾: ä¿ç•™ {len(selected_dates)} å¤©")

    # 2. è‚¡ç¥¨éæ¿¾
    symbols = data_selection.get('symbols')
    if symbols:
        all_data = [item for item in all_data if item[1] in symbols]
        logging.info(f"   è‚¡ç¥¨éæ¿¾: ä¿ç•™ {len(symbols)} æª”")

    # 3. éš¨æ©Ÿæ¡æ¨£
    sample_ratio = data_selection.get('sample_ratio', 1.0)
    if sample_ratio < 1.0:
        np.random.seed(data_selection.get('random_seed', 42))
        n_samples = int(len(all_data) * sample_ratio)
        indices = np.random.choice(len(all_data), n_samples, replace=False)
        all_data = [all_data[i] for i in sorted(indices)]
        logging.info(f"   éš¨æ©Ÿæ¡æ¨£: {sample_ratio:.1%} â†’ {n_samples} å€‹æ–‡ä»¶")

    logging.info(f"âœ… é…ç½®éæ¿¾å®Œæˆ: {len(all_data)} å€‹æ–‡ä»¶")
    return all_data


def make_sample_weight(ret: pd.Series,
                      tt: pd.Series,
                      y: pd.Series,
                      tau: float = 100.0,
                      scale: float = 10.0,
                      balance: bool = True,
                      use_log_scale: bool = True) -> pd.Series:
    """æ¨£æœ¬æ¬Šé‡è¨ˆç®—"""
    from sklearn.utils.class_weight import compute_class_weight

    ret_array = np.array(ret.values, dtype=np.float64)
    tt_array = np.array(tt.values, dtype=np.float64)

    if use_log_scale:
        ret_weight = np.log1p(np.abs(ret_array) * 1000) * scale
        ret_weight = np.maximum(ret_weight, 0.1)
    else:
        ret_weight = np.abs(ret_array) * scale

    time_decay = np.exp(-tt_array / float(tau))
    # C. æ™‚é–“è¡°æ¸›æ¨™æº–åŒ–ï¼šé¿å…æ™šæœŸæ¨£æœ¬æ•´é«”è¢«å£“ä½
    time_decay = time_decay / (time_decay.mean() + 1e-12)

    base = ret_weight * time_decay
    base = np.clip(base, 0.05, None)

    if balance:
        classes = np.array(sorted(y.unique()))
        y_array = np.array(y.values, dtype=np.int64)
        cls_w = compute_class_weight('balanced', classes=classes, y=y_array)

        cls_w = np.clip(cls_w, 0.5, 3.0)
        cls_w = cls_w / cls_w.mean()

        w_map = dict(zip(classes, cls_w))
        cw = np.array(y.map(w_map).values, dtype=np.float64)
        w = base * cw
    else:
        w = base

    w = w / np.mean(w)
    w = np.clip(w, 0.1, 5.0)

    # è£å‰ªå¾Œé‡æ–°æ­¸ä¸€åŒ–ï¼Œç¢ºä¿å‡å€¼=1.0
    w = w / np.mean(w)

    return pd.Series(w, index=y.index)


def zscore_fit(X: np.ndarray, method: str = 'global', window: int = 100, min_periods: int = 20) -> Tuple[np.ndarray, np.ndarray]:
    """
    è¨ˆç®— Z-Score åƒæ•¸ï¼ˆæ”¯æŒå¤šç¨®æ¨™æº–åŒ–æ–¹æ³•ï¼‰

    Parameters:
    -----------
    X : np.ndarray, shape (n_samples, n_features)
        è¼¸å…¥ç‰¹å¾µ
    method : str
        æ¨™æº–åŒ–æ–¹æ³•ï¼š
        - 'global': å…¨å±€çµ±è¨ˆé‡ï¼ˆèˆŠæ–¹æ³•ï¼Œå¯èƒ½å°è‡´åˆ†å¸ƒæ¼‚ç§»ï¼‰
        - 'rolling_zscore': æ»¾å‹•çª—å£ Z-Scoreï¼ˆæ¨è–¦ï¼Œé©æ‡‰å¸‚å ´è®ŠåŒ–ï¼‰
    window : int
        æ»¾å‹•çª—å£å¤§å°ï¼ˆåƒ… rolling_zscore ä½¿ç”¨ï¼‰
    min_periods : int
        æœ€å°æœ‰æ•ˆæ¨£æœ¬æ•¸ï¼ˆåƒ… rolling_zscore ä½¿ç”¨ï¼‰

    Returns:
    --------
    mu, sd : å‡å€¼å’Œæ¨™æº–å·®ï¼ˆglobal æ–¹æ³•ï¼‰æˆ– None, Noneï¼ˆrolling æ–¹æ³•ï¼‰

    Notes:
    ------
    - å…¨å±€æ–¹æ³•ï¼šä½¿ç”¨æ•´å€‹è¨“ç·´é›†çµ±è¨ˆé‡ï¼ˆå¿«é€Ÿä½†å¯èƒ½æ¼‚ç§»ï¼‰
    - æ»¾å‹•æ–¹æ³•ï¼šä½¿ç”¨æœ€è¿‘ N å€‹æ¨£æœ¬çµ±è¨ˆé‡ï¼ˆç©©å®šä½†ç¨æ…¢ï¼‰
    """
    if method == 'global':
        # åŸæœ‰çš„å…¨å±€æ¨™æº–åŒ–
        mu = X.mean(axis=0)
        sd = X.std(axis=0, ddof=0)
        sd = np.where(sd < 1e-8, 1.0, sd)

        if np.any(np.abs(mu) > 1e6):
            logging.warning(f"åµæ¸¬åˆ°ç•°å¸¸å¤§çš„å‡å€¼: max|Î¼|={np.max(np.abs(mu)):.2f}")

        return mu, sd

    elif method == 'rolling_zscore':
        # æ»¾å‹•çª—å£æ¨™æº–åŒ–ä¸éœ€è¦é å…ˆè¨ˆç®—çµ±è¨ˆé‡
        # çµ±è¨ˆé‡åœ¨ zscore_apply ä¸­å¯¦æ™‚è¨ˆç®—
        logging.info(f"ä½¿ç”¨æ»¾å‹•çª—å£æ¨™æº–åŒ–: window={window}, min_periods={min_periods}")
        return None, None

    else:
        raise ValueError(f"æœªçŸ¥çš„æ¨™æº–åŒ–æ–¹æ³•: {method}")


def zscore_apply(X: np.ndarray, mu: np.ndarray, sd: np.ndarray,
                 method: str = 'global', window: int = 100, min_periods: int = 20) -> np.ndarray:
    """
    æ‡‰ç”¨ Z-Score æ­£è¦åŒ–

    Parameters:
    -----------
    X : np.ndarray, shape (n_samples, n_features)
        è¼¸å…¥ç‰¹å¾µ
    mu : np.ndarray or None
        å‡å€¼ï¼ˆglobal æ–¹æ³•ï¼‰æˆ– Noneï¼ˆrolling æ–¹æ³•ï¼‰
    sd : np.ndarray or None
        æ¨™æº–å·®ï¼ˆglobal æ–¹æ³•ï¼‰æˆ– Noneï¼ˆrolling æ–¹æ³•ï¼‰
    method : str
        æ¨™æº–åŒ–æ–¹æ³•
    window : int
        æ»¾å‹•çª—å£å¤§å°
    min_periods : int
        æœ€å°æœ‰æ•ˆæ¨£æœ¬æ•¸

    Returns:
    --------
    normalized : np.ndarray
        æ¨™æº–åŒ–å¾Œçš„ç‰¹å¾µ
    """
    if method == 'global':
        # å…¨å±€æ¨™æº–åŒ–
        return (X - mu.reshape(1, -1)) / sd.reshape(1, -1)

    elif method == 'rolling_zscore':
        # æ»¾å‹•çª—å£æ¨™æº–åŒ–
        n_samples, n_features = X.shape
        normalized = np.zeros_like(X)

        for i in range(n_samples):
            # ç¢ºå®šæ»¾å‹•çª—å£ç¯„åœ
            start_idx = max(0, i - window + 1)
            window_data = X[start_idx:i+1, :]

            # è¨ˆç®—ç•¶å‰çª—å£çš„çµ±è¨ˆé‡
            if len(window_data) >= min_periods:
                mu_rolling = window_data.mean(axis=0)
                sd_rolling = window_data.std(axis=0, ddof=0)
                sd_rolling = np.where(sd_rolling < 1e-8, 1.0, sd_rolling)
            else:
                # warm-up æœŸï¼šä½¿ç”¨ expanding window
                mu_rolling = window_data.mean(axis=0)
                sd_rolling = window_data.std(axis=0, ddof=0)
                sd_rolling = np.where(sd_rolling < 1e-8, 1.0, sd_rolling)

            # æ¨™æº–åŒ–ç•¶å‰æ¨£æœ¬
            normalized[i, :] = (X[i, :] - mu_rolling) / sd_rolling

        return normalized

    else:
        raise ValueError(f"æœªçŸ¥çš„æ¨™æº–åŒ–æ–¹æ³•: {method}")


# ============================================================
# V6 æ ¸å¿ƒï¼šè¼‰å…¥é è™•ç†æ•¸æ“š
# ============================================================

def validate_preprocessed_data(features: np.ndarray, mids: np.ndarray, meta: Dict, npz_path: str) -> bool:
    """
    é©—è­‰é è™•ç†æ•¸æ“šè³ªé‡ï¼ˆä¸ä¿®è£œï¼Œåªæª¢æŸ¥ï¼‰

    Returns:
        True if valid, False if invalid (æœƒè¨˜éŒ„è­¦å‘Š)
    """
    symbol = meta.get('symbol', 'unknown')
    date = meta.get('date', 'unknown')

    # æª¢æŸ¥ 1: mids ä¸èƒ½ç‚º 0
    if (mids == 0).any():
        zero_count = (mids == 0).sum()
        zero_pct = zero_count / len(mids) * 100
        logging.error(
            f"âŒ æ•¸æ“šè³ªé‡éŒ¯èª¤ [{symbol} @ {date}]\n"
            f"   ç™¼ç¾ {zero_count} å€‹ mids=0 ({zero_pct:.1f}%)\n"
            f"   æª”æ¡ˆ: {npz_path}\n"
            f"   â†’ é è™•ç†éšæ®µ (preprocess_single_day.py) æ‡‰è©²ç§»é™¤é€™äº›é»ï¼"
        )
        return False

    # æª¢æŸ¥ 2: mids ä¸èƒ½ç‚º NaN
    if np.isnan(mids).any():
        nan_count = np.isnan(mids).sum()
        nan_pct = nan_count / len(mids) * 100
        logging.error(
            f"âŒ æ•¸æ“šè³ªé‡éŒ¯èª¤ [{symbol} @ {date}]\n"
            f"   ç™¼ç¾ {nan_count} å€‹ NaN ({nan_pct:.1f}%)\n"
            f"   æª”æ¡ˆ: {npz_path}"
        )
        return False

    # æª¢æŸ¥ 3: mids ä¸èƒ½ç‚ºè² æ•¸
    if (mids < 0).any():
        neg_count = (mids < 0).sum()
        logging.error(
            f"âŒ æ•¸æ“šè³ªé‡éŒ¯èª¤ [{symbol} @ {date}]\n"
            f"   ç™¼ç¾ {neg_count} å€‹è² åƒ¹æ ¼\n"
            f"   æª”æ¡ˆ: {npz_path}"
        )
        return False

    # æª¢æŸ¥ 4: features ä¸èƒ½ç‚º NaN
    if np.isnan(features).any():
        nan_count = np.isnan(features).sum()
        logging.error(
            f"âŒ æ•¸æ“šè³ªé‡éŒ¯èª¤ [{symbol} @ {date}]\n"
            f"   ç™¼ç¾ {nan_count} å€‹ NaN ç‰¹å¾µå€¼\n"
            f"   æª”æ¡ˆ: {npz_path}"
        )
        return False

    # æª¢æŸ¥ 5: å½¢ç‹€åŒ¹é…
    if features.shape[0] != len(mids):
        logging.error(
            f"âŒ æ•¸æ“šè³ªé‡éŒ¯èª¤ [{symbol} @ {date}]\n"
            f"   features å’Œ mids é•·åº¦ä¸åŒ¹é…: {features.shape[0]} vs {len(mids)}\n"
            f"   æª”æ¡ˆ: {npz_path}"
        )
        return False

    return True


def load_preprocessed_npz(npz_path: str) -> Optional[Tuple[np.ndarray, np.ndarray, Dict]]:
    """
    è¼‰å…¥é è™•ç†å¾Œçš„ NPZ æª”æ¡ˆï¼ˆV7 ç‰ˆæœ¬ï¼šè¿”å› labelsï¼‰

    Returns:
        (features, labels, metadata) or None if filtered/invalid

    V7 æ”¹å‹•:
        - è¿”å› labels è€Œé mids/bucket_mask
        - å¼·åˆ¶è¦æ±‚ NPZ v2.0+ï¼ˆå¿…é ˆæœ‰ labels å­—æ®µï¼‰
    """
    try:
        data = np.load(npz_path, allow_pickle=True)

        # V7 ç‰ˆæœ¬æª¢æŸ¥ï¼šå¿…é ˆæœ‰ labels å­—æ®µ
        if 'labels' not in data:
            logging.error(
                f"âŒ NPZ ç‰ˆæœ¬éèˆŠï¼ˆv1.0ï¼‰: {npz_path}\n"
                f"   V7 è¦æ±‚ v2.0+ NPZï¼ˆå« labels å­—æ®µï¼‰\n"
                f"   è§£æ±ºæ–¹æ³•:\n"
                f"   1. é‹è¡Œ: scripts\\batch_preprocess.bat\n"
                f"   2. ç¢ºä¿ NPZ å«æœ‰ 'labels' å­—æ®µ"
            )
            return None

        features = data['features']  # (T, 20)
        labels = data['labels']      # (T,) with values {-1, 0, 1} or {0, 1}
        mids = data.get('mids', np.zeros(len(labels)))  # åƒ…ç”¨æ–¼é©—è­‰
        meta = json.loads(str(data['metadata']))

        global_stats["loaded_npz_files"] += 1
        global_stats["labels_from_npz"] += 1  # V7 çµ±è¨ˆ

        # æª¢æŸ¥éæ¿¾ç‹€æ…‹
        if not meta['pass_filter']:
            global_stats["symbols_filtered_out"] += 1
            return None

        # æ•¸æ“šè³ªé‡é©—è­‰ï¼ˆV7 ç°¡åŒ–ç‰ˆï¼‰
        if not validate_preprocessed_data(features, mids, meta, npz_path):
            logging.warning(f"âš ï¸ è·³éæœ‰å•é¡Œçš„æ•¸æ“š: {npz_path}")
            global_stats["data_quality_errors"] += 1
            global_stats["symbols_filtered_out"] += 1
            return None

        # V7 é¡å¤–é©—è­‰ï¼šæª¢æŸ¥ labels
        if len(labels) != len(features):
            logging.error(f"âŒ labels é•·åº¦ä¸åŒ¹é…: {npz_path}")
            return None

        unique_labels = np.unique(labels)
        if not all(label in [-1, 0, 1, 0.0, 1.0] for label in unique_labels):
            logging.error(f"âŒ labels åŒ…å«ç•°å¸¸å€¼ {unique_labels}: {npz_path}")
            return None

        global_stats["symbols_passed_filter"] += 1

        return features, labels, meta

    except Exception as e:
        logging.warning(f"ç„¡æ³•è¼‰å…¥ {npz_path}: {e}")
        return None


def load_all_preprocessed_data(preprocessed_dir: str) -> List[Tuple[str, str, np.ndarray, np.ndarray, Dict]]:
    """
    è¼‰å…¥æ‰€æœ‰é è™•ç†æ•¸æ“šï¼ˆV7 ç‰ˆæœ¬ï¼šè¿”å› labelsï¼‰

    Returns:
        List[(date, symbol, features, labels, metadata)]

    V7 æ”¹å‹•:
        - è¿”å› labels è€Œé mids/bucket_mask
    """
    daily_dir = os.path.join(preprocessed_dir, "daily")

    if not os.path.exists(daily_dir):
        logging.error(f"é è™•ç†ç›®éŒ„ä¸å­˜åœ¨: {daily_dir}")
        return []

    all_data = []

    # æƒææ‰€æœ‰ NPZ æª”æ¡ˆ
    npz_files = sorted(glob.glob(os.path.join(daily_dir, "*", "*.npz")))
    logging.info(f"é–‹å§‹è¼‰å…¥ {len(npz_files)} å€‹ NPZ æª”æ¡ˆ...")

    for npz_file in tqdm(npz_files, desc="è¼‰å…¥ NPZ", unit="æª”"):
        result = load_preprocessed_npz(npz_file)

        if result is None:
            continue

        features, labels, meta = result  # V7: åªè¿”å› features, labels, meta
        date = meta['date']
        symbol = meta['symbol']

        all_data.append((date, symbol, features, labels, meta))

    # G.1: ç©ºæ•¸æ“šå ±å‘Šå¢å¼·
    if len(all_data) == 0:
        logging.error(f"\nâŒ æ²’æœ‰å¯ç”¨çš„é è™•ç†æ•¸æ“šï¼")
        logging.error(f"å¯èƒ½åŸå› :")
        logging.error(f"  1. preprocessed_dir è·¯å¾‘éŒ¯èª¤: {preprocessed_dir}")
        logging.error(f"  2. æ‰€æœ‰è‚¡ç¥¨è¢«éæ¿¾ï¼ˆpass_filter=falseï¼‰")
        logging.error(f"  3. æ•¸æ“šè³ªé‡éŒ¯èª¤ï¼ˆmids=0, NaN ç­‰ï¼‰")
        logging.error(f"  4. é è™•ç†å°šæœªåŸ·è¡Œ")
        logging.error(f"\nå»ºè­°æª¢æŸ¥:")
        logging.error(f"  - ç¢ºèªé è™•ç†ç›®éŒ„å­˜åœ¨: {os.path.exists(daily_dir)}")
        logging.error(f"  - æª¢æŸ¥ NPZ æª”æ¡ˆæ•¸é‡: {len(list(glob.glob(os.path.join(daily_dir, '*', '*.npz'))))}")
        logging.error(f"  - æŸ¥çœ‹é è™•ç†æ—¥èªŒæˆ– summary.json")
    else:
        logging.info(f"è¼‰å…¥äº† {len(all_data)} å€‹ symbol-day çµ„åˆï¼ˆé€šééæ¿¾ï¼‰")
        logging.info(f"éæ¿¾æ‰: {global_stats['symbols_filtered_out']} å€‹")
        if global_stats['data_quality_errors'] > 0:
            logging.warning(f"æ•¸æ“šè³ªé‡éŒ¯èª¤: {global_stats['data_quality_errors']} å€‹ï¼ˆå·²è·³éï¼‰")

    return all_data


# ============================================================
# V6 æ»‘çª—æµç¨‹ï¼ˆç°¡åŒ–ç‰ˆï¼Œæ•¸æ“šå·²æ¸…æ´—ï¼‰
# ============================================================

def sliding_windows_v7(
    preprocessed_data: List[Tuple[str, str, np.ndarray, np.ndarray, Dict]],
    out_dir: str,
    config: Dict[str, Any],
    json_file: Optional[str] = None
):
    """
    V7 ç°¡åŒ–ç‰ˆæ»‘çª—æµç¨‹ï¼ˆå°ˆæ³¨æ•¸æ“šçµ„ç¹”ï¼Œä¸é‡è¤‡è¨ˆç®—ï¼‰

    V7 ç°¡åŒ–æ”¹å‹•ï¼š
    - è¼¸å…¥: (date, symbol, features, labels, meta)
    - ä¸é‡æ–°è¨ˆç®—æ¨™ç±¤ï¼ˆç›´æ¥ä½¿ç”¨ NPZ çš„ labelsï¼‰
    - ä¸é‡æ–°è¨ˆç®—æ³¢å‹•ç‡ï¼ˆä¸éœ€è¦ï¼‰
    - ä¸é‡æ–°è¨ˆç®—æ¬Šé‡ï¼ˆå¾ metadata è®€å–ï¼‰
    - å°ˆæ³¨æ–¼æ»‘å‹•çª—å£ç”Ÿæˆå’Œæ•¸æ“šåŠƒåˆ†

    Args:
        json_file: å‘½ä»¤åƒæ•¸æŒ‡å®šçš„ JSON æª”æ¡ˆè·¯å¾‘ï¼ˆå„ªå…ˆæ–¼é…ç½®æ–‡ä»¶ï¼‰
    """
    global global_stats
    
    logging.info("=" * 80)
    logging.info("V7 ç°¡åŒ–ç‰ˆæ»‘çª—æµç¨‹é–‹å§‹")
    logging.info("=" * 80)

    if not preprocessed_data:
        logging.warning("æ²’æœ‰è³‡æ–™å¯ä¾›ç”¢ç”Ÿ .npz æª”æ¡ˆ")
        return

    # æ­¥é©Ÿ 0: æ‡‰ç”¨æ•¸æ“šé¸æ“‡éæ¿¾
    preprocessed_data = filter_data_by_selection(preprocessed_data, config, json_file)
    
    if not preprocessed_data:
        logging.error("âŒ éæ¿¾å¾Œç„¡æ•¸æ“š")
        return

    logging.info(f"éæ¿¾å¾Œæ•¸æ“š: {len(preprocessed_data)} å€‹ symbol-day")

    # æ­¥é©Ÿ 1: æŒ‰è‚¡ç¥¨åˆ†çµ„ï¼ˆä¿å­˜ metadata ç”¨æ–¼æ¬Šé‡æå–ï¼‰
    stock_data = defaultdict(list)
    stock_metadata = {}  # æ¯å€‹è‚¡ç¥¨çš„ç¬¬ä¸€å€‹ metadataï¼ˆç”¨æ–¼æ¬Šé‡ç­–ç•¥ï¼‰
    for date, sym, features, labels, meta in preprocessed_data:
        stock_data[sym].append((date, features, labels))
        if sym not in stock_metadata:
            stock_metadata[sym] = meta  # ä¿å­˜ç¬¬ä¸€å€‹ metadata

    logging.info(f"å…± {len(stock_data)} å€‹è‚¡ç¥¨")

    # æ­¥é©Ÿ 2: æŒ‰è‚¡ç¥¨åŠƒåˆ† train/val/test (70/15/15)
    symbols = sorted(stock_data.keys())
    n_symbols = len(symbols)
    
    n_train = int(n_symbols * 0.70)
    n_val = int(n_symbols * 0.15)
    
    train_symbols = set(symbols[:n_train])
    val_symbols = set(symbols[n_train:n_train + n_val])
    test_symbols = set(symbols[n_train + n_val:])
    
    logging.info(f"åŠƒåˆ†: train={len(train_symbols)}, val={len(val_symbols)}, test={len(test_symbols)}")

    # æ­¥é©Ÿ 3: æå–æ¬Šé‡ç­–ç•¥é…ç½®
    weight_strategy_name = config.get('sample_weights', {}).get('strategy', 'uniform')
    weight_enabled = config.get('sample_weights', {}).get('enabled', True)
    logging.info(f"æ¨£æœ¬æ¬Šé‡: {'enabled' if weight_enabled else 'disabled'}, strategy='{weight_strategy_name}'")

    # æ­¥é©Ÿ 4: ç”Ÿæˆæ»‘å‹•çª—å£ï¼ˆåŒ…å« weights å’Œ stock_idsï¼‰
    train_X, train_y, train_weights, train_stock_ids = [], [], [], []
    val_X, val_y, val_weights, val_stock_ids = [], [], [], []
    test_X, test_y, test_weights, test_stock_ids = [], [], [], []

    logging.info(f"é–‹å§‹ç”Ÿæˆæ»‘å‹•çª—å£ï¼ˆ{len(symbols)} æª”è‚¡ç¥¨ï¼‰...")
    for sym in tqdm(symbols, desc="ç”Ÿæˆæ»‘çª—", unit="è‚¡"):
        # æå–è©²è‚¡ç¥¨çš„æ¬Šé‡ç­–ç•¥
        meta = stock_metadata.get(sym, {})
        weight_strategies = meta.get('weight_strategies', {})

        # ç²å–æŒ‡å®šç­–ç•¥çš„æ¬Šé‡
        if weight_enabled and weight_strategy_name in weight_strategies:
            class_weights = weight_strategies[weight_strategy_name].get('class_weights', {})
            weight_down = class_weights.get('-1', 1.0)
            weight_neutral = class_weights.get('0', 1.0)
            weight_up = class_weights.get('1', 1.0)
        else:
            # é»˜èªç„¡æ¬Šé‡
            weight_down = weight_neutral = weight_up = 1.0

        # åˆä½µè©²è‚¡ç¥¨æ‰€æœ‰å¤©çš„æ•¸æ“š
        all_features = []
        all_labels = []

        for date, features, labels in sorted(stock_data[sym], key=lambda x: x[0]):
            all_features.append(features)
            all_labels.append(labels)

        # æ‹¼æ¥
        if not all_features:
            continue

        concat_features = np.vstack(all_features)  # (T_total, 20)
        concat_labels = np.hstack(all_labels)      # (T_total,)

        # ç”Ÿæˆæ»‘å‹•çª—å£ (100 timesteps)
        T = len(concat_features)
        if T < SEQ_LEN:
            logging.warning(f"âš ï¸ {sym}: æ•¸æ“šä¸è¶³ {T} < {SEQ_LEN}ï¼Œè·³é")
            continue

        for i in range(T - SEQ_LEN):
            X_window = concat_features[i:i+SEQ_LEN]  # (100, 20)
            y_label = concat_labels[i+SEQ_LEN-1]     # æœ€å¾Œä¸€å€‹æ™‚é–“æ­¥çš„æ¨™ç±¤

            # V7: æ¨™ç±¤è½‰æ› {-1, 0, 1} â†’ {0, 1, 2}ï¼Œä¸¦åˆ†é…æ¬Šé‡
            if y_label == -1:
                y_label = 0
                sample_weight = weight_down
            elif y_label == 0:
                y_label = 1
                sample_weight = weight_neutral
            elif y_label == 1:
                y_label = 2
                sample_weight = weight_up
            else:
                # è™•ç† {0.0, 1.0} æ ¼å¼ï¼ˆæŸäº›é è™•ç†ç‰ˆæœ¬ï¼‰
                if y_label == 0.0:
                    y_label = 1  # è¦–ç‚º neutral
                    sample_weight = weight_neutral
                elif y_label == 1.0:
                    y_label = 2  # è¦–ç‚º up
                    sample_weight = weight_up
                else:
                    logging.warning(f"âš ï¸ ç•°å¸¸æ¨™ç±¤å€¼: {y_label}")
                    continue

            # åˆ†é…åˆ°å°æ‡‰é›†åˆï¼ˆåŒ…å« weights å’Œ stock_idsï¼‰
            if sym in train_symbols:
                train_X.append(X_window)
                train_y.append(y_label)
                train_weights.append(sample_weight)
                train_stock_ids.append(sym)
            elif sym in val_symbols:
                val_X.append(X_window)
                val_y.append(y_label)
                val_weights.append(sample_weight)
                val_stock_ids.append(sym)
            else:
                test_X.append(X_window)
                test_y.append(y_label)
                test_weights.append(sample_weight)
                test_stock_ids.append(sym)

        global_stats["valid_windows"] += (T - SEQ_LEN)

    # æ­¥é©Ÿ 5: è½‰æ›ç‚º numpy é™£åˆ—ï¼ˆåŒ…å« weights å’Œ stock_idsï¼‰
    train_X = np.array(train_X, dtype=np.float32)  # (N_train, 100, 20)
    train_y = np.array(train_y, dtype=np.int32)
    train_weights = np.array(train_weights, dtype=np.float32)
    train_stock_ids = np.array(train_stock_ids, dtype='<U10')  # Unicode string

    val_X = np.array(val_X, dtype=np.float32)
    val_y = np.array(val_y, dtype=np.int32)
    val_weights = np.array(val_weights, dtype=np.float32)
    val_stock_ids = np.array(val_stock_ids, dtype='<U10')

    test_X = np.array(test_X, dtype=np.float32)
    test_y = np.array(test_y, dtype=np.int32)
    test_weights = np.array(test_weights, dtype=np.float32)
    test_stock_ids = np.array(test_stock_ids, dtype='<U10')

    logging.info(f"ç”Ÿæˆæ¨£æœ¬:")
    logging.info(f"  Train: {len(train_X)} æ¨£æœ¬")
    logging.info(f"  Val:   {len(val_X)} æ¨£æœ¬")
    logging.info(f"  Test:  {len(test_X)} æ¨£æœ¬")

    # æ¨™ç±¤åˆ†å¸ƒ
    for name, y in [("Train", train_y), ("Val", val_y), ("Test", test_y)]:
        dist = np.bincount(y, minlength=3)
        pct = dist / len(y) * 100 if len(y) > 0 else [0, 0, 0]
        logging.info(f"  {name} åˆ†å¸ƒ: Down={pct[0]:.1f}%, Neutral={pct[1]:.1f}%, Up={pct[2]:.1f}%")

    # æ­¥é©Ÿ 6: ä¿å­˜ NPZï¼ˆåŒ…å« weights å’Œ stock_idsï¼‰
    out_npz_dir = os.path.join(out_dir, 'npz')
    os.makedirs(out_npz_dir, exist_ok=True)

    train_path = os.path.join(out_npz_dir, 'stock_embedding_train.npz')
    val_path = os.path.join(out_npz_dir, 'stock_embedding_val.npz')
    test_path = os.path.join(out_npz_dir, 'stock_embedding_test.npz')

    logging.info("é–‹å§‹ä¿å­˜ NPZ æª”æ¡ˆï¼ˆåŒ…å« weights å’Œ stock_idsï¼‰...")
    datasets = [
        ("train", train_path, train_X, train_y, train_weights, train_stock_ids),
        ("val", val_path, val_X, val_y, val_weights, val_stock_ids),
        ("test", test_path, test_X, test_y, test_weights, test_stock_ids)
    ]

    for name, path, X, y, weights, stock_ids in tqdm(datasets, desc="ä¿å­˜ NPZ", unit="æª”"):
        np.savez_compressed(path, X=X, y=y, weights=weights, stock_ids=stock_ids)

    logging.info(f"âœ… ä¿å­˜å®Œæˆ:")
    logging.info(f"   {train_path}")
    logging.info(f"   {val_path}")
    logging.info(f"   {test_path}")
    logging.info("   æ ¼å¼: X, y, weights, stock_ids")

    # æ­¥é©Ÿ 7: èšåˆæ‰€æœ‰è‚¡ç¥¨çš„æ¬Šé‡ç­–ç•¥ï¼ˆç”¨æ–¼ metadataï¼‰
    logging.info("èšåˆæ¬Šé‡ç­–ç•¥...")
    aggregated_weight_strategies = {}

    # æ”¶é›†æ‰€æœ‰æ¬Šé‡ç­–ç•¥åç¨±
    all_strategy_names = set()
    for meta in stock_metadata.values():
        ws = meta.get('weight_strategies', {})
        all_strategy_names.update(ws.keys())

    # å°æ¯å€‹ç­–ç•¥è¨ˆç®—å¹³å‡æ¬Šé‡
    for strategy_name in all_strategy_names:
        weights_by_class = {'-1': [], '0': [], '1': []}
        strategy_descriptions = []

        for meta in stock_metadata.values():
            ws = meta.get('weight_strategies', {})
            if strategy_name in ws:
                strategy = ws[strategy_name]
                class_weights = strategy.get('class_weights', {})
                weights_by_class['-1'].append(class_weights.get('-1', 1.0))
                weights_by_class['0'].append(class_weights.get('0', 1.0))
                weights_by_class['1'].append(class_weights.get('1', 1.0))
                if 'description' in strategy and strategy['description'] not in strategy_descriptions:
                    strategy_descriptions.append(strategy['description'])

        # è¨ˆç®—å¹³å‡æ¬Šé‡
        if weights_by_class['-1']:  # ç¢ºä¿æœ‰æ•¸æ“š
            avg_weights = {
                '-1': float(np.mean(weights_by_class['-1'])),
                '0': float(np.mean(weights_by_class['0'])),
                '1': float(np.mean(weights_by_class['1']))
            }

            aggregated_weight_strategies[strategy_name] = {
                'class_weights': avg_weights,
                'type': 'class_weight',
                'description': strategy_descriptions[0] if strategy_descriptions else f'{strategy_name} (aggregated)',
                'aggregation_method': 'mean',
                'n_stocks': len(weights_by_class['-1'])
            }

    logging.info(f"âœ… èšåˆ {len(aggregated_weight_strategies)} ç¨®æ¬Šé‡ç­–ç•¥")

    # æ­¥é©Ÿ 8: ä¿å­˜ metadataï¼ˆåŒ…å«æ¬Šé‡ç­–ç•¥å’Œ normalization ä¿¡æ¯ï¼‰
    meta_path = os.path.join(out_npz_dir, 'normalization_meta.json')

    # V7: æ•¸æ“šä¾†è‡ªé è™•ç†ï¼Œå·²ç¶“æ˜¯åŸå§‹åƒ¹æ ¼ï¼ˆæœªæ¨™æº–åŒ–ï¼‰
    # label_viewer éœ€è¦ normalization æ¬„ä½ä¾†åæ¨™æº–åŒ–ï¼Œä½† V7 æ•¸æ“šä¸éœ€è¦åæ¨™æº–åŒ–
    # å› æ­¤æä¾›ä¸€å€‹ identity transformation (mean=0, std=1)
    normalization_info = {
        "method": "none",
        "note": "V7 æ•¸æ“šä¾†è‡ªé è™•ç† NPZï¼Œfeatures ç‚ºåŸå§‹åƒ¹æ ¼ï¼ˆæœªæ¨™æº–åŒ–ï¼‰",
        "feature_means": [0.0] * 20,  # Identity: mean = 0
        "feature_stds": [1.0] * 20    # Identity: std = 1
    }

    metadata = {
        "format": "deeplob_v7_simplified",
        "version": VERSION,
        "creation_date": datetime.now().isoformat(),
        "normalization": normalization_info,  # æ·»åŠ  normalization æ¬„ä½
        "data_source": {
            "preprocessed_files": len(preprocessed_data),
            "symbols_count": len(symbols)
        },
        "data_split": {
            "method": "by_symbol",
            "train": {
                "symbols": len(train_symbols),
                "samples": len(train_X),
                "label_dist": train_y.tolist() if len(train_y) > 0 else []
            },
            "val": {
                "symbols": len(val_symbols),
                "samples": len(val_X),
                "label_dist": val_y.tolist() if len(val_y) > 0 else []
            },
            "test": {
                "symbols": len(test_symbols),
                "samples": len(test_X),
                "label_dist": test_y.tolist() if len(test_y) > 0 else []
            }
        },
        "label_source": {
            "method": "preprocessed",
            "note": "ç›´æ¥ä½¿ç”¨é è™•ç† NPZ çš„ labels å­—æ®µï¼Œæœªé‡æ–°è¨ˆç®—"
        },
        "sample_weights": {
            "enabled": weight_enabled,
            "strategy_used": weight_strategy_name,
            "available_strategies": list(aggregated_weight_strategies.keys()),
            "note": "æ¬Šé‡ç­–ç•¥å¾é è™•ç†æ•¸æ“šèšåˆè€Œä¾†ï¼ˆå¹³å‡å€¼ï¼‰"
        },
        "weight_strategies": aggregated_weight_strategies
    }

    with open(meta_path, 'w', encoding='utf-8') as f:
        json.dump(metadata, f, indent=2, ensure_ascii=False)

    logging.info(f"âœ… Metadata ä¿å­˜: {meta_path}")
    logging.info("=" * 80)


def parse_args():
    p = argparse.ArgumentParser("extract_tw_stock_data_v6", description="V7 ç°¡åŒ–ç‰ˆè³‡æ–™æµæ°´ç·šï¼ˆéšæ®µ2ï¼‰")
    p.add_argument("--preprocessed-dir", default="./data/preprocessed_v5", help="é è™•ç†çµæœç›®éŒ„")
    p.add_argument("--output-dir", default="./data/processed_v6", help="è¼¸å‡ºç›®éŒ„")
    p.add_argument("--config", default="./configs/config_pro_v5_ml_optimal.yaml", help="é…ç½®æ–‡ä»¶")
    p.add_argument("--json", default=None, help="dataset_selection.json æª”æ¡ˆè·¯å¾‘ï¼ˆå„ªå…ˆæ–¼é…ç½®æ–‡ä»¶ä¸­çš„è¨­å®šï¼‰")
    return p.parse_args()


def main():
    try:
        args = parse_args()

        # è¼‰å…¥é…ç½®
        if not os.path.exists(args.config):
            logging.error(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {args.config}")
            return 1

        yaml_manager = YAMLManager(args.config)
        config = yaml_manager.as_dict()

        # é©—è­‰ç›®éŒ„
        if not os.path.exists(args.preprocessed_dir):
            logging.error(f"é è™•ç†ç›®éŒ„ä¸å­˜åœ¨: {args.preprocessed_dir}")
            return 1

        os.makedirs(args.output_dir, exist_ok=True)

        logging.info(f"{'='*60}")
        logging.info(f"V6 è³‡æ–™æµæ°´ç·šå•Ÿå‹•ï¼ˆéšæ®µ2ï¼šè®€å–é è™•ç†æ•¸æ“šï¼‰")
        logging.info(f"{'='*60}")
        logging.info(f"é è™•ç†ç›®éŒ„: {args.preprocessed_dir}")
        logging.info(f"è¼¸å‡ºç›®éŒ„: {args.output_dir}")
        logging.info(f"é…ç½®ç‰ˆæœ¬: {config['version']}")
        logging.info(f"{'='*60}\n")

        # è¼‰å…¥é è™•ç†æ•¸æ“š
        preprocessed_data = load_all_preprocessed_data(args.preprocessed_dir)

        if not preprocessed_data:
            logging.error("æ²’æœ‰å¯ç”¨çš„é è™•ç†æ•¸æ“šï¼")
            return 1

        # ç”Ÿæˆè¨“ç·´æ•¸æ“š
        sliding_windows_v7(
            preprocessed_data,
            args.output_dir,  # ä¿®æ­£ï¼šç›´æ¥å‚³å…¥ output_dirï¼Œå‡½æ•¸å…§éƒ¨æœƒåŠ ä¸Š npz
            config,
            args.json  # å‚³å…¥å‘½ä»¤åƒæ•¸æŒ‡å®šçš„ JSON æª”æ¡ˆ
        )

        logging.info(f"\n{'='*60}")
        logging.info(f"[å®Œæˆ] V6 è½‰æ›æˆåŠŸï¼Œè¼¸å‡ºè³‡æ–™å¤¾: {args.output_dir}")
        logging.info(f"{'='*60}")
        logging.info(f"çµ±è¨ˆè³‡æ–™:")
        logging.info(f"  è¼‰å…¥ NPZ: {global_stats['loaded_npz_files']:,}")
        logging.info(f"  é€šééæ¿¾: {global_stats['symbols_passed_filter']:,}")
        logging.info(f"  è¢«éæ¿¾: {global_stats['symbols_filtered_out']:,}")

        # æ•¸æ“šè³ªé‡å ±å‘Šï¼ˆé‡è¦ï¼ï¼‰
        if global_stats['data_quality_errors'] > 0:
            logging.warning(f"  âš ï¸ æ•¸æ“šè³ªé‡éŒ¯èª¤: {global_stats['data_quality_errors']:,} å€‹æª”æ¡ˆ")
            logging.warning(f"     â†’ è«‹æª¢æŸ¥ä¸Šæ–¹çš„éŒ¯èª¤è¨Šæ¯ï¼Œä¸¦é‡æ–°é‹è¡Œé è™•ç†ï¼")
        else:
            logging.info(f"  âœ… æ•¸æ“šè³ªé‡æª¢æŸ¥: å…¨éƒ¨é€šé")

        logging.info(f"  æœ‰æ•ˆçª—å£: {global_stats['valid_windows']:,}")
        logging.info(f"  æ¨™ç±¤ä¾†æº: NPZ é è™•ç†æ•¸æ“šï¼ˆ{global_stats['labels_from_npz']:,} å€‹æª”æ¡ˆï¼‰")
        logging.info(f"{'='*60}\n")

        # å¦‚æœæœ‰æ•¸æ“šè³ªé‡éŒ¯èª¤ï¼Œè¿”å›è­¦å‘Šç‹€æ…‹ç¢¼
        if global_stats['data_quality_errors'] > 0:
            logging.warning(f"\nâš ï¸ è­¦å‘Šï¼šç™¼ç¾ {global_stats['data_quality_errors']} å€‹æ•¸æ“šè³ªé‡å•é¡Œ")
            logging.warning(f"å»ºè­°é‡æ–°é‹è¡Œé è™•ç†è…³æœ¬ä»¥ä¿®å¾©å•é¡Œ")
            return 2  # è¿”å› 2 è¡¨ç¤ºæœ‰è­¦å‘Š

        return 0

    except Exception as e:
        logging.error(f"ç¨‹å¼åŸ·è¡Œå¤±æ•—: {e}")
        import traceback
        traceback.print_exc()
        return 1


if __name__ == "__main__":
    import sys
    sys.exit(main())
