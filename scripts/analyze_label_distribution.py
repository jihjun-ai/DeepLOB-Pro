# -*- coding: utf-8 -*-
"""
analyze_label_distribution.py - æ™ºèƒ½æ¨™ç±¤åˆ†å¸ƒåˆ†æèˆ‡æ•¸æ“šé›†é¸å–å·¥å…· v3.0
=============================================================================
ã€æ ¸å¿ƒåŠŸèƒ½ã€‘
  1. è‡ªå‹•å¾èµ·å§‹æ—¥æœŸé–‹å§‹ï¼Œé€æ—¥éå¢æƒææ‰€æœ‰é è™•ç† NPZ æ•¸æ“š
  2. åŸºæ–¼æ¨™ç±¤åˆ†å¸ƒï¼Œæ™ºèƒ½çµ„åˆå‡ºæœ€é©åˆå­¸ç¿’çš„æ—¥æœŸ+å€‹è‚¡çµ„åˆ
  3. è‡ªå‹•è¨ˆç®—æ‰€éœ€æ•¸é‡ï¼Œç¢ºä¿é”åˆ°ç›®æ¨™æ¨™ç±¤åˆ†å¸ƒï¼ˆå¯å®Œæ•´å­¸ç¿’ï¼‰
  4. äº’å‹•å¼é¸æ“‡ç•Œé¢ï¼ˆé¡¯ç¤ºå¤šå€‹å€™é¸æ–¹æ¡ˆï¼Œè®“ä½¿ç”¨è€…é¸æ“‡ï¼‰
  5. ğŸ†• æœ€å°åŒ–æ¨è–¦æ¨¡å¼ï¼ˆé€æª”ç´¯ç©ï¼Œé”æ¨™å³åœï¼ŒNeff æœ€å„ªåŒ–ï¼‰
  6. é¸å–å¾Œç”Ÿæˆè©³ç´°å ±å‘Šï¼ˆæ—¥æœŸåˆ—è¡¨ã€å€‹è‚¡IDã€æ•¸å€¼æ¯”ä¾‹ï¼‰

ã€ä½¿ç”¨æ–¹å¼ã€‘
  # åŸºç¤åˆ†ææ¨¡å¼ï¼ˆæƒææ‰€æœ‰æ•¸æ“šï¼‰
  python scripts/analyze_label_distribution.py \
      --preprocessed-dir data/preprocessed_v5 \
      --mode analyze

  # æ™ºèƒ½æ¨è–¦æ¨¡å¼ï¼ˆè‡ªå‹•çµ„åˆæœ€ä½³æ•¸æ“šé›†ï¼‰
  python scripts/analyze_label_distribution.py \
      --preprocessed-dir data/preprocessed_v5 \
      --mode smart_recommend \
      --start-date 20250901 \
      --target-dist "0.30,0.40,0.30" \
      --min-samples 100000 \
      --output dataset_selection.json

  # äº’å‹•æ¨¡å¼ï¼ˆé¡¯ç¤ºå€™é¸æ–¹æ¡ˆè®“ä½¿ç”¨è€…é¸æ“‡ï¼‰
  python scripts/analyze_label_distribution.py \
      --preprocessed-dir data/preprocessed_v5 \
      --mode interactive \
      --start-date 20250901 \
      --target-dist "0.30,0.40,0.30"

  # ğŸ†• æœ€å°åŒ–æ¨è–¦æ¨¡å¼ï¼ˆé€æª”ç´¯ç©ï¼Œé”æ¨™å³åœï¼Œæ§åˆ¶æ•¸æ“šé‡ï¼‰
  python scripts/analyze_label_distribution.py \
      --preprocessed-dir data/preprocessed_v5 \
      --mode minimal \
      --min-samples 1000000 \
      --days 10 \
      --output dataset_selection_minimal.json

  # ğŸ†• æœ€å°åŒ–æ¨¡å¼ + æŒ‡å®šç›®æ¨™åˆ†å¸ƒ
  python scripts/analyze_label_distribution.py \
      --preprocessed-dir data/preprocessed_v5 \
      --mode minimal \
      --min-samples 1000000 \
      --target-dist "0.30,0.40,0.30" \
      --days 15 \
      --output dataset_selection_minimal.json

ã€è¼¸å‡ºç¯„ä¾‹ã€‘
  å€™é¸æ–¹æ¡ˆ 1:
    - æ—¥æœŸç¯„åœ: 20250901-20250910 (10 å¤©)
    - å€‹è‚¡æ•¸é‡: 145 æª”
    - ç¸½æ¨£æœ¬æ•¸: 1,234,567
    - æ¨™ç±¤åˆ†å¸ƒ: Down 30.2% | Neutral 39.8% | Up 30.0%
    - åå·®åº¦: 0.012 (èˆ‡ç›®æ¨™ 30/40/30 çš„å·®ç•°)

  é¸å–æ–¹æ¡ˆ 1 å¾Œï¼š
    âœ… å·²é¸å– 145 æª”è‚¡ç¥¨ï¼Œ10 å¤©æ•¸æ“š
    ğŸ“‹ æ—¥æœŸåˆ—è¡¨: [20250901, 20250902, ..., 20250910]
    ğŸ¢ å€‹è‚¡åˆ—è¡¨: [2330, 2317, 2454, ..., 6488]
    ğŸ“Š æœ€çµ‚åˆ†å¸ƒ: Down 372,567 (30.2%) | Neutral 490,123 (39.8%) | Up 371,877 (30.0%)

ã€æ¼”ç®—æ³•èªªæ˜ã€‘
  1. æ—¥æœŸæ’åºï¼šæŒ‰æ—¥æœŸå‡åºæ’åˆ—ï¼ˆå¾èµ·å§‹æ—¥æœŸé–‹å§‹ï¼‰
  2. é€æ—¥ç´¯ç©ï¼šé€æ—¥åŠ å…¥è‚¡ç¥¨ï¼Œè¨ˆç®—ç´¯ç©æ¨™ç±¤åˆ†å¸ƒ
  3. åå·®è©•ä¼°ï¼šè¨ˆç®—èˆ‡ç›®æ¨™åˆ†å¸ƒçš„ KL æ•£åº¦æˆ– L2 è·é›¢
  4. å¤šæ–¹æ¡ˆç”Ÿæˆï¼š
     - ä¿å®ˆæ–¹æ¡ˆï¼ˆåå·® < 0.01ï¼Œå¯èƒ½æ¨£æœ¬è¼ƒå°‘ï¼‰
     - å¹³è¡¡æ–¹æ¡ˆï¼ˆåå·® < 0.02ï¼Œæ¨£æœ¬æ•¸é©ä¸­ï¼‰
     - ç©æ¥µæ–¹æ¡ˆï¼ˆåå·® < 0.03ï¼Œæ¨£æœ¬æ•¸è¼ƒå¤šï¼‰
  5. ä½¿ç”¨è€…é¸æ“‡ï¼šé¡¯ç¤º 3-5 å€‹å€™é¸æ–¹æ¡ˆï¼Œè®“ä½¿ç”¨è€…æ±ºå®š

ã€ğŸ†• æœ€å°åŒ–æ¨è–¦æ¼”ç®—æ³•ã€‘(mode=minimal)
  æ ¸å¿ƒç†å¿µï¼šåœ¨ä¸çˆ†é‡çš„å‰æä¸‹ï¼Œçµ„å‡ºé”æ¨™ä¸” Neff æœ€å„ªçš„è¨“ç·´é›†

  æ¼”ç®—æ³•æµç¨‹ï¼š
    1. å¤©æ•¸é™åˆ¶ï¼šåªä½¿ç”¨æœ€è¿‘ --days å¤©çš„æ•¸æ“šï¼ˆé è¨­ 10 å¤©ï¼‰
    2. æŒ‰è‚¡ç¥¨åˆ†çµ„ï¼šæ¯æª”è‚¡ç¥¨åŒ…å«æ‰€æœ‰å¯ç”¨æ—¥æœŸçš„æ•¸æ“š
    3. é€æª”ç´¯ç©ï¼šæŒ‰å›ºå®šé †åºï¼ˆè‚¡ç¥¨ä»£ç¢¼ï¼‰é€æª”åŠ å…¥
    4. å®‰å…¨æª¢æŸ¥ï¼š
       - ä»»ä¸€é¡åˆ¥å æ¯”ä¸å¾— > 60%
       - ä»»ä¸€é¡åˆ¥æ¨£æœ¬æ•¸ä¸å¾—ç‚º 0
    5. é”æ¨™å³åœï¼štotal_samples >= min_samples ä¸”å®‰å…¨æ™‚åœæ­¢
    6. æœ€å„ªé¸æ“‡ï¼š
       - æœªæŒ‡å®š target_distï¼šé¸ Neff æœ€å¤§ + å‡å‹»åˆ†å¸ƒåå·®æœ€å°
       - æŒ‡å®š target_distï¼šé¸ Neff æœ€å¤§ + ç›®æ¨™åˆ†å¸ƒåå·®æœ€å°

  åƒæ•¸èªªæ˜ï¼š
    --min-samples (å¿…å¡«): ç¸½æ¨£æœ¬ä¸‹é™ï¼ˆä¾‹ï¼š1,000,000ï¼‰
    --target-dist (é¸å¡«): ç›®æ¨™åˆ†å¸ƒï¼ˆä¾‹ï¼š0.30,0.40,0.30ï¼‰ï¼Œæœªæä¾›æ™‚ç”¨å‡å‹»åˆ†å¸ƒ
    --days (é¸å¡«): æœ€å¤§å¤©æ•¸ä¸Šé™ï¼ˆé è¨­ 10ï¼‰ï¼Œé¿å…è™•ç†é‡éå¤§

  Neff è¨ˆç®—å…¬å¼ï¼š
    Neff = N / max(w_i)
    å…¶ä¸­ w_i = 1 / p_iï¼ˆé¡åˆ¥ i çš„æ¬Šé‡ï¼‰
    Neff è¶Šå¤§è¡¨ç¤ºé¡åˆ¥è¶Šå¹³è¡¡

ã€ç‰ˆæœ¬ã€‘v3.0
ã€æ›´æ–°ã€‘2025-10-24
ã€ä½œè€…ã€‘DeepLOB-Pro Team
"""

import os
import json
import glob
import argparse
import logging
from typing import List, Dict, Any, Tuple, Optional
from collections import defaultdict
from datetime import datetime

import numpy as np
import pandas as pd
from pathlib import Path

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)


# ============================================================
# è³‡æ–™è¼‰å…¥èˆ‡å‰è™•ç†
# ============================================================

def load_all_stocks_metadata(preprocessed_dir: str, start_date: Optional[str] = None) -> List[Dict]:
    """
    è¼‰å…¥æ‰€æœ‰è‚¡ç¥¨çš„ metadataï¼ˆæ”¯æ´æ—¥æœŸéæ¿¾ï¼‰

    Args:
        preprocessed_dir: é è™•ç†æ•¸æ“šç›®éŒ„
        start_date: èµ·å§‹æ—¥æœŸï¼ˆæ ¼å¼: YYYYMMDDï¼‰ï¼ŒNone è¡¨ç¤ºè¼‰å…¥å…¨éƒ¨

    Returns:
        è‚¡ç¥¨ metadata åˆ—è¡¨ï¼ŒæŒ‰ (date, symbol) å‡åºæ’åˆ—
    """
    all_metadata = []

    # æ‰¾åˆ°æ‰€æœ‰ NPZ æª”æ¡ˆ
    npz_pattern = os.path.join(preprocessed_dir, "daily", "*", "*.npz")
    npz_files = glob.glob(npz_pattern)

    logging.info(f"æƒæåˆ° {len(npz_files)} å€‹ NPZ æª”æ¡ˆ")

    for npz_file in npz_files:
        try:
            data = np.load(npz_file, allow_pickle=True)
            metadata = json.loads(str(data['metadata']))

            # åªä¿ç•™é€šééæ¿¾ä¸”æœ‰æ¨™ç±¤é è¦½çš„è‚¡ç¥¨
            if not metadata.get('pass_filter', False):
                continue
            if metadata.get('label_preview') is None:
                continue

            # æ—¥æœŸéæ¿¾
            date_str = metadata['date']
            if start_date and date_str < start_date:
                continue

            lp = metadata['label_preview']
            all_metadata.append({
                'symbol': metadata['symbol'],
                'date': date_str,
                'file_path': npz_file,
                'range_pct': metadata['range_pct'],
                'n_points': metadata['n_points'],
                'total_labels': lp['total_labels'],
                'down_count': lp['down_count'],
                'neutral_count': lp['neutral_count'],
                'up_count': lp['up_count'],
                'down_pct': lp['down_pct'],
                'neutral_pct': lp['neutral_pct'],
                'up_pct': lp['up_pct']
            })
        except Exception as e:
            logging.debug(f"è®€å– {npz_file} å¤±æ•—: {e}")
            continue

    # æŒ‰æ—¥æœŸå’Œè‚¡ç¥¨ä»£ç¢¼æ’åº
    all_metadata.sort(key=lambda x: (x['date'], x['symbol']))

    logging.info(f"è¼‰å…¥ {len(all_metadata)} æª”æœ‰æ•ˆè‚¡ç¥¨ï¼ˆæ—¥æœŸ >= {start_date or 'å…¨éƒ¨'}ï¼‰")
    return all_metadata


def group_by_date(stocks: List[Dict]) -> Dict[str, List[Dict]]:
    """æŒ‰æ—¥æœŸåˆ†çµ„è‚¡ç¥¨"""
    grouped = defaultdict(list)
    for stock in stocks:
        grouped[stock['date']].append(stock)
    return dict(sorted(grouped.items()))


# ============================================================
# æ¨™ç±¤åˆ†å¸ƒè¨ˆç®—èˆ‡è©•ä¼°
# ============================================================

def calculate_balance_score(stock: Dict) -> float:
    """
    è¨ˆç®—å–®ä¸€è‚¡ç¥¨çš„æ¨™ç±¤å¹³è¡¡åº¦åˆ†æ•¸ï¼ˆè¶Šé«˜è¶Šå¹³è¡¡ï¼‰

    ä½¿ç”¨é¦™è¾²ç†µ (Shannon Entropy) ä¾†è¡¡é‡åˆ†å¸ƒå‡å‹»åº¦
    å®Œå…¨å‡å‹»åˆ†å¸ƒæ™‚ç†µæœ€å¤§ (log(3) â‰ˆ 1.099)
    å®Œå…¨ä¸å¹³è¡¡æ™‚ç†µç‚º 0

    Args:
        stock: å–®ä¸€è‚¡ç¥¨çš„ metadata

    Returns:
        å¹³è¡¡åº¦åˆ†æ•¸ (0~1ï¼Œ1 è¡¨ç¤ºå®Œå…¨å¹³è¡¡)
    """
    # æå–ä¸‰é¡å æ¯”
    down_pct = stock.get('down_pct', 0.0)
    neutral_pct = stock.get('neutral_pct', 0.0)
    up_pct = stock.get('up_pct', 0.0)

    # é¿å… log(0)
    probs = [max(p, 1e-10) for p in [down_pct, neutral_pct, up_pct]]

    # è¨ˆç®—é¦™è¾²ç†µ
    entropy = -sum(p * np.log(p) for p in probs)

    # æ­£è¦åŒ–åˆ° [0, 1]ï¼ˆæœ€å¤§ç†µç‚º log(3)ï¼‰
    max_entropy = np.log(3)
    balance_score = entropy / max_entropy

    return float(balance_score)


def calculate_stock_group_balance(stocks: List[Dict]) -> float:
    """
    è¨ˆç®—ä¸€çµ„è‚¡ç¥¨ï¼ˆåŒä¸€æª”è‚¡ç¥¨çš„å¤šå¤©æ•¸æ“šï¼‰çš„å¹³å‡å¹³è¡¡åº¦

    Args:
        stocks: åŒä¸€æª”è‚¡ç¥¨çš„æ‰€æœ‰æ—¥æœŸæ•¸æ“š

    Returns:
        å¹³å‡å¹³è¡¡åº¦åˆ†æ•¸
    """
    if not stocks:
        return 0.0

    scores = [calculate_balance_score(s) for s in stocks]
    return float(np.mean(scores))


def calculate_neff(stocks: List[Dict]) -> float:
    """
    è¨ˆç®—æœ‰æ•ˆæ¨£æœ¬æ•¸ (Neff) - ä½¿ç”¨é¡åˆ¥æ¬Šé‡å€’æ•¸åŠ æ¬Š

    å…¬å¼: Neff = N / max(w_i)
    å…¶ä¸­ w_i æ˜¯å„é¡åˆ¥çš„æ¬Šé‡ (1 / p_i)

    Args:
        stocks: è‚¡ç¥¨ metadata åˆ—è¡¨

    Returns:
        æœ‰æ•ˆæ¨£æœ¬æ•¸ Neff (è¶Šå¤§è¶Šå¥½)
    """
    dist = calculate_distribution(stocks)

    # é¿å…é™¤ä»¥é›¶
    down_pct = max(dist['down_pct'], 1e-10)
    neutral_pct = max(dist['neutral_pct'], 1e-10)
    up_pct = max(dist['up_pct'], 1e-10)

    # è¨ˆç®—å„é¡åˆ¥æ¬Šé‡
    w_down = 1.0 / down_pct
    w_neutral = 1.0 / neutral_pct
    w_up = 1.0 / up_pct

    # Neff = N / max(w_i)
    max_weight = max(w_down, w_neutral, w_up)
    neff = dist['total_samples'] / max_weight

    return float(neff)


def calculate_distribution(stocks: List[Dict]) -> Dict[str, Any]:
    """
    è¨ˆç®—çµ¦å®šè‚¡ç¥¨åˆ—è¡¨çš„æ¨™ç±¤åˆ†å¸ƒ

    Returns:
        {
            'total_stocks': int,
            'total_samples': int,
            'down_count': int,
            'neutral_count': int,
            'up_count': int,
            'down_pct': float,
            'neutral_pct': float,
            'up_pct': float
        }
    """
    total_down = sum(s['down_count'] for s in stocks)
    total_neutral = sum(s['neutral_count'] for s in stocks)
    total_up = sum(s['up_count'] for s in stocks)
    total_all = total_down + total_neutral + total_up

    return {
        'total_stocks': len(stocks),
        'total_samples': total_all,
        'down_count': total_down,
        'neutral_count': total_neutral,
        'up_count': total_up,
        'down_pct': total_down / total_all if total_all > 0 else 0.0,
        'neutral_pct': total_neutral / total_all if total_all > 0 else 0.0,
        'up_pct': total_up / total_all if total_all > 0 else 0.0
    }


def calculate_deviation(
    current_dist: Tuple[float, float, float],
    target_dist: Tuple[float, float, float],
    method: str = 'l2'
) -> float:
    """
    è¨ˆç®—ç•¶å‰åˆ†å¸ƒèˆ‡ç›®æ¨™åˆ†å¸ƒçš„åå·®åº¦

    Args:
        current_dist: (down%, neutral%, up%)
        target_dist: (down%, neutral%, up%)
        method: 'l2' (L2è·é›¢) æˆ– 'kl' (KLæ•£åº¦)

    Returns:
        åå·®åº¦ï¼ˆè¶Šå°è¶Šå¥½ï¼‰
    """
    c = np.array(current_dist)
    t = np.array(target_dist)

    if method == 'l2':
        # L2 è·é›¢ï¼ˆæ­å¼è·é›¢ï¼‰
        return float(np.sqrt(np.sum((c - t) ** 2)))
    elif method == 'kl':
        # KL æ•£åº¦ï¼ˆå°æ•¸å·®ç•°ï¼‰
        epsilon = 1e-10
        c = np.clip(c, epsilon, 1.0)
        t = np.clip(t, epsilon, 1.0)
        return float(np.sum(t * np.log(t / c)))
    else:
        raise ValueError(f"æœªçŸ¥çš„åå·®è¨ˆç®—æ–¹æ³•: {method}")


# ============================================================
# æ™ºèƒ½æ¨è–¦æ¼”ç®—æ³•ï¼ˆæ ¸å¿ƒï¼‰
# ============================================================

def smart_minimal_recommend(
    stocks: List[Dict],
    min_samples: int = 1000000,
    target_dist: Optional[Tuple[float, float, float]] = None,
    max_days: int = 10
) -> Dict:
    """
    æ™ºèƒ½æœ€å°åŒ–æ¨è–¦æ¼”ç®—æ³•ï¼ˆæ–°ç‰ˆï¼‰

    ç›®æ¨™: åœ¨ä¸çˆ†é‡çš„å‰æä¸‹ï¼Œçµ„å‡ºé”åˆ°æ¨£æœ¬é–€æª»ä¸” Neff æœ€ä½³ã€é¡åˆ¥å¹³è¡¡çš„è¨“ç·´è³‡æ–™çµ„

    æ¼”ç®—æ³•é‚è¼¯:
      1. æŒ‰è‚¡ç¥¨åˆ†çµ„ï¼ˆæ¯æª”è‚¡ç¥¨åŒ…å«å¤šå¤©æ•¸æ“šï¼‰
      2. é€æª”ç´¯ç©åŠ å…¥è‚¡ç¥¨ï¼ˆå›ºå®šé †åºï¼šæŒ‰å¸‚å€¼æˆ–ä»£ç¢¼æ’åºï¼‰
      3. é”æ¨™å³åœï¼štotal_samples >= min_samples æ™‚åœæ­¢åŠ å…¥ä¸‹ä¸€æª”
      4. å¤©æ•¸é™åˆ¶ï¼šåªä½¿ç”¨æœ€è¿‘ max_days å¤©çš„æ•¸æ“š
      5. å®‰å…¨é–¾å€¼æª¢æŸ¥ï¼š
         - ä»»ä¸€é¡åˆ¥å æ¯”ä¸å¾— > 60%
         - ä»»ä¸€é¡åˆ¥æ¨£æœ¬æ•¸ä¸å¾—ç‚º 0
      6. æœ€å„ªé¸æ“‡ï¼š
         - æœªæŒ‡å®š target_distï¼šé¸ Neff æœ€å¤§ä¸”èˆ‡å‡å‹»åˆ†å¸ƒåå·®æœ€å°çš„çµ„åˆ
         - æŒ‡å®š target_distï¼šé¸ Neff æœ€å¤§ä¸”èˆ‡ç›®æ¨™åˆ†å¸ƒåå·®æœ€å°çš„çµ„åˆ

    Args:
        stocks: æ‰€æœ‰è‚¡ç¥¨ metadataï¼ˆå·²æŒ‰æ—¥æœŸæ’åºï¼‰
        min_samples: ç¸½æ¨£æœ¬ä¸‹é™ï¼ˆå¿…å¡«ï¼‰
        target_dist: ä¸‰åˆ†é¡ç›®æ¨™åˆ†å¸ƒ (down%, neutral%, up%)ï¼ŒNone è¡¨ç¤ºä½¿ç”¨å‡å‹»åˆ†å¸ƒ
        max_days: åƒè€ƒçš„æœ€å¤§å¤©æ•¸ä¸Šé™ï¼ˆé è¨­ 10 å¤©ï¼‰

    Returns:
        æœ€ä½³æ¨è–¦æ–¹æ¡ˆå­—å…¸ï¼Œè‹¥ç„¡æ³•é”æ¨™å‰‡è¿”å›æœ€ä½³å¯è¡Œè§£ï¼ˆå¸¶ warningï¼‰
    """
    # é è¨­ä½¿ç”¨å‡å‹»åˆ†å¸ƒ
    if target_dist is None:
        target_dist = (1/3, 1/3, 1/3)
        logging.info("æœªæŒ‡å®š --target-distï¼Œä½¿ç”¨å‡å‹»åˆ†å¸ƒä½œç‚ºå¹³è¡¡ç›®æ¨™: (0.333, 0.333, 0.333)")

    # å¤©æ•¸éæ¿¾ï¼šåªä¿ç•™æœ€è¿‘ max_days å¤©çš„æ•¸æ“š
    grouped_by_date = group_by_date(stocks)
    sorted_dates = sorted(grouped_by_date.keys(), reverse=True)  # é™åºï¼ˆæœ€æ–°åœ¨å‰ï¼‰

    if len(sorted_dates) > max_days:
        selected_dates = sorted(sorted_dates[:max_days])  # å–æœ€æ–° max_days å¤©ï¼Œå†å‡åº
        logging.info(f"å¤©æ•¸é™åˆ¶: åƒ…ä½¿ç”¨æœ€è¿‘ {max_days} å¤©æ•¸æ“šï¼ˆ{selected_dates[0]} - {selected_dates[-1]}ï¼‰")
        filtered_stocks = [s for s in stocks if s['date'] in selected_dates]
    else:
        selected_dates = sorted_dates
        filtered_stocks = stocks
        logging.info(f"å¤©æ•¸é™åˆ¶: ç¸½å…± {len(selected_dates)} å¤©æ•¸æ“šï¼Œå…¨éƒ¨ä½¿ç”¨")

    # æŒ‰è‚¡ç¥¨åˆ†çµ„ï¼ˆæ¯æª”è‚¡ç¥¨åŒ…å«æ‰€æœ‰æ—¥æœŸçš„æ•¸æ“šï¼‰
    grouped_by_symbol = defaultdict(list)
    for stock in filtered_stocks:
        grouped_by_symbol[stock['symbol']].append(stock)

    # ğŸ†• æŒ‰æ¨™ç±¤å¹³è¡¡åº¦æ’åºï¼ˆå¹³è¡¡åº¦é«˜çš„å„ªå…ˆï¼‰
    # è¨ˆç®—æ¯æª”è‚¡ç¥¨çš„å¹³å‡å¹³è¡¡åº¦åˆ†æ•¸
    symbol_balance_scores = {
        symbol: calculate_stock_group_balance(stocks)
        for symbol, stocks in grouped_by_symbol.items()
    }

    # æŒ‰å¹³è¡¡åº¦é™åºæ’åºï¼ˆå¹³è¡¡åº¦é«˜çš„åœ¨å‰ï¼‰
    sorted_symbols = sorted(
        grouped_by_symbol.keys(),
        key=lambda s: symbol_balance_scores[s],
        reverse=True
    )

    logging.info("  è‚¡ç¥¨æ’åº: æŒ‰æ¨™ç±¤å¹³è¡¡åº¦ï¼ˆå¹³è¡¡ -> ä¸å¹³è¡¡ï¼‰")
    logging.info(f"  å¹³è¡¡åº¦ç¯„åœ: {min(symbol_balance_scores.values()):.3f} ~ {max(symbol_balance_scores.values()):.3f}")

    logging.info("\né–‹å§‹æ™ºèƒ½æœ€å°åŒ–æ¨è–¦ï¼š")
    logging.info(f"  ç›®æ¨™åˆ†å¸ƒ: Down {target_dist[0]:.1%} | Neutral {target_dist[1]:.1%} | Up {target_dist[2]:.1%}")
    logging.info(f"  æœ€å°æ¨£æœ¬æ•¸: {min_samples:,}")
    logging.info(f"  å¯ç”¨è‚¡ç¥¨æ•¸: {len(sorted_symbols)} æª”")
    logging.info(f"  å¤©æ•¸ç¯„åœ: {len(selected_dates)} å¤©")

    # é€æª”ç´¯ç©
    cumulative_stocks = []
    candidates = []

    for symbol in sorted_symbols:
        # åŠ å…¥ç•¶å‰è‚¡ç¥¨çš„æ‰€æœ‰æ—¥æœŸæ•¸æ“š
        cumulative_stocks.extend(grouped_by_symbol[symbol])

        # è¨ˆç®—ç´¯ç©åˆ†å¸ƒ
        dist = calculate_distribution(cumulative_stocks)

        # å®‰å…¨é–¾å€¼æª¢æŸ¥
        is_safe = True
        reasons = []

        # æª¢æŸ¥æ˜¯å¦æœ‰é¡åˆ¥ç‚º 0
        if dist['down_count'] == 0:
            is_safe = False
            reasons.append("Down é¡åˆ¥æ¨£æœ¬æ•¸ç‚º 0")
        if dist['neutral_count'] == 0:
            is_safe = False
            reasons.append("Neutral é¡åˆ¥æ¨£æœ¬æ•¸ç‚º 0")
        if dist['up_count'] == 0:
            is_safe = False
            reasons.append("Up é¡åˆ¥æ¨£æœ¬æ•¸ç‚º 0")

        # æª¢æŸ¥æ˜¯å¦æœ‰é¡åˆ¥ > 80% (å¯¬é¬†é–¾å€¼ï¼Œé©æ‡‰è¶¨å‹¢æ¨™ç±¤æ•¸æ“š)
        if dist['down_pct'] > 0.80:
            is_safe = False
            reasons.append(f"Down é¡åˆ¥å æ¯”éé«˜ ({dist['down_pct']:.1%} > 80%)")
        if dist['neutral_pct'] > 0.80:
            is_safe = False
            reasons.append(f"Neutral é¡åˆ¥å æ¯”éé«˜ ({dist['neutral_pct']:.1%} > 80%)")
        if dist['up_pct'] > 0.80:
            is_safe = False
            reasons.append(f"Up é¡åˆ¥å æ¯”éé«˜ ({dist['up_pct']:.1%} > 80%)")

        # è¨ˆç®—åå·®å’Œ Neff
        current_dist = (dist['down_pct'], dist['neutral_pct'], dist['up_pct'])
        deviation = calculate_deviation(current_dist, target_dist, method='l2')
        neff = calculate_neff(cumulative_stocks)

        # è¨˜éŒ„ç‚ºå€™é¸æ–¹æ¡ˆï¼ˆç„¡è«–æ˜¯å¦é”æ¨™æˆ–å®‰å…¨ï¼‰
        num_symbols = len(set(s['symbol'] for s in cumulative_stocks))
        dates_used = sorted(set(s['date'] for s in cumulative_stocks))

        candidate = {
            'symbols': sorted(set(s['symbol'] for s in cumulative_stocks)),
            'dates': dates_used,
            'date_range': f"{dates_used[0]}-{dates_used[-1]}",
            'num_dates': len(dates_used),
            'num_stocks': num_symbols,
            'stock_records': cumulative_stocks.copy(),
            'total_records': len(cumulative_stocks),
            'distribution': dist,
            'deviation': deviation,
            'neff': neff,
            'is_safe': is_safe,
            'is_sufficient': dist['total_samples'] >= min_samples,
            'reasons': reasons
        }

        candidates.append(candidate)

        # é”æ¨™å³åœï¼ˆæ¨£æœ¬æ•¸å¤  + å®‰å…¨ï¼‰
        if dist['total_samples'] >= min_samples and is_safe:
            logging.info(f"  âœ… é”æ¨™ï¼ç´¯ç© {num_symbols} æª”è‚¡ç¥¨ï¼Œ{dist['total_samples']:,} æ¨£æœ¬")
            logging.info(f"     åå·®: {deviation:.4f}, Neff: {neff:,.0f}")
            break

        # èª¿è©¦ä¿¡æ¯
        if dist['total_samples'] % 100000 < 50000 or not is_safe:  # æ¯ 10 è¬æ¨£æœ¬æˆ–ä¸å®‰å…¨æ™‚è¼¸å‡º
            status = "âœ…" if is_safe else "âš ï¸"
            logging.debug(f"  {status} {num_symbols:>3} æª”: {dist['total_samples']:>9,} æ¨£æœ¬, "
                         f"åå·® {deviation:.4f}, Neff {neff:>10,.0f}")
            if not is_safe:
                logging.debug(f"      åŸå› : {', '.join(reasons)}")

    # åœ¨æ‰€æœ‰å€™é¸ä¸­é¸æ“‡æœ€å„ªè§£
    valid_candidates = [c for c in candidates if c['is_safe']]
    sufficient_candidates = [c for c in valid_candidates if c['is_sufficient']]

    if sufficient_candidates:
        # æœ‰é”æ¨™çš„å®‰å…¨æ–¹æ¡ˆï¼Œé¸ Neff æœ€å¤§ + åå·®æœ€å°
        best = max(sufficient_candidates, key=lambda x: (x['neff'], -x['deviation']))
        best['status'] = 'success'
        best['message'] = f"æˆåŠŸé”æ¨™ï¼Neff: {best['neff']:,.0f}, åå·®: {best['deviation']:.4f}"
        logging.info(f"\nâœ… {best['message']}")
    elif valid_candidates:
        # æ²’æœ‰é”æ¨™ä½†æœ‰å®‰å…¨æ–¹æ¡ˆï¼Œé¸æœ€æ¥è¿‘ç›®æ¨™çš„ï¼ˆè­¦å‘Šï¼‰
        best = max(valid_candidates, key=lambda x: (x['neff'], -x['deviation']))
        best['status'] = 'warning'
        best['message'] = (f"æœªé”æ¨£æœ¬é–€æª» ({best['distribution']['total_samples']:,} < {min_samples:,})ï¼Œ"
                          f"è¿”å›æœ€ä½³å¯è¡Œè§£ï¼ˆNeff: {best['neff']:,.0f}ï¼‰")
        logging.warning(f"\nâš ï¸  {best['message']}")
    else:
        # å®Œå…¨ç„¡å®‰å…¨æ–¹æ¡ˆï¼Œé¸æœ€å®‰å…¨çš„ï¼ˆéŒ¯èª¤ï¼‰
        best = min(candidates, key=lambda x: len(x['reasons']))
        best['status'] = 'error'
        best['message'] = f"ç„¡æ³•æ‰¾åˆ°ç¬¦åˆå®‰å…¨é–¾å€¼çš„æ–¹æ¡ˆï¼æœ€æ¥è¿‘æ–¹æ¡ˆå•é¡Œ: {', '.join(best['reasons'])}"
        logging.error(f"\nâŒ {best['message']}")

    return best


def smart_recommend_datasets(
    stocks: List[Dict],
    target_dist: Tuple[float, float, float] = (0.30, 0.40, 0.30),
    min_samples: int = 100000,
    max_deviation_levels: List[float] = [0.01, 0.02, 0.03, 0.05]
) -> List[Dict]:
    """
    æ™ºèƒ½æ¨è–¦æ•¸æ“šé›†çµ„åˆï¼ˆé€æ—¥ç´¯ç©ï¼Œå¤šæ–¹æ¡ˆç”Ÿæˆï¼‰

    æ¼”ç®—æ³•é‚è¼¯ï¼š
      1. æŒ‰æ—¥æœŸåˆ†çµ„ï¼ˆå‡åºï¼‰
      2. é€æ—¥ç´¯ç©åŠ å…¥è‚¡ç¥¨
      3. æ¯æ¬¡ç´¯ç©å¾Œè¨ˆç®—æ¨™ç±¤åˆ†å¸ƒå’Œåå·®åº¦
      4. ç•¶é”åˆ°ä¸åŒåå·®é–¾å€¼æ™‚ï¼Œè¨˜éŒ„ç‚ºå€™é¸æ–¹æ¡ˆ
      5. è¿”å›å¤šå€‹å€™é¸æ–¹æ¡ˆä¾›ä½¿ç”¨è€…é¸æ“‡

    Args:
        stocks: æ‰€æœ‰è‚¡ç¥¨ metadataï¼ˆå·²æŒ‰æ—¥æœŸæ’åºï¼‰
        target_dist: ç›®æ¨™æ¨™ç±¤åˆ†å¸ƒ (down%, neutral%, up%)
        min_samples: æœ€å°æ¨£æœ¬æ•¸ï¼ˆéæ¿¾æ‰æ¨£æœ¬å¤ªå°‘çš„æ–¹æ¡ˆï¼‰
        max_deviation_levels: åå·®é–¾å€¼åˆ—è¡¨ï¼ˆç”Ÿæˆå¤šå€‹æ–¹æ¡ˆï¼‰

    Returns:
        å€™é¸æ–¹æ¡ˆåˆ—è¡¨ï¼ŒæŒ‰åå·®åº¦æ’åºï¼ˆæœ€ä½³æ–¹æ¡ˆåœ¨å‰ï¼‰
        æ¯å€‹æ–¹æ¡ˆåŒ…å«ï¼š
          - dates: æ—¥æœŸåˆ—è¡¨
          - stocks: è‚¡ç¥¨åˆ—è¡¨
          - distribution: æ¨™ç±¤åˆ†å¸ƒ
          - deviation: åå·®åº¦
          - description: æ–¹æ¡ˆæè¿°
    """
    grouped = group_by_date(stocks)
    sorted_dates = sorted(grouped.keys())

    logging.info(f"\né–‹å§‹æ™ºèƒ½æ¨è–¦ï¼š")
    logging.info(f"  ç›®æ¨™åˆ†å¸ƒ: Down {target_dist[0]:.1%} | Neutral {target_dist[1]:.1%} | Up {target_dist[2]:.1%}")
    logging.info(f"  æœ€å°æ¨£æœ¬æ•¸: {min_samples:,}")
    logging.info(f"  æ—¥æœŸç¯„åœ: {sorted_dates[0]} - {sorted_dates[-1]} ({len(sorted_dates)} å¤©)")

    # é€æ—¥ç´¯ç©
    cumulative_stocks = []
    candidates = []
    found_levels = set()

    for date in sorted_dates:
        # åŠ å…¥ç•¶æ—¥æ‰€æœ‰è‚¡ç¥¨
        cumulative_stocks.extend(grouped[date])

        # è¨ˆç®—ç´¯ç©åˆ†å¸ƒ
        dist = calculate_distribution(cumulative_stocks)

        # æ¨£æœ¬æ•¸éæ¿¾
        if dist['total_samples'] < min_samples:
            continue

        # è¨ˆç®—åå·®åº¦
        current_dist = (dist['down_pct'], dist['neutral_pct'], dist['up_pct'])
        deviation = calculate_deviation(current_dist, target_dist, method='l2')

        # æª¢æŸ¥æ˜¯å¦ç¬¦åˆä»»ä¸€åå·®é–¾å€¼ï¼ˆä¸”å°šæœªè¨˜éŒ„è©²ç­‰ç´šï¼‰
        for level in max_deviation_levels:
            if deviation <= level and level not in found_levels:
                found_levels.add(level)

                # ç”Ÿæˆæ–¹æ¡ˆæè¿°
                if level <= 0.01:
                    desc = "ä¿å®ˆæ–¹æ¡ˆï¼ˆæœ€é«˜ç²¾åº¦ï¼Œåå·® < 1%ï¼‰"
                elif level <= 0.02:
                    desc = "å¹³è¡¡æ–¹æ¡ˆï¼ˆé«˜ç²¾åº¦ï¼Œåå·® < 2%ï¼‰"
                elif level <= 0.03:
                    desc = "ç©æ¥µæ–¹æ¡ˆï¼ˆä¸­ç­‰ç²¾åº¦ï¼Œåå·® < 3%ï¼‰"
                else:
                    desc = "å¯¬é¬†æ–¹æ¡ˆï¼ˆè¼ƒå¤§æ¨£æœ¬ï¼Œåå·® < 5%ï¼‰"

                # æ”¶é›†æ—¥æœŸå’Œè‚¡ç¥¨åˆ—è¡¨
                dates_used = sorted(set(s['date'] for s in cumulative_stocks))
                symbols_used = sorted(set(s['symbol'] for s in cumulative_stocks))

                candidates.append({
                    'dates': dates_used,
                    'date_range': f"{dates_used[0]}-{dates_used[-1]}",
                    'num_dates': len(dates_used),
                    'symbols': symbols_used,
                    'num_stocks': len(symbols_used),
                    'stock_records': cumulative_stocks.copy(),  # å®Œæ•´è¨˜éŒ„ï¼ˆå«é‡è¤‡æ—¥æœŸï¼‰
                    'total_records': len(cumulative_stocks),
                    'distribution': dist,
                    'deviation': deviation,
                    'level': level,
                    'description': desc
                })

                logging.info(f"  âœ… æ‰¾åˆ°å€™é¸æ–¹æ¡ˆï¼ˆåå·® {deviation:.4f} <= {level:.2f}ï¼‰: "
                           f"{len(dates_used)} å¤©, {len(symbols_used)} æª”, {dist['total_samples']:,} æ¨£æœ¬")

                break  # ä¸€å€‹æ–¹æ¡ˆåªè¨˜éŒ„æœ€ç·Šçš„é–¾å€¼

    # æŒ‰åå·®åº¦æ’åºï¼ˆæœ€ä½³åœ¨å‰ï¼‰
    candidates.sort(key=lambda x: x['deviation'])

    logging.info(f"\nå…±ç”Ÿæˆ {len(candidates)} å€‹å€™é¸æ–¹æ¡ˆ")
    return candidates


# ============================================================
# äº’å‹•å¼é¸æ“‡ç•Œé¢
# ============================================================

def display_candidates(candidates: List[Dict], target_dist: Tuple[float, float, float]) -> None:
    """é¡¯ç¤ºå€™é¸æ–¹æ¡ˆï¼ˆç¾åŒ–è¡¨æ ¼ï¼‰"""
    print("\n" + "="*100)
    print("ğŸ“‹ å€™é¸æ•¸æ“šé›†æ–¹æ¡ˆ")
    print("="*100)

    print(f"\nğŸ¯ ç›®æ¨™åˆ†å¸ƒ: Down {target_dist[0]:.1%} | Neutral {target_dist[1]:.1%} | Up {target_dist[2]:.1%}")
    print(f"\nå…±æ‰¾åˆ° {len(candidates)} å€‹å€™é¸æ–¹æ¡ˆï¼ˆæŒ‰åå·®åº¦æ’åºï¼‰ï¼š\n")

    for i, cand in enumerate(candidates, 1):
        dist = cand['distribution']
        print(f"ã€æ–¹æ¡ˆ {i}ã€‘{cand['description']}")
        print(f"  ğŸ“… æ—¥æœŸç¯„åœ: {cand['date_range']} ({cand['num_dates']} å¤©)")
        print(f"  ğŸ¢ å€‹è‚¡æ•¸é‡: {cand['num_stocks']} æª”")
        print(f"  ğŸ“Š ç¸½æ¨£æœ¬æ•¸: {dist['total_samples']:,}")
        print(f"  ğŸ“ˆ æ¨™ç±¤åˆ†å¸ƒ: Down {dist['down_pct']:.2%} ({dist['down_count']:,}) | "
              f"Neutral {dist['neutral_pct']:.2%} ({dist['neutral_count']:,}) | "
              f"Up {dist['up_pct']:.2%} ({dist['up_count']:,})")
        print(f"  ğŸ“ åå·®åº¦: {cand['deviation']:.4f}")
        print()


def interactive_selection(candidates: List[Dict], target_dist: Tuple[float, float, float]) -> Optional[Dict]:
    """äº’å‹•å¼é¸æ“‡å€™é¸æ–¹æ¡ˆ"""
    if not candidates:
        logging.error("æ²’æœ‰å€™é¸æ–¹æ¡ˆå¯ä¾›é¸æ“‡ï¼")
        return None

    display_candidates(candidates, target_dist)

    while True:
        try:
            choice = input(f"è«‹é¸æ“‡æ–¹æ¡ˆ (1-{len(candidates)})ï¼Œæˆ–è¼¸å…¥ 'q' é€€å‡º: ").strip()

            if choice.lower() == 'q':
                logging.info("ä½¿ç”¨è€…å–æ¶ˆé¸æ“‡")
                return None

            idx = int(choice) - 1
            if 0 <= idx < len(candidates):
                return candidates[idx]
            else:
                print(f"âŒ ç„¡æ•ˆé¸æ“‡ï¼Œè«‹è¼¸å…¥ 1-{len(candidates)}")
        except ValueError:
            print("âŒ è«‹è¼¸å…¥æœ‰æ•ˆæ•¸å­—")
        except KeyboardInterrupt:
            print("\nä½¿ç”¨è€…ä¸­æ–·")
            return None


# ============================================================
# é¸å–å¾Œçš„è©³ç´°å ±å‘Š
# ============================================================

def print_selection_report(selected: Dict, target_dist: Tuple[float, float, float]) -> None:
    """åˆ—å°é¸å–æ–¹æ¡ˆçš„è©³ç´°å ±å‘Š"""
    dist = selected['distribution']

    print("\n" + "="*100)
    print("[SELECTED] å·²é¸å–æ•¸æ“šé›†")
    print("="*100)

    # é¡¯ç¤ºæ–¹æ¡ˆæè¿°å’Œåå·®åº¦ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    if 'description' in selected:
        print(f"\n[Description] {selected['description']}")
    print(f"[Deviation] {selected['deviation']:.4f}")
    if 'neff' in selected:
        print(f"[Neff] {selected['neff']:,.0f}\n")
    else:
        print()

    print("[Date List]")
    dates = selected['dates']
    print(f"  Range: {dates[0]} - {dates[-1]}")
    print(f"  Days: {len(dates)}")
    print(f"  Details: {', '.join(dates)}\n")

    print("[Stock List]")
    symbols = selected['symbols']
    print(f"  Count: {len(symbols)} stocks (unique)")
    if len(symbols) <= 20:
        print(f"  Details: {', '.join(symbols)}\n")
    else:
        print(f"  First 20: {', '.join(symbols[:20])} ...")
        print("  (See full list in output JSON)\n")

    print("[File Pairs]")
    print(f"  Total: {selected['total_records']} records (date x symbol pairs)")
    print("  Note: Each pair corresponds to one NPZ file")

    # é¡¯ç¤ºå‰ 10 å€‹é…å°ç¯„ä¾‹
    stock_records = selected['stock_records']
    print("  Sample (first 10):")
    for i, record in enumerate(stock_records[:10], 1):
        print(f"    {i:>2}. {record['date']}-{record['symbol']:<6} -> {record['file_path']}")
    if len(stock_records) > 10:
        print(f"    ... ({len(stock_records) - 10} more pairs, see JSON)\n")
    else:
        print()

    print("[Total Samples]")
    print(f"  {dist['total_samples']:,} samples (sum of all pairs)\n")

    print("[Label Distribution]")
    print(f"  Down:    {dist['down_count']:>12,} ({dist['down_pct']:>6.2%})  [Target: {target_dist[0]:.2%}, Deviation: {dist['down_pct'] - target_dist[0]:+.2%}]")
    print(f"  Neutral: {dist['neutral_count']:>12,} ({dist['neutral_pct']:>6.2%})  [Target: {target_dist[1]:.2%}, Deviation: {dist['neutral_pct'] - target_dist[1]:+.2%}]")
    print(f"  Up:      {dist['up_count']:>12,} ({dist['up_pct']:>6.2%})  [Target: {target_dist[2]:.2%}, Deviation: {dist['up_pct'] - target_dist[2]:+.2%}]")

    print("\n" + "="*100 + "\n")


def save_selection_to_json(selected: Dict, output_path: str) -> None:
    """ä¿å­˜é¸å–çµæœåˆ° JSON"""
    # å¾ stock_records ç”Ÿæˆã€Œæ—¥æœŸ+è‚¡ç¥¨ã€é…å°åˆ—è¡¨
    stock_records = selected['stock_records']
    file_list = [
        {
            'date': record['date'],
            'symbol': record['symbol'],
            'file_path': record['file_path'],
            'n_points': record['n_points'],
            'total_labels': record['total_labels'],
            'down_count': record['down_count'],
            'neutral_count': record['neutral_count'],
            'up_count': record['up_count']
        }
        for record in stock_records
    ]

    # æŒ‰ç…§ (æ—¥æœŸ, è‚¡ç¥¨ä»£ç¢¼) æ’åº
    file_list.sort(key=lambda x: (x['date'], x['symbol']))

    output_data = {
        'description': selected.get('description', 'Minimal recommend result'),
        'date_range': selected['date_range'],
        'dates': selected['dates'],
        'num_dates': selected['num_dates'],
        'symbols': selected['symbols'],
        'num_stocks': selected['num_stocks'],
        'total_records': selected['total_records'],
        'file_list': file_list,  # å®Œæ•´çš„ã€Œæ—¥æœŸ+è‚¡ç¥¨ã€é…å°åˆ—è¡¨
        'distribution': selected['distribution'],
        'deviation': selected['deviation'],
        'level': selected.get('level', 'N/A'),
        'neff': selected.get('neff', 0),
        'status': selected.get('status', 'unknown'),
        'mode': selected.get('mode', 'N/A'),
        'max_days': selected.get('max_days', 'N/A')
    }

    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(output_data, f, ensure_ascii=False, indent=2)

    logging.info(f"[SAVED] é¸å–çµæœå·²ä¿å­˜åˆ°: {output_path}")


# ============================================================
# å‚³çµ±åˆ†ææ¨¡å¼ï¼ˆä¿ç•™åŸåŠŸèƒ½ï¼‰
# ============================================================

def analyze_overall_distribution(stocks: List[Dict]) -> Dict:
    """åˆ†ææ•´é«”æ¨™ç±¤åˆ†å¸ƒï¼ˆèˆŠç‰ˆç›¸å®¹ï¼‰"""
    return calculate_distribution(stocks)


def group_by_neutral_ratio(stocks: List[Dict]) -> Dict[str, List[Dict]]:
    """æŒ‰æŒå¹³æ¯”ä¾‹åˆ†çµ„"""
    groups = {
        'very_low': [],    # < 10%
        'low': [],         # 10-30%
        'medium': [],      # 30-50%
        'high': [],        # 50-70%
        'very_high': []    # > 70%
    }

    for stock in stocks:
        neutral_pct = stock['neutral_pct']
        if neutral_pct < 0.10:
            groups['very_low'].append(stock)
        elif neutral_pct < 0.30:
            groups['low'].append(stock)
        elif neutral_pct < 0.50:
            groups['medium'].append(stock)
        elif neutral_pct < 0.70:
            groups['high'].append(stock)
        else:
            groups['very_high'].append(stock)

    return groups


def print_summary_report(stocks: List[Dict]):
    """åˆ—å°æ‘˜è¦å ±å‘Šï¼ˆèˆŠç‰ˆç›¸å®¹ï¼‰"""
    print("\n" + "="*80)
    print("æ¨™ç±¤åˆ†å¸ƒåˆ†æå ±å‘Š")
    print("="*80)

    # æ•´é«”åˆ†å¸ƒ
    overall = analyze_overall_distribution(stocks)
    print(f"\nğŸ“Š æ•´é«”æ¨™ç±¤åˆ†å¸ƒ ({overall['total_stocks']} æª”è‚¡ç¥¨):")
    print(f"   ç¸½æ¨£æœ¬æ•¸: {overall['total_samples']:,}")
    print(f"   Down:    {overall['down_count']:>10,} ({overall['down_pct']:>6.2%})")
    print(f"   Neutral: {overall['neutral_count']:>10,} ({overall['neutral_pct']:>6.2%})")
    print(f"   Up:      {overall['up_count']:>10,} ({overall['up_pct']:>6.2%})")

    # æŒ‰æ—¥æœŸåˆ†çµ„
    grouped = group_by_date(stocks)
    print(f"\nğŸ“… æŒ‰æ—¥æœŸåˆ†çµ„ ({len(grouped)} å¤©):")
    for date, stocks_in_date in list(grouped.items())[:5]:  # åªé¡¯ç¤ºå‰5å¤©
        dist = calculate_distribution(stocks_in_date)
        print(f"   {date}: {len(stocks_in_date)} æª”, {dist['total_samples']:,} æ¨£æœ¬, "
              f"Down {dist['down_pct']:.1%} | Neutral {dist['neutral_pct']:.1%} | Up {dist['up_pct']:.1%}")
    if len(grouped) > 5:
        print(f"   ... (é‚„æœ‰ {len(grouped) - 5} å¤©)")

    # æŒ‰æŒå¹³æ¯”ä¾‹åˆ†çµ„
    groups = group_by_neutral_ratio(stocks)
    print(f"\nğŸ“ˆ æŒ‰æŒå¹³æ¯”ä¾‹åˆ†çµ„:")
    for group_name, group_stocks in groups.items():
        if not group_stocks:
            continue
        group_dist = calculate_distribution(group_stocks)
        print(f"   {group_name.upper()} ({len(group_stocks)} æª”):")
        print(f"      Down: {group_dist['down_pct']:.1%} | "
              f"Neutral: {group_dist['neutral_pct']:.1%} | "
              f"Up: {group_dist['up_pct']:.1%}")

    # æ¥µç«¯æ¡ˆä¾‹
    print(f"\nâš ï¸  æ¥µç«¯æ¡ˆä¾‹:")
    very_low_neutral = [s for s in stocks if s['neutral_pct'] < 0.10]
    very_high_neutral = [s for s in stocks if s['neutral_pct'] > 0.70]

    if very_low_neutral:
        print(f"   æŒå¹³ < 10%: {len(very_low_neutral)} æª”")
        for s in very_low_neutral[:3]:
            print(f"      {s['date']}-{s['symbol']}: Down {s['down_pct']:.1%} | "
                  f"Neutral {s['neutral_pct']:.1%} | Up {s['up_pct']:.1%}")

    if very_high_neutral:
        print(f"   æŒå¹³ > 70%: {len(very_high_neutral)} æª”")
        for s in very_high_neutral[:3]:
            print(f"      {s['date']}-{s['symbol']}: Down {s['down_pct']:.1%} | "
                  f"Neutral {s['neutral_pct']:.1%} | Up {s['up_pct']:.1%}")

    print("\n" + "="*80 + "\n")


# ============================================================
# ä¸»ç¨‹å¼
# ============================================================

def main():
    parser = argparse.ArgumentParser(
        description="æ™ºèƒ½æ¨™ç±¤åˆ†å¸ƒåˆ†æèˆ‡æ•¸æ“šé›†é¸å–å·¥å…· v2.0",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¯„ä¾‹ç”¨æ³•ï¼š
  # åŸºç¤åˆ†æï¼ˆæ‰€æœ‰æ•¸æ“šï¼‰
  python scripts/analyze_label_distribution.py --preprocessed-dir data/preprocessed_v5 --mode analyze

  # æ™ºèƒ½æ¨è–¦ï¼ˆè‡ªå‹•ç”Ÿæˆå€™é¸æ–¹æ¡ˆï¼‰
  python scripts/analyze_label_distribution.py --preprocessed-dir data/preprocessed_v5 --mode smart_recommend --start-date 20250901 --output selection.json

  # äº’å‹•æ¨¡å¼ï¼ˆé¡¯ç¤ºå€™é¸æ–¹æ¡ˆä¸¦è®“ä½¿ç”¨è€…é¸æ“‡ï¼‰
  python scripts/analyze_label_distribution.py --preprocessed-dir data/preprocessed_v5 --mode interactive --start-date 20250901
        """
    )

    parser.add_argument("--preprocessed-dir", required=True, help="é è™•ç†æ•¸æ“šç›®éŒ„")
    parser.add_argument("--mode", default="analyze",
                       choices=["analyze", "smart_recommend", "interactive", "minimal"],
                       help="åŸ·è¡Œæ¨¡å¼: analyze=åˆ†æå…¨éƒ¨, smart_recommend=è‡ªå‹•æ¨è–¦, interactive=äº’å‹•é¸æ“‡, minimal=æœ€å°åŒ–æ¨è–¦ï¼ˆæ–°ç‰ˆï¼‰")
    parser.add_argument("--start-date", help="èµ·å§‹æ—¥æœŸ (YYYYMMDD)ï¼Œä¾‹å¦‚: 20250901")
    parser.add_argument("--target-dist", default=None,
                       help="ç›®æ¨™æ¨™ç±¤åˆ†å¸ƒ (down,neutral,up)ï¼Œä¾‹å¦‚: 0.30,0.40,0.30ã€‚è‹¥æœªæŒ‡å®šï¼Œå‰‡ä½¿ç”¨æ•´é«”åˆ†å¸ƒ")
    parser.add_argument("--min-samples", type=int, default=100000,
                       help="æœ€å°æ¨£æœ¬æ•¸ï¼ˆéæ¿¾æ‰å¤ªå°‘çš„æ–¹æ¡ˆï¼‰")
    parser.add_argument("--days", type=int, default=10,
                       help="æœ€å¤§å¤©æ•¸ä¸Šé™ï¼ˆminimal æ¨¡å¼å°ˆç”¨ï¼Œé è¨­ 10 å¤©ï¼‰")
    parser.add_argument("--output", help="è¼¸å‡º JSON æª”æ¡ˆè·¯å¾‘")

    args = parser.parse_args()

    # è¼‰å…¥æ•¸æ“š
    logging.info(f"è¼‰å…¥é è™•ç†æ•¸æ“š...")
    stocks = load_all_stocks_metadata(args.preprocessed_dir, args.start_date)

    if not stocks:
        logging.error("æ‰¾ä¸åˆ°æœ‰æ•ˆçš„è‚¡ç¥¨æ•¸æ“šï¼")
        return

    # è§£æç›®æ¨™åˆ†å¸ƒ
    if args.target_dist is None:
        # æœªæŒ‡å®š target-distï¼Œä½¿ç”¨æ•´é«”åˆ†å¸ƒ
        overall = calculate_distribution(stocks)
        target_dist = (overall['down_pct'], overall['neutral_pct'], overall['up_pct'])
        logging.info("æœªæŒ‡å®š --target-distï¼Œä½¿ç”¨æ•´é«”æ¨™ç±¤åˆ†å¸ƒ:")
        logging.info(f"  Down: {target_dist[0]:.4f} ({target_dist[0]:.2%})")
        logging.info(f"  Neutral: {target_dist[1]:.4f} ({target_dist[1]:.2%})")
        logging.info(f"  Up: {target_dist[2]:.4f} ({target_dist[2]:.2%})")
    else:
        # æ‰‹å‹•æŒ‡å®š target-dist
        target_dist = tuple(map(float, args.target_dist.split(',')))
        if len(target_dist) != 3 or abs(sum(target_dist) - 1.0) > 0.01:
            logging.error(f"ç›®æ¨™åˆ†å¸ƒæ ¼å¼éŒ¯èª¤: {args.target_dist}ï¼Œæ‡‰ç‚ºä¸‰å€‹åŠ ç¸½ç‚º1çš„æ•¸å­—")
            return
        logging.info(f"ä½¿ç”¨æŒ‡å®šçš„ç›®æ¨™åˆ†å¸ƒ: Down {target_dist[0]:.2%} | Neutral {target_dist[1]:.2%} | Up {target_dist[2]:.2%}")

    # æ¨¡å¼ 1: åŸºç¤åˆ†æ
    if args.mode == "analyze":
        print_summary_report(stocks)

    # æ¨¡å¼ 2: æ™ºèƒ½æ¨è–¦ï¼ˆè‡ªå‹•ç”Ÿæˆä½†ä¸é¸æ“‡ï¼‰
    elif args.mode == "smart_recommend":
        candidates = smart_recommend_datasets(
            stocks=stocks,
            target_dist=target_dist,
            min_samples=args.min_samples
        )

        if not candidates:
            logging.error("ç„¡æ³•ç”Ÿæˆä»»ä½•å€™é¸æ–¹æ¡ˆï¼è«‹é™ä½ min_samples æˆ–æ”¾å¯¬åå·®é–¾å€¼")
            return

        # é¡¯ç¤ºå€™é¸æ–¹æ¡ˆ
        display_candidates(candidates, target_dist)

        # è‡ªå‹•é¸æ“‡æœ€ä½³æ–¹æ¡ˆï¼ˆåå·®æœ€å°ï¼‰
        best = candidates[0]
        logging.info(f"\nè‡ªå‹•é¸æ“‡æœ€ä½³æ–¹æ¡ˆï¼ˆåå·® {best['deviation']:.4f}ï¼‰")

        print_selection_report(best, target_dist)

        # ä¿å­˜çµæœ
        if args.output:
            save_selection_to_json(best, args.output)

    # æ¨¡å¼ 3: äº’å‹•é¸æ“‡
    elif args.mode == "interactive":
        candidates = smart_recommend_datasets(
            stocks=stocks,
            target_dist=target_dist,
            min_samples=args.min_samples
        )

        if not candidates:
            logging.error("ç„¡æ³•ç”Ÿæˆä»»ä½•å€™é¸æ–¹æ¡ˆï¼è«‹é™ä½ min_samples æˆ–æ”¾å¯¬åå·®é–¾å€¼")
            return

        # ä½¿ç”¨è€…é¸æ“‡
        selected = interactive_selection(candidates, target_dist)

        if selected:
            print_selection_report(selected, target_dist)

            # ä¿å­˜çµæœ
            if args.output:
                save_selection_to_json(selected, args.output)
            else:
                # é è¨­è¼¸å‡ºè·¯å¾‘
                default_output = f"dataset_selection_{selected['date_range']}.json"
                save_selection_to_json(selected, default_output)

    # æ¨¡å¼ 4: æœ€å°åŒ–æ¨è–¦ï¼ˆæ–°ç‰ˆï¼‰
    elif args.mode == "minimal":
        # è§£æç›®æ¨™åˆ†å¸ƒï¼ˆminimal æ¨¡å¼å…è¨±ä¸æŒ‡å®šï¼Œé è¨­å‡å‹»åˆ†å¸ƒï¼‰
        if args.target_dist is None:
            target_dist_minimal = None  # ä½¿ç”¨å‡å‹»åˆ†å¸ƒ
        else:
            target_dist_minimal = tuple(map(float, args.target_dist.split(',')))
            if len(target_dist_minimal) != 3 or abs(sum(target_dist_minimal) - 1.0) > 0.01:
                logging.error(f"ç›®æ¨™åˆ†å¸ƒæ ¼å¼éŒ¯èª¤: {args.target_dist}ï¼Œæ‡‰ç‚ºä¸‰å€‹åŠ ç¸½ç‚º1çš„æ•¸å­—")
                return

        # èª¿ç”¨æœ€å°åŒ–æ¨è–¦æ¼”ç®—æ³•
        best = smart_minimal_recommend(
            stocks=stocks,
            min_samples=args.min_samples,
            target_dist=target_dist_minimal,
            max_days=args.days
        )

        # é¡¯ç¤ºçµæœ
        print("\n" + "="*100)
        print("[Minimal Mode] æœ€å°åŒ–æ¨è–¦çµæœ")
        print("="*100)

        # ç‹€æ…‹é¡¯ç¤º
        if best['status'] == 'success':
            print(f"\n[SUCCESS] {best['message']}\n")
        elif best['status'] == 'warning':
            print(f"\n[WARNING] {best['message']}\n")
        else:
            print(f"\n[ERROR] {best['message']}\n")

        # ä½¿ç”¨ç¾æœ‰çš„å ±å‘Šå‡½æ•¸ï¼ˆèˆ‡å…¶ä»–æ¨¡å¼å…¼å®¹ï¼‰
        # éœ€è¦è½‰æ›æ ¼å¼ä»¥ç¬¦åˆ print_selection_report çš„é æœŸ
        final_target_dist = target_dist_minimal if target_dist_minimal else (1/3, 1/3, 1/3)

        # åªæœ‰åœ¨æœ‰æœ‰æ•ˆæ•¸æ“šæ™‚æ‰é¡¯ç¤ºè©³ç´°å ±å‘Š
        if best.get('stock_records') and len(best['stock_records']) > 0:
            print_selection_report(best, final_target_dist)
        else:
            print("\n[ERROR] ç„¡æœ‰æ•ˆæ•¸æ“šå¯ä¾›é¡¯ç¤º\n")

        # ä¿å­˜çµæœ
        if args.output:
            # æ·»åŠ é¡å¤–ä¿¡æ¯åˆ°è¼¸å‡º
            best_with_meta = best.copy()
            best_with_meta['mode'] = 'minimal'
            best_with_meta['max_days'] = args.days
            save_selection_to_json(best_with_meta, args.output)
        else:
            # é è¨­è¼¸å‡ºè·¯å¾‘
            default_output = f"dataset_selection_minimal_{best['date_range']}.json"
            best_with_meta = best.copy()
            best_with_meta['mode'] = 'minimal'
            best_with_meta['max_days'] = args.days
            save_selection_to_json(best_with_meta, default_output)

    print("[DONE] å®Œæˆ\n")


if __name__ == "__main__":
    main()
