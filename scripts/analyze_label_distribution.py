# -*- coding: utf-8 -*-
"""
analyze_label_distribution.py - æ™ºèƒ½æ¨™ç±¤åˆ†å¸ƒåˆ†æèˆ‡æ•¸æ“šé›†é¸å–å·¥å…· v2.0
=============================================================================
ã€æ ¸å¿ƒåŠŸèƒ½ã€‘
  1. è‡ªå‹•å¾èµ·å§‹æ—¥æœŸé–‹å§‹ï¼Œé€æ—¥éå¢æƒææ‰€æœ‰é è™•ç† NPZ æ•¸æ“š
  2. åŸºæ–¼æ¨™ç±¤åˆ†å¸ƒï¼Œæ™ºèƒ½çµ„åˆå‡ºæœ€é©åˆå­¸ç¿’çš„æ—¥æœŸ+å€‹è‚¡çµ„åˆ
  3. è‡ªå‹•è¨ˆç®—æ‰€éœ€æ•¸é‡ï¼Œç¢ºä¿é”åˆ°ç›®æ¨™æ¨™ç±¤åˆ†å¸ƒï¼ˆå¯å®Œæ•´å­¸ç¿’ï¼‰
  4. äº’å‹•å¼é¸æ“‡ç•Œé¢ï¼ˆé¡¯ç¤ºå¤šå€‹å€™é¸æ–¹æ¡ˆï¼Œè®“ä½¿ç”¨è€…é¸æ“‡ï¼‰
  5. é¸å–å¾Œç”Ÿæˆè©³ç´°å ±å‘Šï¼ˆæ—¥æœŸåˆ—è¡¨ã€å€‹è‚¡IDã€æ•¸å€¼æ¯”ä¾‹ï¼‰

ã€ä½¿ç”¨æ–¹å¼ã€‘
  # åŸºç¤åˆ†ææ¨¡å¼ï¼ˆæƒææ‰€æœ‰æ•¸æ“šï¼‰
  python scripts/analyze_label_distribution.py \
      --preprocessed-dir data/preprocessed_v5 \
      --mode analyze

  # æ™ºèƒ½æ¨è–¦æ¨¡å¼ï¼ˆè‡ªå‹•çµ„åˆæœ€ä½³æ•¸æ“šé›†ï¼‰
  python scripts/analyze_label_distribution.py \
      --preprocessed-dir data/preprocessed_v5 \
      --mode smart_recommend \
      --start-date 20250901 \
      --target-dist "0.30,0.40,0.30" \
      --min-samples 100000 \
      --output dataset_selection.json

  # äº’å‹•æ¨¡å¼ï¼ˆé¡¯ç¤ºå€™é¸æ–¹æ¡ˆè®“ä½¿ç”¨è€…é¸æ“‡ï¼‰
  python scripts/analyze_label_distribution.py \
      --preprocessed-dir data/preprocessed_v5 \
      --mode interactive \
      --start-date 20250901 \
      --target-dist "0.30,0.40,0.30"

ã€è¼¸å‡ºç¯„ä¾‹ã€‘
  å€™é¸æ–¹æ¡ˆ 1:
    - æ—¥æœŸç¯„åœ: 20250901-20250910 (10 å¤©)
    - å€‹è‚¡æ•¸é‡: 145 æª”
    - ç¸½æ¨£æœ¬æ•¸: 1,234,567
    - æ¨™ç±¤åˆ†å¸ƒ: Down 30.2% | Neutral 39.8% | Up 30.0%
    - åå·®åº¦: 0.012 (èˆ‡ç›®æ¨™ 30/40/30 çš„å·®ç•°)

  é¸å–æ–¹æ¡ˆ 1 å¾Œï¼š
    âœ… å·²é¸å– 145 æª”è‚¡ç¥¨ï¼Œ10 å¤©æ•¸æ“š
    ğŸ“‹ æ—¥æœŸåˆ—è¡¨: [20250901, 20250902, ..., 20250910]
    ğŸ¢ å€‹è‚¡åˆ—è¡¨: [2330, 2317, 2454, ..., 6488]
    ğŸ“Š æœ€çµ‚åˆ†å¸ƒ: Down 372,567 (30.2%) | Neutral 490,123 (39.8%) | Up 371,877 (30.0%)

ã€æ¼”ç®—æ³•èªªæ˜ã€‘
  1. æ—¥æœŸæ’åºï¼šæŒ‰æ—¥æœŸå‡åºæ’åˆ—ï¼ˆå¾èµ·å§‹æ—¥æœŸé–‹å§‹ï¼‰
  2. é€æ—¥ç´¯ç©ï¼šé€æ—¥åŠ å…¥è‚¡ç¥¨ï¼Œè¨ˆç®—ç´¯ç©æ¨™ç±¤åˆ†å¸ƒ
  3. åå·®è©•ä¼°ï¼šè¨ˆç®—èˆ‡ç›®æ¨™åˆ†å¸ƒçš„ KL æ•£åº¦æˆ– L2 è·é›¢
  4. å¤šæ–¹æ¡ˆç”Ÿæˆï¼š
     - ä¿å®ˆæ–¹æ¡ˆï¼ˆåå·® < 0.01ï¼Œå¯èƒ½æ¨£æœ¬è¼ƒå°‘ï¼‰
     - å¹³è¡¡æ–¹æ¡ˆï¼ˆåå·® < 0.02ï¼Œæ¨£æœ¬æ•¸é©ä¸­ï¼‰
     - ç©æ¥µæ–¹æ¡ˆï¼ˆåå·® < 0.03ï¼Œæ¨£æœ¬æ•¸è¼ƒå¤šï¼‰
  5. ä½¿ç”¨è€…é¸æ“‡ï¼šé¡¯ç¤º 3-5 å€‹å€™é¸æ–¹æ¡ˆï¼Œè®“ä½¿ç”¨è€…æ±ºå®š

ã€ç‰ˆæœ¬ã€‘v2.0
ã€æ›´æ–°ã€‘2025-10-23
ã€ä½œè€…ã€‘DeepLOB-Pro Team
"""

import os
import json
import glob
import argparse
import logging
from typing import List, Dict, Any, Tuple, Optional
from collections import defaultdict
from datetime import datetime

import numpy as np
import pandas as pd
from pathlib import Path

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)


# ============================================================
# è³‡æ–™è¼‰å…¥èˆ‡å‰è™•ç†
# ============================================================

def load_all_stocks_metadata(preprocessed_dir: str, start_date: Optional[str] = None) -> List[Dict]:
    """
    è¼‰å…¥æ‰€æœ‰è‚¡ç¥¨çš„ metadataï¼ˆæ”¯æ´æ—¥æœŸéæ¿¾ï¼‰

    Args:
        preprocessed_dir: é è™•ç†æ•¸æ“šç›®éŒ„
        start_date: èµ·å§‹æ—¥æœŸï¼ˆæ ¼å¼: YYYYMMDDï¼‰ï¼ŒNone è¡¨ç¤ºè¼‰å…¥å…¨éƒ¨

    Returns:
        è‚¡ç¥¨ metadata åˆ—è¡¨ï¼ŒæŒ‰ (date, symbol) å‡åºæ’åˆ—
    """
    all_metadata = []

    # æ‰¾åˆ°æ‰€æœ‰ NPZ æª”æ¡ˆ
    npz_pattern = os.path.join(preprocessed_dir, "daily", "*", "*.npz")
    npz_files = glob.glob(npz_pattern)

    logging.info(f"æƒæåˆ° {len(npz_files)} å€‹ NPZ æª”æ¡ˆ")

    for npz_file in npz_files:
        try:
            data = np.load(npz_file, allow_pickle=True)
            metadata = json.loads(str(data['metadata']))

            # åªä¿ç•™é€šééæ¿¾ä¸”æœ‰æ¨™ç±¤é è¦½çš„è‚¡ç¥¨
            if not metadata.get('pass_filter', False):
                continue
            if metadata.get('label_preview') is None:
                continue

            # æ—¥æœŸéæ¿¾
            date_str = metadata['date']
            if start_date and date_str < start_date:
                continue

            lp = metadata['label_preview']
            all_metadata.append({
                'symbol': metadata['symbol'],
                'date': date_str,
                'file_path': npz_file,
                'range_pct': metadata['range_pct'],
                'n_points': metadata['n_points'],
                'total_labels': lp['total_labels'],
                'down_count': lp['down_count'],
                'neutral_count': lp['neutral_count'],
                'up_count': lp['up_count'],
                'down_pct': lp['down_pct'],
                'neutral_pct': lp['neutral_pct'],
                'up_pct': lp['up_pct']
            })
        except Exception as e:
            logging.debug(f"è®€å– {npz_file} å¤±æ•—: {e}")
            continue

    # æŒ‰æ—¥æœŸå’Œè‚¡ç¥¨ä»£ç¢¼æ’åº
    all_metadata.sort(key=lambda x: (x['date'], x['symbol']))

    logging.info(f"è¼‰å…¥ {len(all_metadata)} æª”æœ‰æ•ˆè‚¡ç¥¨ï¼ˆæ—¥æœŸ >= {start_date or 'å…¨éƒ¨'}ï¼‰")
    return all_metadata


def group_by_date(stocks: List[Dict]) -> Dict[str, List[Dict]]:
    """æŒ‰æ—¥æœŸåˆ†çµ„è‚¡ç¥¨"""
    grouped = defaultdict(list)
    for stock in stocks:
        grouped[stock['date']].append(stock)
    return dict(sorted(grouped.items()))


# ============================================================
# æ¨™ç±¤åˆ†å¸ƒè¨ˆç®—èˆ‡è©•ä¼°
# ============================================================

def calculate_distribution(stocks: List[Dict]) -> Dict[str, Any]:
    """
    è¨ˆç®—çµ¦å®šè‚¡ç¥¨åˆ—è¡¨çš„æ¨™ç±¤åˆ†å¸ƒ

    Returns:
        {
            'total_stocks': int,
            'total_samples': int,
            'down_count': int,
            'neutral_count': int,
            'up_count': int,
            'down_pct': float,
            'neutral_pct': float,
            'up_pct': float
        }
    """
    total_down = sum(s['down_count'] for s in stocks)
    total_neutral = sum(s['neutral_count'] for s in stocks)
    total_up = sum(s['up_count'] for s in stocks)
    total_all = total_down + total_neutral + total_up

    return {
        'total_stocks': len(stocks),
        'total_samples': total_all,
        'down_count': total_down,
        'neutral_count': total_neutral,
        'up_count': total_up,
        'down_pct': total_down / total_all if total_all > 0 else 0.0,
        'neutral_pct': total_neutral / total_all if total_all > 0 else 0.0,
        'up_pct': total_up / total_all if total_all > 0 else 0.0
    }


def calculate_deviation(
    current_dist: Tuple[float, float, float],
    target_dist: Tuple[float, float, float],
    method: str = 'l2'
) -> float:
    """
    è¨ˆç®—ç•¶å‰åˆ†å¸ƒèˆ‡ç›®æ¨™åˆ†å¸ƒçš„åå·®åº¦

    Args:
        current_dist: (down%, neutral%, up%)
        target_dist: (down%, neutral%, up%)
        method: 'l2' (L2è·é›¢) æˆ– 'kl' (KLæ•£åº¦)

    Returns:
        åå·®åº¦ï¼ˆè¶Šå°è¶Šå¥½ï¼‰
    """
    c = np.array(current_dist)
    t = np.array(target_dist)

    if method == 'l2':
        # L2 è·é›¢ï¼ˆæ­å¼è·é›¢ï¼‰
        return float(np.sqrt(np.sum((c - t) ** 2)))
    elif method == 'kl':
        # KL æ•£åº¦ï¼ˆå°æ•¸å·®ç•°ï¼‰
        epsilon = 1e-10
        c = np.clip(c, epsilon, 1.0)
        t = np.clip(t, epsilon, 1.0)
        return float(np.sum(t * np.log(t / c)))
    else:
        raise ValueError(f"æœªçŸ¥çš„åå·®è¨ˆç®—æ–¹æ³•: {method}")


# ============================================================
# æ™ºèƒ½æ¨è–¦æ¼”ç®—æ³•ï¼ˆæ ¸å¿ƒï¼‰
# ============================================================

def smart_recommend_datasets(
    stocks: List[Dict],
    target_dist: Tuple[float, float, float] = (0.30, 0.40, 0.30),
    min_samples: int = 100000,
    max_deviation_levels: List[float] = [0.01, 0.02, 0.03, 0.05]
) -> List[Dict]:
    """
    æ™ºèƒ½æ¨è–¦æ•¸æ“šé›†çµ„åˆï¼ˆé€æ—¥ç´¯ç©ï¼Œå¤šæ–¹æ¡ˆç”Ÿæˆï¼‰

    æ¼”ç®—æ³•é‚è¼¯ï¼š
      1. æŒ‰æ—¥æœŸåˆ†çµ„ï¼ˆå‡åºï¼‰
      2. é€æ—¥ç´¯ç©åŠ å…¥è‚¡ç¥¨
      3. æ¯æ¬¡ç´¯ç©å¾Œè¨ˆç®—æ¨™ç±¤åˆ†å¸ƒå’Œåå·®åº¦
      4. ç•¶é”åˆ°ä¸åŒåå·®é–¾å€¼æ™‚ï¼Œè¨˜éŒ„ç‚ºå€™é¸æ–¹æ¡ˆ
      5. è¿”å›å¤šå€‹å€™é¸æ–¹æ¡ˆä¾›ä½¿ç”¨è€…é¸æ“‡

    Args:
        stocks: æ‰€æœ‰è‚¡ç¥¨ metadataï¼ˆå·²æŒ‰æ—¥æœŸæ’åºï¼‰
        target_dist: ç›®æ¨™æ¨™ç±¤åˆ†å¸ƒ (down%, neutral%, up%)
        min_samples: æœ€å°æ¨£æœ¬æ•¸ï¼ˆéæ¿¾æ‰æ¨£æœ¬å¤ªå°‘çš„æ–¹æ¡ˆï¼‰
        max_deviation_levels: åå·®é–¾å€¼åˆ—è¡¨ï¼ˆç”Ÿæˆå¤šå€‹æ–¹æ¡ˆï¼‰

    Returns:
        å€™é¸æ–¹æ¡ˆåˆ—è¡¨ï¼ŒæŒ‰åå·®åº¦æ’åºï¼ˆæœ€ä½³æ–¹æ¡ˆåœ¨å‰ï¼‰
        æ¯å€‹æ–¹æ¡ˆåŒ…å«ï¼š
          - dates: æ—¥æœŸåˆ—è¡¨
          - stocks: è‚¡ç¥¨åˆ—è¡¨
          - distribution: æ¨™ç±¤åˆ†å¸ƒ
          - deviation: åå·®åº¦
          - description: æ–¹æ¡ˆæè¿°
    """
    grouped = group_by_date(stocks)
    sorted_dates = sorted(grouped.keys())

    logging.info(f"\né–‹å§‹æ™ºèƒ½æ¨è–¦ï¼š")
    logging.info(f"  ç›®æ¨™åˆ†å¸ƒ: Down {target_dist[0]:.1%} | Neutral {target_dist[1]:.1%} | Up {target_dist[2]:.1%}")
    logging.info(f"  æœ€å°æ¨£æœ¬æ•¸: {min_samples:,}")
    logging.info(f"  æ—¥æœŸç¯„åœ: {sorted_dates[0]} - {sorted_dates[-1]} ({len(sorted_dates)} å¤©)")

    # é€æ—¥ç´¯ç©
    cumulative_stocks = []
    candidates = []
    found_levels = set()

    for date in sorted_dates:
        # åŠ å…¥ç•¶æ—¥æ‰€æœ‰è‚¡ç¥¨
        cumulative_stocks.extend(grouped[date])

        # è¨ˆç®—ç´¯ç©åˆ†å¸ƒ
        dist = calculate_distribution(cumulative_stocks)

        # æ¨£æœ¬æ•¸éæ¿¾
        if dist['total_samples'] < min_samples:
            continue

        # è¨ˆç®—åå·®åº¦
        current_dist = (dist['down_pct'], dist['neutral_pct'], dist['up_pct'])
        deviation = calculate_deviation(current_dist, target_dist, method='l2')

        # æª¢æŸ¥æ˜¯å¦ç¬¦åˆä»»ä¸€åå·®é–¾å€¼ï¼ˆä¸”å°šæœªè¨˜éŒ„è©²ç­‰ç´šï¼‰
        for level in max_deviation_levels:
            if deviation <= level and level not in found_levels:
                found_levels.add(level)

                # ç”Ÿæˆæ–¹æ¡ˆæè¿°
                if level <= 0.01:
                    desc = "ä¿å®ˆæ–¹æ¡ˆï¼ˆæœ€é«˜ç²¾åº¦ï¼Œåå·® < 1%ï¼‰"
                elif level <= 0.02:
                    desc = "å¹³è¡¡æ–¹æ¡ˆï¼ˆé«˜ç²¾åº¦ï¼Œåå·® < 2%ï¼‰"
                elif level <= 0.03:
                    desc = "ç©æ¥µæ–¹æ¡ˆï¼ˆä¸­ç­‰ç²¾åº¦ï¼Œåå·® < 3%ï¼‰"
                else:
                    desc = "å¯¬é¬†æ–¹æ¡ˆï¼ˆè¼ƒå¤§æ¨£æœ¬ï¼Œåå·® < 5%ï¼‰"

                # æ”¶é›†æ—¥æœŸå’Œè‚¡ç¥¨åˆ—è¡¨
                dates_used = sorted(set(s['date'] for s in cumulative_stocks))
                symbols_used = sorted(set(s['symbol'] for s in cumulative_stocks))

                candidates.append({
                    'dates': dates_used,
                    'date_range': f"{dates_used[0]}-{dates_used[-1]}",
                    'num_dates': len(dates_used),
                    'symbols': symbols_used,
                    'num_stocks': len(symbols_used),
                    'stock_records': cumulative_stocks.copy(),  # å®Œæ•´è¨˜éŒ„ï¼ˆå«é‡è¤‡æ—¥æœŸï¼‰
                    'total_records': len(cumulative_stocks),
                    'distribution': dist,
                    'deviation': deviation,
                    'level': level,
                    'description': desc
                })

                logging.info(f"  âœ… æ‰¾åˆ°å€™é¸æ–¹æ¡ˆï¼ˆåå·® {deviation:.4f} <= {level:.2f}ï¼‰: "
                           f"{len(dates_used)} å¤©, {len(symbols_used)} æª”, {dist['total_samples']:,} æ¨£æœ¬")

                break  # ä¸€å€‹æ–¹æ¡ˆåªè¨˜éŒ„æœ€ç·Šçš„é–¾å€¼

    # æŒ‰åå·®åº¦æ’åºï¼ˆæœ€ä½³åœ¨å‰ï¼‰
    candidates.sort(key=lambda x: x['deviation'])

    logging.info(f"\nå…±ç”Ÿæˆ {len(candidates)} å€‹å€™é¸æ–¹æ¡ˆ")
    return candidates


# ============================================================
# äº’å‹•å¼é¸æ“‡ç•Œé¢
# ============================================================

def display_candidates(candidates: List[Dict], target_dist: Tuple[float, float, float]) -> None:
    """é¡¯ç¤ºå€™é¸æ–¹æ¡ˆï¼ˆç¾åŒ–è¡¨æ ¼ï¼‰"""
    print("\n" + "="*100)
    print("ğŸ“‹ å€™é¸æ•¸æ“šé›†æ–¹æ¡ˆ")
    print("="*100)

    print(f"\nğŸ¯ ç›®æ¨™åˆ†å¸ƒ: Down {target_dist[0]:.1%} | Neutral {target_dist[1]:.1%} | Up {target_dist[2]:.1%}")
    print(f"\nå…±æ‰¾åˆ° {len(candidates)} å€‹å€™é¸æ–¹æ¡ˆï¼ˆæŒ‰åå·®åº¦æ’åºï¼‰ï¼š\n")

    for i, cand in enumerate(candidates, 1):
        dist = cand['distribution']
        print(f"ã€æ–¹æ¡ˆ {i}ã€‘{cand['description']}")
        print(f"  ğŸ“… æ—¥æœŸç¯„åœ: {cand['date_range']} ({cand['num_dates']} å¤©)")
        print(f"  ğŸ¢ å€‹è‚¡æ•¸é‡: {cand['num_stocks']} æª”")
        print(f"  ğŸ“Š ç¸½æ¨£æœ¬æ•¸: {dist['total_samples']:,}")
        print(f"  ğŸ“ˆ æ¨™ç±¤åˆ†å¸ƒ: Down {dist['down_pct']:.2%} ({dist['down_count']:,}) | "
              f"Neutral {dist['neutral_pct']:.2%} ({dist['neutral_count']:,}) | "
              f"Up {dist['up_pct']:.2%} ({dist['up_count']:,})")
        print(f"  ğŸ“ åå·®åº¦: {cand['deviation']:.4f}")
        print()


def interactive_selection(candidates: List[Dict], target_dist: Tuple[float, float, float]) -> Optional[Dict]:
    """äº’å‹•å¼é¸æ“‡å€™é¸æ–¹æ¡ˆ"""
    if not candidates:
        logging.error("æ²’æœ‰å€™é¸æ–¹æ¡ˆå¯ä¾›é¸æ“‡ï¼")
        return None

    display_candidates(candidates, target_dist)

    while True:
        try:
            choice = input(f"è«‹é¸æ“‡æ–¹æ¡ˆ (1-{len(candidates)})ï¼Œæˆ–è¼¸å…¥ 'q' é€€å‡º: ").strip()

            if choice.lower() == 'q':
                logging.info("ä½¿ç”¨è€…å–æ¶ˆé¸æ“‡")
                return None

            idx = int(choice) - 1
            if 0 <= idx < len(candidates):
                return candidates[idx]
            else:
                print(f"âŒ ç„¡æ•ˆé¸æ“‡ï¼Œè«‹è¼¸å…¥ 1-{len(candidates)}")
        except ValueError:
            print("âŒ è«‹è¼¸å…¥æœ‰æ•ˆæ•¸å­—")
        except KeyboardInterrupt:
            print("\nä½¿ç”¨è€…ä¸­æ–·")
            return None


# ============================================================
# é¸å–å¾Œçš„è©³ç´°å ±å‘Š
# ============================================================

def print_selection_report(selected: Dict, target_dist: Tuple[float, float, float]) -> None:
    """åˆ—å°é¸å–æ–¹æ¡ˆçš„è©³ç´°å ±å‘Š"""
    dist = selected['distribution']

    print("\n" + "="*100)
    print("âœ… å·²é¸å–æ•¸æ“šé›†")
    print("="*100)

    print(f"\nğŸ“‹ æ–¹æ¡ˆæè¿°: {selected['description']}")
    print(f"ğŸ“ åå·®åº¦: {selected['deviation']:.4f}\n")

    print("ã€æ—¥æœŸåˆ—è¡¨ã€‘")
    dates = selected['dates']
    print(f"  ç¯„åœ: {dates[0]} - {dates[-1]}")
    print(f"  å¤©æ•¸: {len(dates)}")
    print(f"  æ˜ç´°: {', '.join(dates)}\n")

    print("ã€å€‹è‚¡åˆ—è¡¨ã€‘")
    symbols = selected['symbols']
    print(f"  æ•¸é‡: {len(symbols)} æª”ï¼ˆä¸é‡è¤‡ï¼‰")
    if len(symbols) <= 20:
        print(f"  æ˜ç´°: {', '.join(symbols)}\n")
    else:
        print(f"  å‰20æª”: {', '.join(symbols[:20])} ...")
        print(f"  (å®Œæ•´åˆ—è¡¨è«‹æŸ¥çœ‹è¼¸å‡º JSON)\n")

    print("ã€æª”æ¡ˆé…å°åˆ—è¡¨ã€‘")
    print(f"  ç¸½è¨˜éŒ„æ•¸: {selected['total_records']} å€‹ï¼ˆæ—¥æœŸÃ—è‚¡ç¥¨é…å°ï¼‰")
    print("  èªªæ˜: æ¯å€‹é…å°å°æ‡‰ä¸€å€‹ NPZ æª”æ¡ˆ")

    # é¡¯ç¤ºå‰ 10 å€‹é…å°ç¯„ä¾‹
    stock_records = selected['stock_records']
    print("  ç¯„ä¾‹å‰10å€‹:")
    for i, record in enumerate(stock_records[:10], 1):
        print(f"    {i:>2}. {record['date']}-{record['symbol']:<6} â†’ {record['file_path']}")
    if len(stock_records) > 10:
        print(f"    ... (é‚„æœ‰ {len(stock_records) - 10} å€‹é…å°ï¼Œè©³è¦‹ JSON)\n")
    else:
        print()

    print("ã€ç¸½æ¨£æœ¬æ•¸ã€‘")
    print(f"  {dist['total_samples']:,} å€‹æ¨£æœ¬ï¼ˆæ‰€æœ‰é…å°åŠ ç¸½ï¼‰\n")

    print("ã€æ¨™ç±¤åˆ†å¸ƒã€‘")
    print(f"  Down:    {dist['down_count']:>12,} ({dist['down_pct']:>6.2%})  [ç›®æ¨™: {target_dist[0]:.2%}, åå·®: {dist['down_pct'] - target_dist[0]:+.2%}]")
    print(f"  Neutral: {dist['neutral_count']:>12,} ({dist['neutral_pct']:>6.2%})  [ç›®æ¨™: {target_dist[1]:.2%}, åå·®: {dist['neutral_pct'] - target_dist[1]:+.2%}]")
    print(f"  Up:      {dist['up_count']:>12,} ({dist['up_pct']:>6.2%})  [ç›®æ¨™: {target_dist[2]:.2%}, åå·®: {dist['up_pct'] - target_dist[2]:+.2%}]")

    print("\n" + "="*100 + "\n")


def save_selection_to_json(selected: Dict, output_path: str) -> None:
    """ä¿å­˜é¸å–çµæœåˆ° JSON"""
    # å¾ stock_records ç”Ÿæˆã€Œæ—¥æœŸ+è‚¡ç¥¨ã€é…å°åˆ—è¡¨
    stock_records = selected['stock_records']
    file_list = [
        {
            'date': record['date'],
            'symbol': record['symbol'],
            'file_path': record['file_path'],
            'n_points': record['n_points'],
            'total_labels': record['total_labels'],
            'down_count': record['down_count'],
            'neutral_count': record['neutral_count'],
            'up_count': record['up_count']
        }
        for record in stock_records
    ]

    output_data = {
        'description': selected['description'],
        'date_range': selected['date_range'],
        'dates': selected['dates'],
        'num_dates': selected['num_dates'],
        'symbols': selected['symbols'],
        'num_stocks': selected['num_stocks'],
        'total_records': selected['total_records'],
        'file_list': file_list,  # ğŸ†• å®Œæ•´çš„ã€Œæ—¥æœŸ+è‚¡ç¥¨ã€é…å°åˆ—è¡¨
        'distribution': selected['distribution'],
        'deviation': selected['deviation'],
        'level': selected['level']
    }

    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(output_data, f, ensure_ascii=False, indent=2)

    logging.info(f"âœ… é¸å–çµæœå·²ä¿å­˜åˆ°: {output_path}")


# ============================================================
# å‚³çµ±åˆ†ææ¨¡å¼ï¼ˆä¿ç•™åŸåŠŸèƒ½ï¼‰
# ============================================================

def analyze_overall_distribution(stocks: List[Dict]) -> Dict:
    """åˆ†ææ•´é«”æ¨™ç±¤åˆ†å¸ƒï¼ˆèˆŠç‰ˆç›¸å®¹ï¼‰"""
    return calculate_distribution(stocks)


def group_by_neutral_ratio(stocks: List[Dict]) -> Dict[str, List[Dict]]:
    """æŒ‰æŒå¹³æ¯”ä¾‹åˆ†çµ„"""
    groups = {
        'very_low': [],    # < 10%
        'low': [],         # 10-30%
        'medium': [],      # 30-50%
        'high': [],        # 50-70%
        'very_high': []    # > 70%
    }

    for stock in stocks:
        neutral_pct = stock['neutral_pct']
        if neutral_pct < 0.10:
            groups['very_low'].append(stock)
        elif neutral_pct < 0.30:
            groups['low'].append(stock)
        elif neutral_pct < 0.50:
            groups['medium'].append(stock)
        elif neutral_pct < 0.70:
            groups['high'].append(stock)
        else:
            groups['very_high'].append(stock)

    return groups


def print_summary_report(stocks: List[Dict]):
    """åˆ—å°æ‘˜è¦å ±å‘Šï¼ˆèˆŠç‰ˆç›¸å®¹ï¼‰"""
    print("\n" + "="*80)
    print("æ¨™ç±¤åˆ†å¸ƒåˆ†æå ±å‘Š")
    print("="*80)

    # æ•´é«”åˆ†å¸ƒ
    overall = analyze_overall_distribution(stocks)
    print(f"\nğŸ“Š æ•´é«”æ¨™ç±¤åˆ†å¸ƒ ({overall['total_stocks']} æª”è‚¡ç¥¨):")
    print(f"   ç¸½æ¨£æœ¬æ•¸: {overall['total_samples']:,}")
    print(f"   Down:    {overall['down_count']:>10,} ({overall['down_pct']:>6.2%})")
    print(f"   Neutral: {overall['neutral_count']:>10,} ({overall['neutral_pct']:>6.2%})")
    print(f"   Up:      {overall['up_count']:>10,} ({overall['up_pct']:>6.2%})")

    # æŒ‰æ—¥æœŸåˆ†çµ„
    grouped = group_by_date(stocks)
    print(f"\nğŸ“… æŒ‰æ—¥æœŸåˆ†çµ„ ({len(grouped)} å¤©):")
    for date, stocks_in_date in list(grouped.items())[:5]:  # åªé¡¯ç¤ºå‰5å¤©
        dist = calculate_distribution(stocks_in_date)
        print(f"   {date}: {len(stocks_in_date)} æª”, {dist['total_samples']:,} æ¨£æœ¬, "
              f"Down {dist['down_pct']:.1%} | Neutral {dist['neutral_pct']:.1%} | Up {dist['up_pct']:.1%}")
    if len(grouped) > 5:
        print(f"   ... (é‚„æœ‰ {len(grouped) - 5} å¤©)")

    # æŒ‰æŒå¹³æ¯”ä¾‹åˆ†çµ„
    groups = group_by_neutral_ratio(stocks)
    print(f"\nğŸ“ˆ æŒ‰æŒå¹³æ¯”ä¾‹åˆ†çµ„:")
    for group_name, group_stocks in groups.items():
        if not group_stocks:
            continue
        group_dist = calculate_distribution(group_stocks)
        print(f"   {group_name.upper()} ({len(group_stocks)} æª”):")
        print(f"      Down: {group_dist['down_pct']:.1%} | "
              f"Neutral: {group_dist['neutral_pct']:.1%} | "
              f"Up: {group_dist['up_pct']:.1%}")

    # æ¥µç«¯æ¡ˆä¾‹
    print(f"\nâš ï¸  æ¥µç«¯æ¡ˆä¾‹:")
    very_low_neutral = [s for s in stocks if s['neutral_pct'] < 0.10]
    very_high_neutral = [s for s in stocks if s['neutral_pct'] > 0.70]

    if very_low_neutral:
        print(f"   æŒå¹³ < 10%: {len(very_low_neutral)} æª”")
        for s in very_low_neutral[:3]:
            print(f"      {s['date']}-{s['symbol']}: Down {s['down_pct']:.1%} | "
                  f"Neutral {s['neutral_pct']:.1%} | Up {s['up_pct']:.1%}")

    if very_high_neutral:
        print(f"   æŒå¹³ > 70%: {len(very_high_neutral)} æª”")
        for s in very_high_neutral[:3]:
            print(f"      {s['date']}-{s['symbol']}: Down {s['down_pct']:.1%} | "
                  f"Neutral {s['neutral_pct']:.1%} | Up {s['up_pct']:.1%}")

    print("\n" + "="*80 + "\n")


# ============================================================
# ä¸»ç¨‹å¼
# ============================================================

def main():
    parser = argparse.ArgumentParser(
        description="æ™ºèƒ½æ¨™ç±¤åˆ†å¸ƒåˆ†æèˆ‡æ•¸æ“šé›†é¸å–å·¥å…· v2.0",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¯„ä¾‹ç”¨æ³•ï¼š
  # åŸºç¤åˆ†æï¼ˆæ‰€æœ‰æ•¸æ“šï¼‰
  python scripts/analyze_label_distribution.py --preprocessed-dir data/preprocessed_v5 --mode analyze

  # æ™ºèƒ½æ¨è–¦ï¼ˆè‡ªå‹•ç”Ÿæˆå€™é¸æ–¹æ¡ˆï¼‰
  python scripts/analyze_label_distribution.py --preprocessed-dir data/preprocessed_v5 --mode smart_recommend --start-date 20250901 --output selection.json

  # äº’å‹•æ¨¡å¼ï¼ˆé¡¯ç¤ºå€™é¸æ–¹æ¡ˆä¸¦è®“ä½¿ç”¨è€…é¸æ“‡ï¼‰
  python scripts/analyze_label_distribution.py --preprocessed-dir data/preprocessed_v5 --mode interactive --start-date 20250901
        """
    )

    parser.add_argument("--preprocessed-dir", required=True, help="é è™•ç†æ•¸æ“šç›®éŒ„")
    parser.add_argument("--mode", default="analyze",
                       choices=["analyze", "smart_recommend", "interactive"],
                       help="åŸ·è¡Œæ¨¡å¼: analyze=åˆ†æå…¨éƒ¨, smart_recommend=è‡ªå‹•æ¨è–¦, interactive=äº’å‹•é¸æ“‡")
    parser.add_argument("--start-date", help="èµ·å§‹æ—¥æœŸ (YYYYMMDD)ï¼Œä¾‹å¦‚: 20250901")
    parser.add_argument("--target-dist", default=None,
                       help="ç›®æ¨™æ¨™ç±¤åˆ†å¸ƒ (down,neutral,up)ï¼Œä¾‹å¦‚: 0.30,0.40,0.30ã€‚è‹¥æœªæŒ‡å®šï¼Œå‰‡ä½¿ç”¨æ•´é«”åˆ†å¸ƒ")
    parser.add_argument("--min-samples", type=int, default=100000,
                       help="æœ€å°æ¨£æœ¬æ•¸ï¼ˆéæ¿¾æ‰å¤ªå°‘çš„æ–¹æ¡ˆï¼‰")
    parser.add_argument("--output", help="è¼¸å‡º JSON æª”æ¡ˆè·¯å¾‘")

    args = parser.parse_args()

    # è¼‰å…¥æ•¸æ“š
    logging.info(f"è¼‰å…¥é è™•ç†æ•¸æ“š...")
    stocks = load_all_stocks_metadata(args.preprocessed_dir, args.start_date)

    if not stocks:
        logging.error("æ‰¾ä¸åˆ°æœ‰æ•ˆçš„è‚¡ç¥¨æ•¸æ“šï¼")
        return

    # è§£æç›®æ¨™åˆ†å¸ƒ
    if args.target_dist is None:
        # æœªæŒ‡å®š target-distï¼Œä½¿ç”¨æ•´é«”åˆ†å¸ƒ
        overall = calculate_distribution(stocks)
        target_dist = (overall['down_pct'], overall['neutral_pct'], overall['up_pct'])
        logging.info("æœªæŒ‡å®š --target-distï¼Œä½¿ç”¨æ•´é«”æ¨™ç±¤åˆ†å¸ƒ:")
        logging.info(f"  Down: {target_dist[0]:.4f} ({target_dist[0]:.2%})")
        logging.info(f"  Neutral: {target_dist[1]:.4f} ({target_dist[1]:.2%})")
        logging.info(f"  Up: {target_dist[2]:.4f} ({target_dist[2]:.2%})")
    else:
        # æ‰‹å‹•æŒ‡å®š target-dist
        target_dist = tuple(map(float, args.target_dist.split(',')))
        if len(target_dist) != 3 or abs(sum(target_dist) - 1.0) > 0.01:
            logging.error(f"ç›®æ¨™åˆ†å¸ƒæ ¼å¼éŒ¯èª¤: {args.target_dist}ï¼Œæ‡‰ç‚ºä¸‰å€‹åŠ ç¸½ç‚º1çš„æ•¸å­—")
            return
        logging.info(f"ä½¿ç”¨æŒ‡å®šçš„ç›®æ¨™åˆ†å¸ƒ: Down {target_dist[0]:.2%} | Neutral {target_dist[1]:.2%} | Up {target_dist[2]:.2%}")

    # æ¨¡å¼ 1: åŸºç¤åˆ†æ
    if args.mode == "analyze":
        print_summary_report(stocks)

    # æ¨¡å¼ 2: æ™ºèƒ½æ¨è–¦ï¼ˆè‡ªå‹•ç”Ÿæˆä½†ä¸é¸æ“‡ï¼‰
    elif args.mode == "smart_recommend":
        candidates = smart_recommend_datasets(
            stocks=stocks,
            target_dist=target_dist,
            min_samples=args.min_samples
        )

        if not candidates:
            logging.error("ç„¡æ³•ç”Ÿæˆä»»ä½•å€™é¸æ–¹æ¡ˆï¼è«‹é™ä½ min_samples æˆ–æ”¾å¯¬åå·®é–¾å€¼")
            return

        # é¡¯ç¤ºå€™é¸æ–¹æ¡ˆ
        display_candidates(candidates, target_dist)

        # è‡ªå‹•é¸æ“‡æœ€ä½³æ–¹æ¡ˆï¼ˆåå·®æœ€å°ï¼‰
        best = candidates[0]
        logging.info(f"\nè‡ªå‹•é¸æ“‡æœ€ä½³æ–¹æ¡ˆï¼ˆåå·® {best['deviation']:.4f}ï¼‰")

        print_selection_report(best, target_dist)

        # ä¿å­˜çµæœ
        if args.output:
            save_selection_to_json(best, args.output)

    # æ¨¡å¼ 3: äº’å‹•é¸æ“‡
    elif args.mode == "interactive":
        candidates = smart_recommend_datasets(
            stocks=stocks,
            target_dist=target_dist,
            min_samples=args.min_samples
        )

        if not candidates:
            logging.error("ç„¡æ³•ç”Ÿæˆä»»ä½•å€™é¸æ–¹æ¡ˆï¼è«‹é™ä½ min_samples æˆ–æ”¾å¯¬åå·®é–¾å€¼")
            return

        # ä½¿ç”¨è€…é¸æ“‡
        selected = interactive_selection(candidates, target_dist)

        if selected:
            print_selection_report(selected, target_dist)

            # ä¿å­˜çµæœ
            if args.output:
                save_selection_to_json(selected, args.output)
            else:
                # é è¨­è¼¸å‡ºè·¯å¾‘
                default_output = f"dataset_selection_{selected['date_range']}.json"
                save_selection_to_json(selected, default_output)

    print("âœ… å®Œæˆ\n")


if __name__ == "__main__":
    main()
