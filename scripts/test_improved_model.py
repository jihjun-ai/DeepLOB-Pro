#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
test_improved_model.py - æ¸¬è©¦æ”¹é€²ç‰ˆ DeepLOB æ¨¡å‹
=============================================================================
å¿«é€Ÿé©—è­‰æ”¹é€²ç‰ˆæ¨¡å‹æ˜¯å¦æ­£å¸¸å·¥ä½œ

ä½¿ç”¨æ–¹å¼:
    conda activate deeplob-pro
    python scripts/test_improved_model.py
=============================================================================
"""

import sys
from pathlib import Path
import torch

# æ·»åŠ å°ˆæ¡ˆæ ¹ç›®éŒ„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from src.models.deeplob_improved import DeepLOBImproved

def test_model_creation():
    """æ¸¬è©¦æ¨¡å‹å‰µå»º"""
    print("="*60)
    print("æ¸¬è©¦ 1: æ¨¡å‹å‰µå»º")
    print("="*60)

    try:
        model = DeepLOBImproved(
            num_classes=3,
            conv_filters=32,
            lstm_hidden=64,
            fc_hidden=64,
            dropout=0.6,
            use_layer_norm=True,
            use_attention=True,
            input_shape=(100, 20)
        )
        print("âœ… æ¨¡å‹å‰µå»ºæˆåŠŸ")

        # çµ±è¨ˆåƒæ•¸
        total_params = sum(p.numel() for p in model.parameters())
        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)

        print(f"\næ¨¡å‹çµ±è¨ˆ:")
        print(f"  ç¸½åƒæ•¸é‡: {total_params:,}")
        print(f"  å¯è¨“ç·´åƒæ•¸: {trainable_params:,}")

        return model

    except Exception as e:
        print(f"âŒ æ¨¡å‹å‰µå»ºå¤±æ•—: {e}")
        import traceback
        traceback.print_exc()
        return None


def test_forward_pass(model):
    """æ¸¬è©¦å‰å‘å‚³æ’­"""
    print("\n" + "="*60)
    print("æ¸¬è©¦ 2: å‰å‘å‚³æ’­")
    print("="*60)

    try:
        batch_size = 16
        seq_len = 100
        features = 20

        # å‰µå»ºéš¨æ©Ÿè¼¸å…¥
        x = torch.randn(batch_size, seq_len, features)
        print(f"è¼¸å…¥å½¢ç‹€: {x.shape}")

        # å‰å‘å‚³æ’­
        logits = model(x)
        print(f"è¼¸å‡ºå½¢ç‹€: {logits.shape}")

        # æª¢æŸ¥è¼¸å‡º
        assert logits.shape == (batch_size, 3), f"è¼¸å‡ºå½¢ç‹€éŒ¯èª¤: {logits.shape}"
        assert not torch.isnan(logits).any(), "è¼¸å‡ºåŒ…å« NaN"
        assert not torch.isinf(logits).any(), "è¼¸å‡ºåŒ…å« Inf"

        print(f"âœ… å‰å‘å‚³æ’­æˆåŠŸ")
        print(f"  è¼¸å‡ºç¯„åœ: [{logits.min():.4f}, {logits.max():.4f}]")

        return True

    except Exception as e:
        print(f"âŒ å‰å‘å‚³æ’­å¤±æ•—: {e}")
        import traceback
        traceback.print_exc()
        return False


def test_attention_weights(model):
    """æ¸¬è©¦æ³¨æ„åŠ›æ¬Šé‡"""
    print("\n" + "="*60)
    print("æ¸¬è©¦ 3: æ³¨æ„åŠ›æ©Ÿåˆ¶")
    print("="*60)

    if not model.use_attention:
        print("âš ï¸  æ¨¡å‹æœªå•Ÿç”¨æ³¨æ„åŠ›æ©Ÿåˆ¶ï¼Œè·³éæ¸¬è©¦")
        return True

    try:
        x = torch.randn(8, 100, 20)

        # ç²å–æ³¨æ„åŠ›æ¬Šé‡
        logits, attn_weights = model(x, return_attention=True)

        print(f"æ³¨æ„åŠ›æ¬Šé‡å½¢ç‹€: {attn_weights.shape}")
        print(f"æ³¨æ„åŠ›æ¬Šé‡ç¯„åœ: [{attn_weights.min():.4f}, {attn_weights.max():.4f}]")

        # æª¢æŸ¥æ¬Šé‡å’Œç‚º 1
        attn_sum = attn_weights.sum(dim=1)
        print(f"æ¬Šé‡å’Œ: {attn_sum[0].item():.6f} (æ‡‰è©² â‰ˆ 1.0)")

        assert attn_weights.shape == (8, 100, 1), f"æ³¨æ„åŠ›æ¬Šé‡å½¢ç‹€éŒ¯èª¤: {attn_weights.shape}"
        assert torch.allclose(attn_sum, torch.ones_like(attn_sum), atol=1e-5), "æ³¨æ„åŠ›æ¬Šé‡å’Œä¸ç‚º 1"

        print("âœ… æ³¨æ„åŠ›æ©Ÿåˆ¶æ­£å¸¸")

        # é¡¯ç¤ºæ¬Šé‡åˆ†ä½ˆ
        print(f"\næ¬Šé‡çµ±è¨ˆ:")
        print(f"  Mean: {attn_weights.mean():.6f}")
        print(f"  Std:  {attn_weights.std():.6f}")
        print(f"  Max:  {attn_weights.max():.6f}")
        print(f"  Min:  {attn_weights.min():.6f}")

        return True

    except Exception as e:
        print(f"âŒ æ³¨æ„åŠ›æ¸¬è©¦å¤±æ•—: {e}")
        import traceback
        traceback.print_exc()
        return False


def test_backward_pass(model):
    """æ¸¬è©¦åå‘å‚³æ’­"""
    print("\n" + "="*60)
    print("æ¸¬è©¦ 4: åå‘å‚³æ’­")
    print("="*60)

    try:
        x = torch.randn(16, 100, 20, requires_grad=True)
        labels = torch.randint(0, 3, (16,))

        # å‰å‘
        logits = model(x)

        # è¨ˆç®—æå¤±
        criterion = torch.nn.CrossEntropyLoss()
        loss = criterion(logits, labels)

        print(f"æå¤±å€¼: {loss.item():.4f}")

        # åå‘
        loss.backward()

        # æª¢æŸ¥æ¢¯åº¦
        has_grad = False
        grad_norms = []

        for name, param in model.named_parameters():
            if param.grad is not None:
                has_grad = True
                grad_norm = param.grad.norm().item()
                grad_norms.append(grad_norm)

                # æª¢æŸ¥ç•°å¸¸æ¢¯åº¦
                if torch.isnan(param.grad).any():
                    print(f"  âš ï¸  {name} æ¢¯åº¦åŒ…å« NaN")
                if torch.isinf(param.grad).any():
                    print(f"  âš ï¸  {name} æ¢¯åº¦åŒ…å« Inf")

        assert has_grad, "æ²’æœ‰è¨ˆç®—ä»»ä½•æ¢¯åº¦"

        avg_grad_norm = sum(grad_norms) / len(grad_norms)
        max_grad_norm = max(grad_norms)

        print(f"âœ… åå‘å‚³æ’­æˆåŠŸ")
        print(f"  å¹³å‡æ¢¯åº¦ç¯„æ•¸: {avg_grad_norm:.4f}")
        print(f"  æœ€å¤§æ¢¯åº¦ç¯„æ•¸: {max_grad_norm:.4f}")

        if max_grad_norm > 10.0:
            print(f"  âš ï¸  è­¦å‘Š: æ¢¯åº¦ç¯„æ•¸è¼ƒå¤§ ({max_grad_norm:.2f})")

        return True

    except Exception as e:
        print(f"âŒ åå‘å‚³æ’­å¤±æ•—: {e}")
        import traceback
        traceback.print_exc()
        return False


def test_gpu_support():
    """æ¸¬è©¦ GPU æ”¯æ´"""
    print("\n" + "="*60)
    print("æ¸¬è©¦ 5: GPU æ”¯æ´")
    print("="*60)

    if not torch.cuda.is_available():
        print("âš ï¸  CUDA ä¸å¯ç”¨ï¼Œè·³é GPU æ¸¬è©¦")
        return True

    try:
        device = torch.device("cuda")

        model = DeepLOBImproved(
            num_classes=3,
            lstm_hidden=64,
            dropout=0.6,
            use_layer_norm=True,
            use_attention=True
        ).to(device)

        x = torch.randn(16, 100, 20).to(device)

        # å‰å‘å‚³æ’­
        logits = model(x)

        assert logits.device.type == "cuda", "è¼¸å‡ºæœªåœ¨ GPU ä¸Š"

        print(f"âœ… GPU æ”¯æ´æ­£å¸¸")
        print(f"  è¨­å‚™: {torch.cuda.get_device_name(0)}")
        print(f"  è¨˜æ†¶é«”ä½¿ç”¨: {torch.cuda.memory_allocated(0) / 1024**2:.2f} MB")

        return True

    except Exception as e:
        print(f"âŒ GPU æ¸¬è©¦å¤±æ•—: {e}")
        import traceback
        traceback.print_exc()
        return False


def main():
    """ä¸»å‡½æ•¸"""
    print("\n" + "="*60)
    print("ğŸ§ª æ”¹é€²ç‰ˆ DeepLOB æ¨¡å‹æ¸¬è©¦")
    print("="*60)

    results = {}

    # æ¸¬è©¦ 1: æ¨¡å‹å‰µå»º
    model = test_model_creation()
    results['creation'] = model is not None

    if model is None:
        print("\nâŒ æ¨¡å‹å‰µå»ºå¤±æ•—ï¼Œå¾ŒçºŒæ¸¬è©¦è·³é")
        return 1

    # æ¸¬è©¦ 2: å‰å‘å‚³æ’­
    results['forward'] = test_forward_pass(model)

    # æ¸¬è©¦ 3: æ³¨æ„åŠ›æ©Ÿåˆ¶
    results['attention'] = test_attention_weights(model)

    # æ¸¬è©¦ 4: åå‘å‚³æ’­
    results['backward'] = test_backward_pass(model)

    # æ¸¬è©¦ 5: GPU æ”¯æ´
    results['gpu'] = test_gpu_support()

    # ç¸½çµ
    print("\n" + "="*60)
    print("æ¸¬è©¦ç¸½çµ")
    print("="*60)

    for test_name, passed in results.items():
        status = "âœ… é€šé" if passed else "âŒ å¤±æ•—"
        print(f"  {test_name.capitalize():12s}: {status}")

    all_passed = all(results.values())

    if all_passed:
        print("\nğŸ‰ æ‰€æœ‰æ¸¬è©¦é€šéï¼æ”¹é€²ç‰ˆæ¨¡å‹å¯ä»¥ä½¿ç”¨ã€‚")
        print("\nä¸‹ä¸€æ­¥:")
        print("  conda activate deeplob-pro")
        print("  python scripts/train_deeplob_v5.py \\")
        print("      --config configs/train_v5_improved.yaml \\")
        print("      --epochs 10")
        return 0
    else:
        print("\nâŒ éƒ¨åˆ†æ¸¬è©¦å¤±æ•—ï¼Œè«‹æª¢æŸ¥éŒ¯èª¤è¨Šæ¯")
        return 1


if __name__ == "__main__":
    sys.exit(main())
