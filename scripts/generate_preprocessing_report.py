# -*- coding: utf-8 -*-
"""
generate_preprocessing_report.py - é è™•ç†æ•´é«”å ±å‘Šç”Ÿæˆå·¥å…·
=============================================================================
ã€æ›´æ–°æ—¥æœŸã€‘2025-10-21
ã€ç‰ˆæœ¬èªªæ˜ã€‘v1.0 - å½™ç¸½æ‰€æœ‰é è™•ç†çµæœ

åŠŸèƒ½ï¼š
  1. æƒæ preprocessed_v5/daily/ ä¸­çš„æ‰€æœ‰ summary.json
  2. å½™ç¸½çµ±è¨ˆè³‡è¨Š
  3. ç”Ÿæˆ CSV å’Œ JSON å ±å‘Š
  4. è¼¸å‡ºæ§åˆ¶å°æ‘˜è¦

ä½¿ç”¨æ–¹å¼ï¼š
  python scripts/generate_preprocessing_report.py \
      --preprocessed-dir ./data/preprocessed_v5

è¼¸å‡ºï¼š
  - data/preprocessed_v5/reports/overall_summary.json
  - data/preprocessed_v5/reports/daily_statistics.csv
  - data/preprocessed_v5/reports/filter_decisions.csv

ç‰ˆæœ¬ï¼šv1.0
æ›´æ–°ï¼š2025-10-21
"""

import os
import json
import argparse
import logging
import glob
from typing import List, Dict, Any
from datetime import datetime

import numpy as np
import pandas as pd

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)


def load_all_summaries(preprocessed_dir: str) -> List[Dict[str, Any]]:
    """è¼‰å…¥æ‰€æœ‰ summary.json"""
    daily_dir = os.path.join(preprocessed_dir, "daily")

    if not os.path.exists(daily_dir):
        logging.error(f"ç›®éŒ„ä¸å­˜åœ¨: {daily_dir}")
        return []

    summaries = []

    for summary_file in sorted(glob.glob(os.path.join(daily_dir, "*", "summary.json"))):
        try:
            with open(summary_file, "r", encoding="utf-8") as f:
                summary = json.load(f)
                summaries.append(summary)
        except Exception as e:
            logging.warning(f"ç„¡æ³•è®€å– {summary_file}: {e}")

    logging.info(f"è¼‰å…¥äº† {len(summaries)} å€‹æ¯æ—¥æ‘˜è¦")

    return summaries


def generate_overall_report(summaries: List[Dict], output_dir: str):
    """ç”Ÿæˆæ•´é«”å ±å‘Š"""
    if not summaries:
        logging.warning("ç„¡å¯ç”¨æ‘˜è¦ï¼Œè·³éå ±å‘Šç”Ÿæˆ")
        return

    # å½™ç¸½çµ±è¨ˆ
    total_symbols = sum(s['total_symbols'] for s in summaries)
    total_passed = sum(s['passed_filter'] for s in summaries)
    total_filtered = sum(s['filtered_out'] for s in summaries)

    # æ¯æ—¥çµ±è¨ˆ DataFrame
    daily_stats = []

    for s in summaries:
        daily_stats.append({
            'date': s['date'],
            'total_symbols': s['total_symbols'],
            'passed_filter': s['passed_filter'],
            'filtered_out': s['filtered_out'],
            'pass_rate': s['passed_filter'] / s['total_symbols'] * 100 if s['total_symbols'] > 0 else 0,
            'filter_threshold': s['filter_threshold'],
            'filter_method': s['filter_method'],
            'range_min': s['volatility_distribution']['min'],
            'range_max': s['volatility_distribution']['max'],
            'range_mean': s['volatility_distribution']['mean'],
            'range_P50': s['volatility_distribution']['P50'],
            'pred_down': s['predicted_label_dist']['down'],
            'pred_neutral': s['predicted_label_dist']['neutral'],
            'pred_up': s['predicted_label_dist']['up']
        })

    df_daily = pd.DataFrame(daily_stats)

    # éæ¿¾æ±ºç­– DataFrame
    filter_decisions = []

    for s in summaries:
        filter_decisions.append({
            'date': s['date'],
            'filter_method': s['filter_method'],
            'filter_threshold': s['filter_threshold'],
            'P10': s['volatility_distribution']['P10'],
            'P25': s['volatility_distribution']['P25'],
            'P50': s['volatility_distribution']['P50'],
            'P75': s['volatility_distribution']['P75'],
            'pred_neutral_pct': s['predicted_label_dist']['neutral'] * 100
        })

    df_filter = pd.DataFrame(filter_decisions)

    # æ•´é«”æ‘˜è¦
    overall_summary = {
        "generated_at": datetime.now().isoformat(),
        "n_days": len(summaries),
        "date_range": {
            "start": summaries[0]['date'] if summaries else None,
            "end": summaries[-1]['date'] if summaries else None
        },

        "total_statistics": {
            "total_symbols": int(total_symbols),
            "passed_filter": int(total_passed),
            "filtered_out": int(total_filtered),
            "overall_pass_rate": float(total_passed / total_symbols * 100) if total_symbols > 0 else 0
        },

        "filter_threshold_distribution": {
            "methods": df_filter['filter_method'].value_counts().to_dict(),
            "threshold_mean": float(df_filter['filter_threshold'].mean()),
            "threshold_std": float(df_filter['filter_threshold'].std()),
            "threshold_min": float(df_filter['filter_threshold'].min()),
            "threshold_max": float(df_filter['filter_threshold'].max())
        },

        "volatility_statistics": {
            "range_mean_avg": float(df_daily['range_mean'].mean()),
            "range_mean_std": float(df_daily['range_mean'].std()),
            "range_P50_avg": float(df_daily['range_P50'].mean())
        },

        "predicted_label_distribution": {
            "down_avg": float(df_daily['pred_down'].mean()),
            "neutral_avg": float(df_daily['pred_neutral'].mean()),
            "up_avg": float(df_daily['pred_up'].mean())
        }
    }

    # ä¿å­˜å ±å‘Š
    os.makedirs(output_dir, exist_ok=True)

    # JSON æ‘˜è¦
    json_path = os.path.join(output_dir, "overall_summary.json")
    with open(json_path, "w", encoding="utf-8") as f:
        json.dump(overall_summary, f, ensure_ascii=False, indent=2)
    logging.info(f"âœ… å·²ä¿å­˜: {json_path}")

    # æ¯æ—¥çµ±è¨ˆ CSV
    csv_daily_path = os.path.join(output_dir, "daily_statistics.csv")
    df_daily.to_csv(csv_daily_path, index=False, encoding='utf-8-sig')
    logging.info(f"âœ… å·²ä¿å­˜: {csv_daily_path}")

    # éæ¿¾æ±ºç­– CSV
    csv_filter_path = os.path.join(output_dir, "filter_decisions.csv")
    df_filter.to_csv(csv_filter_path, index=False, encoding='utf-8-sig')
    logging.info(f"âœ… å·²ä¿å­˜: {csv_filter_path}")

    # æ§åˆ¶å°è¼¸å‡º
    print_console_summary(overall_summary, df_daily, df_filter)


def print_console_summary(summary: Dict, df_daily: pd.DataFrame, df_filter: pd.DataFrame):
    """æ§åˆ¶å°è¼¸å‡ºæ‘˜è¦"""
    logging.info("\n" + "="*60)
    logging.info("ğŸ“Š é è™•ç†æ•´é«”å ±å‘Š")
    logging.info("="*60)

    logging.info(f"è™•ç†æœŸé–“: {summary['date_range']['start']} ~ {summary['date_range']['end']}")
    logging.info(f"ç¸½å¤©æ•¸: {summary['n_days']} å¤©")

    logging.info(f"\nğŸ“ˆ è‚¡ç¥¨çµ±è¨ˆ:")
    logging.info(f"  ç¸½ symbol-day æ•¸: {summary['total_statistics']['total_symbols']:,}")
    logging.info(f"  é€šééæ¿¾: {summary['total_statistics']['passed_filter']:,} ({summary['total_statistics']['overall_pass_rate']:.1f}%)")
    logging.info(f"  è¢«éæ¿¾: {summary['total_statistics']['filtered_out']:,}")

    logging.info(f"\nğŸ¯ éæ¿¾é–¾å€¼çµ±è¨ˆ:")
    logging.info(f"  å¹³å‡é–¾å€¼: {summary['filter_threshold_distribution']['threshold_mean']:.4f}")
    logging.info(f"  é–¾å€¼ç¯„åœ: [{summary['filter_threshold_distribution']['threshold_min']:.4f}, {summary['filter_threshold_distribution']['threshold_max']:.4f}]")
    logging.info(f"  é¸æ“‡æ–¹æ³•åˆ†å¸ƒ:")
    for method, count in summary['filter_threshold_distribution']['methods'].items():
        logging.info(f"    {method}: {count} å¤©")

    logging.info(f"\nğŸ“Š æ³¢å‹•ç‡çµ±è¨ˆ:")
    logging.info(f"  å¹³å‡éœ‡ç›ªå¹…åº¦: {summary['volatility_statistics']['range_mean_avg']*100:.2f}%")
    logging.info(f"  ä¸­ä½æ•¸éœ‡ç›ª (P50): {summary['volatility_statistics']['range_P50_avg']*100:.2f}%")

    logging.info(f"\nğŸ·ï¸ é æ¸¬æ¨™ç±¤åˆ†å¸ƒ (å¹³å‡):")
    pred = summary['predicted_label_distribution']
    logging.info(f"  Down: {pred['down_avg']*100:.1f}%")
    logging.info(f"  Neutral: {pred['neutral_avg']*100:.1f}%")
    logging.info(f"  Up: {pred['up_avg']*100:.1f}%")

    # æª¢æŸ¥æ˜¯å¦ç¬¦åˆç›®æ¨™
    target_neutral = 40.0
    actual_neutral = pred['neutral_avg'] * 100

    if 30 <= actual_neutral <= 45:
        logging.info(f"  âœ… Neutral æ¯”ä¾‹ç¬¦åˆç›®æ¨™ (30-45%)")
    elif actual_neutral > 50:
        logging.warning(f"  âš ï¸ Neutral æ¯”ä¾‹éé«˜ ({actual_neutral:.1f}% > 50%)ï¼Œå»ºè­°æé«˜éæ¿¾é–¾å€¼")
    elif actual_neutral < 20:
        logging.warning(f"  âš ï¸ Neutral æ¯”ä¾‹éä½ ({actual_neutral:.1f}% < 20%)ï¼Œå»ºè­°é™ä½éæ¿¾é–¾å€¼")

    logging.info("="*60 + "\n")


def parse_args():
    p = argparse.ArgumentParser("generate_preprocessing_report", description="é è™•ç†æ•´é«”å ±å‘Šç”Ÿæˆå·¥å…·")
    p.add_argument("--preprocessed-dir", default="./data/preprocessed_v5", help="é è™•ç†çµæœç›®éŒ„")
    return p.parse_args()


def main():
    args = parse_args()

    preprocessed_dir = args.preprocessed_dir

    if not os.path.exists(preprocessed_dir):
        logging.error(f"é è™•ç†ç›®éŒ„ä¸å­˜åœ¨: {preprocessed_dir}")
        return 1

    # è¼‰å…¥æ‘˜è¦
    summaries = load_all_summaries(preprocessed_dir)

    if not summaries:
        logging.error("æ²’æœ‰å¯ç”¨çš„æ‘˜è¦æ•¸æ“š")
        return 1

    # ç”Ÿæˆå ±å‘Š
    reports_dir = os.path.join(preprocessed_dir, "reports")
    generate_overall_report(summaries, reports_dir)

    logging.info("\nâœ… å ±å‘Šç”Ÿæˆå®Œæˆï¼\n")

    return 0


if __name__ == "__main__":
    import sys
    sys.exit(main())
