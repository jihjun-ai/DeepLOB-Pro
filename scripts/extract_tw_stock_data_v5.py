# -*- coding: utf-8 -*-
"""
extract_tw_stock_data_v5.py - V5 Pro ä¸“ä¸šåŒ–èµ„æ–™æµæ°´çº¿
=============================================================================
ã€æ›´æ–°æ—¥æœŸã€‘2025-10-18
ã€ç‰ˆæœ¬è¯´æ˜ã€‘v5.0 - ä¸“ä¸šçº§æ ‡ç­¾ä¸æ³¢åŠ¨ç‡ï¼ˆtriple-barrier + archï¼‰

åŸºäºã€ŠV5_Pro_NoMLFinLab_Guide.mdã€‹å®ç°ï¼Œå°† V4 çš„å›ºå®š k æ­¥æ ‡ç­¾å…¨é¢å‡çº§ä¸ºï¼š
  1) ä¸“ä¸šæ³¢åŠ¨ç‡ä¼°è®¡ï¼ˆEWMA / Yang-Zhang / GARCHï¼‰
  2) Triple-Barrier äº‹ä»¶æ‰“æ ‡ï¼ˆæ­¢ç›ˆ/æ­¢æŸ/åˆ°æœŸï¼‰
  3) æ ·æœ¬æƒé‡ï¼ˆæ”¶ç›Š Ã— æ—¶é—´è¡°å‡ Ã— ç±»åˆ«å¹³è¡¡ï¼‰

=============================================================================
æ ¸å¿ƒæ”¹è¿›ï¼ˆV4 â†’ V5ï¼‰
=============================================================================
âœ… æ³¢åŠ¨ç‡ä¼°è®¡ï¼š
  - V4: ç®€å•ç›¸å¯¹æ³¢åŠ¨ç‡ std(mid) / mean(mid)
  - V5: arch åº“ä¸“ä¸šä¼°è®¡ï¼ˆEWMA / Yang-Zhang / GARCHï¼‰

âœ… æ ‡ç­¾ç”Ÿæˆï¼š
  - V4: å›ºå®š k æ­¥ä»·æ ¼å˜åŠ¨ï¼ˆalpha=0.002 é˜ˆå€¼ï¼‰
  - V5: Triple-Barrier è‡ªé€‚åº”æ ‡ç­¾ï¼ˆåŸºäºæ³¢åŠ¨ç‡å€æ•°ï¼‰

âœ… æ ·æœ¬æƒé‡ï¼š
  - V4: æ— æƒé‡ï¼ˆæˆ–ç®€å•è¿‡æ»¤ï¼‰
  - V5: æ”¶ç›ŠåŠ æƒ + æ—¶é—´è¡°å‡ + ç±»åˆ«å¹³è¡¡

âœ… è¾“å‡ºæ ¼å¼ï¼š
  - V4: X (N,100,20), y {0,1,2}
  - V5: X (N,100,20), y {0,1,2}, w (N,) + è¯¦ç»† metadata

=============================================================================
ä½¿ç”¨æ–¹å¼
=============================================================================

ã€åŸºæœ¬ä½¿ç”¨ã€‘ï¼ˆä½¿ç”¨é¢„è®¾é…ç½®ï¼‰ï¼š
  python scripts/extract_tw_stock_data_v5.py \
      --input-dir ./data/temp \
      --output-dir ./data/processed_v5 \
      --config configs/config_pro_v5.yaml

ã€è¾“å‡ºç»“æœã€‘ï¼š
  1. NPZ æ–‡ä»¶ï¼ˆæ–°å¢æ ·æœ¬æƒé‡ï¼‰ï¼š
     - ./data/processed_v5/npz/stock_embedding_train.npz
       å†…å«ï¼šX, y, stock_ids, weights (æ–°å¢)
     - ./data/processed_v5/npz/stock_embedding_val.npz
     - ./data/processed_v5/npz/stock_embedding_test.npz

  2. Metadataï¼ˆåŒ…å«å®Œæ•´ V5 é…ç½®ï¼‰ï¼š
     - ./data/processed_v5/npz/normalization_meta.json
       {
         "version": "5.0.0",
         "volatility_method": "ewma",
         "triple_barrier": {...},
         "sample_weights": {...},
         "label_distribution": {...}
       }

=============================================================================
å›ºå®šè®¾å®šï¼ˆç»§æ‰¿è‡ª V4ï¼‰
=============================================================================
- NoAuctionï¼šç§»é™¤ IsTrialMatch=='1'ï¼Œå¹¶é™åˆ¶ 09:00:00â€“13:30:00
- 10 äº‹ä»¶èšåˆï¼ˆä¸é‡å ï¼‰ï¼šä»·æ ¼/æ•°é‡çš†å–è§†çª—æœ«ç«¯
- æ ‡å‡†åŒ–ï¼šZ-Scoreï¼ˆåŸºäºè®­ç»ƒé›†ï¼‰
- æ—¶é—´åºåˆ—çª—å£ï¼š100 timesteps
- è¾“å‡ºç»´åº¦ï¼š20 ç»´ LOB ç‰¹å¾ï¼ˆ5 æ¡£ LOBï¼‰

=============================================================================
ä¾èµ–å¥—ä»¶ï¼ˆæ–°å¢ï¼‰
=============================================================================
pip install triple-barrier arch ruamel.yaml pandas scikit-learn

ã€å¯é€‰ã€‘ï¼š
pip install skfolio vectorbt nautilus-trader  # ç”¨äºåç»­ CV/å›æµ‹/æ¡å½¢èšåˆ

ç‰ˆæœ¬ï¼šv5.0
æ›´æ–°ï¼š2025-10-18
"""

import os
import re
import json
import argparse
import glob
import logging
import traceback
import warnings
from collections import defaultdict
from datetime import datetime
from typing import List, Tuple, Dict, Any, Optional

import numpy as np
import pandas as pd
import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° Python è·„å¾„
project_root = Path(__file__).parent.parent
if str(project_root) not in sys.path:
    sys.path.insert(0, str(project_root))

from src.utils.yaml_manager import YAMLManager

# è®¾å®šç‰ˆæœ¬å·
VERSION = "5.0.0"

# è®¾å®šæ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)

# å¿½ç•¥éƒ¨åˆ†ç¬¬ä¸‰æ–¹åº“è­¦å‘Š
warnings.filterwarnings('ignore', category=FutureWarning)
warnings.filterwarnings('ignore', category=RuntimeWarning)

# å›ºå®šå¸¸æ•°ï¼ˆç»§æ‰¿è‡ª V4ï¼‰
AGG_FACTOR = 10
SEQ_LEN = 100
TRADING_START = 90000   # 09:00:00
TRADING_END   = 133000  # 13:30:00

# æ¬„ä½ç´¢å¼•ï¼ˆè¼¸å…¥ï¼‰
IDX_REF = 3
IDX_UPPER = 4
IDX_LOWER = 5
IDX_LASTPRICE = 9
IDX_LASTVOL = 10
IDX_TV = 11
IDX_TIME = 32
IDX_TRIAL = 33

# äº”æª”åƒ¹é‡ç´¢å¼•å°æ‡‰
BID_P_IDX = [12, 14, 16, 18, 20]
BID_Q_IDX = [13, 15, 17, 19, 21]
ASK_P_IDX = [22, 24, 26, 28, 30]
ASK_Q_IDX = [23, 25, 27, 29, 31]

# å…¨åŸŸçµ±è¨ˆè®Šæ•¸
global_stats = {
    "total_raw_events": 0,
    "cleaned_events": 0,
    "aggregated_points": 0,
    "valid_windows": 0,
    "tb_success": 0,
    "volatility_stats": [],  # æ–°å¢ï¼šå„²å­˜æ¯å€‹ symbol-day çš„éœ‡ç›ªçµ±è¨ˆ
    "volatility_filtered": 0  # æ–°å¢ï¼šè¢«éœ‡ç›ªç¯©é¸éæ¿¾çš„æ¨£æœ¬æ•¸
}


# ============================================================
# V5 æ ¸å¿ƒæ¨¡å—ï¼šä¸“ä¸šæ³¢åŠ¨ç‡ä¼°è®¡ï¼ˆarchï¼‰
# ============================================================

def ewma_vol(close: pd.Series, halflife: int = 60) -> pd.Series:
    """
    EWMA æ³¢åŠ¨ç‡ä¼°è®¡

    Args:
        close: æ”¶ç›˜ä»·åºåˆ—
        halflife: EWMA åŠè¡°æœŸï¼ˆbarsï¼‰

    Returns:
        vol: æ³¢åŠ¨ç‡åºåˆ—
    """
    ret = np.log(close).diff()
    var = ret.ewm(halflife=halflife, adjust=False).var()
    vol = np.sqrt(var).bfill()
    return vol


def yang_zhang_vol(ohlc: pd.DataFrame, window: int = 60) -> pd.Series:
    """
    Yang-Zhang æ³¢åŠ¨ç‡ä¼°è®¡ï¼ˆåˆ©ç”¨ OHLCï¼‰

    Args:
        ohlc: åŒ…å« open, high, low, close çš„ DataFrame
        window: æ»šåŠ¨çª—å£å¤§å°

    Returns:
        vol: Yang-Zhang æ³¢åŠ¨ç‡åºåˆ—
    """
    o = ohlc['open'].astype(float)
    h = ohlc['high'].astype(float)
    l = ohlc['low'].astype(float)
    c = ohlc['close'].astype(float)

    k = 0.34 / (1.34 + (window + 1) / (window - 1))

    log_oc = np.log(o / c.shift(1))
    log_co = np.log(c / o)
    rs = (np.log(h / l)) ** 2

    sigma_o = log_oc.rolling(window).var()
    sigma_c = log_co.rolling(window).var()
    sigma_rs = rs.rolling(window).mean()

    yz = sigma_o + k * sigma_c + (1 - k) * sigma_rs
    vol = np.sqrt(yz).replace([np.inf, -np.inf], np.nan).bfill()

    return vol


def garch11_vol(close: pd.Series) -> pd.Series:
    """
    GARCH(1,1) æ³¢åŠ¨ç‡ä¼°è®¡

    Args:
        close: æ”¶ç›˜ä»·åºåˆ—

    Returns:
        vol: GARCH æ³¢åŠ¨ç‡åºåˆ—
    """
    try:
        from arch import arch_model

        ret = 100 * np.log(close).diff().dropna()

        if len(ret) < 50:
            logging.warning("GARCH: èµ„æ–™ç‚¹ä¸è¶³ï¼Œå›é€€åˆ° EWMA")
            return ewma_vol(close, halflife=60)

        am = arch_model(ret, vol='GARCH', p=1, q=1, dist='normal')
        res = am.fit(disp='off', show_warning=False)
        fcast = res.forecast(horizon=1, reindex=True).variance
        vol = np.sqrt(fcast.squeeze()) / 100.0

        return vol.reindex(close.index).bfill()

    except Exception as e:
        logging.warning(f"GARCH å¤±è´¥: {e}ï¼Œå›é€€åˆ° EWMA")
        return ewma_vol(close, halflife=60)


# ============================================================
# V5 æ ¸å¿ƒæ¨¡å—ï¼šTriple-Barrier æ ‡ç­¾ç”Ÿæˆ
# ============================================================

def tb_labels(close: pd.Series,
              vol: pd.Series,
              pt_mult: float = 2.0,
              sl_mult: float = 2.0,
              max_holding: int = 200,
              min_return: float = 0.0001,
              day_end_idx: Optional[int] = None) -> pd.DataFrame:
    """
    Triple-Barrier æ ‡ç­¾ç”Ÿæˆï¼ˆè‡ªå®šä¹‰å®ç°ï¼‰

    Args:
        close: æ”¶ç›˜ä»·åºåˆ—
        vol: æ³¢åŠ¨ç‡åºåˆ—
        pt_mult: æ­¢ç›ˆå€æ•°
        sl_mult: æ­¢æŸå€æ•°
        max_holding: æœ€å¤§æŒæœ‰æœŸï¼ˆbarsï¼‰
        min_return: æœ€å°æŠ¥é…¬é˜ˆå€¼
        day_end_idx: å½“æ—¥æœ€åä¸€ä¸ªç´¢å¼•ï¼ˆé˜²æ­¢è¶Šæ—¥ï¼ŒNone è¡¨ç¤ºæ— é™åˆ¶ï¼‰

    Returns:
        DataFrame åŒ…å«:
            - y: {-1, 0, 1} æ ‡ç­¾
            - ret: å®é™…æ”¶ç›Š
            - tt: è§¦å‘æ—¶é—´æ­¥æ•°
            - why: è§¦å‘åŸå›  {'up', 'down', 'time'}
            - up_p: æ­¢ç›ˆä»·æ ¼
            - dn_p: æ­¢æŸä»·æ ¼
    """
    try:
        n = len(close)
        results = []

        for i in range(n - 1):
            entry_price = close.iloc[i]
            entry_vol = vol.iloc[i]

            # è®¡ç®—æ­¢ç›ˆæ­¢æŸä»·æ ¼
            up_barrier = entry_price * (1 + pt_mult * entry_vol)
            dn_barrier = entry_price * (1 - sl_mult * entry_vol)

            # æŸ¥æ‰¾è§¦å‘ç‚¹ï¼ˆå…³é”®ä¿®æ­£ï¼šé™åˆ¶åœ¨å½“æ—¥å†…ï¼‰
            if day_end_idx is not None:
                end_idx = min(i + max_holding, day_end_idx + 1, n)
            else:
                end_idx = min(i + max_holding, n)

            triggered = False
            trigger_idx = end_idx - 1
            trigger_why = 'time'

            for j in range(i + 1, end_idx):
                future_price = close.iloc[j]

                # æ£€æŸ¥æ˜¯å¦è§¦åŠæ­¢ç›ˆ
                if future_price >= up_barrier:
                    trigger_idx = j
                    trigger_why = 'up'
                    triggered = True
                    break

                # æ£€æŸ¥æ˜¯å¦è§¦åŠæ­¢æŸ
                if future_price <= dn_barrier:
                    trigger_idx = j
                    trigger_why = 'down'
                    triggered = True
                    break

            # è®¡ç®—å®é™…æ”¶ç›Š
            exit_price = close.iloc[trigger_idx]
            ret = (exit_price - entry_price) / entry_price

            # æ ¸å¿ƒä¿®æ­£ï¼šåªæœ‰ verticalï¼ˆæ—¶é—´åˆ°ï¼‰æ—¶æ‰ç”¨ min_return åˆ¤æ–­
            if trigger_why == 'time':
                # æ—¶é—´åˆ°æœŸï¼šæ£€æŸ¥æ˜¯å¦è¶…è¿‡æœ€å°æŠ¥é…¬é˜ˆå€¼
                if np.abs(ret) < min_return:
                    label = 0  # æŒå¹³ï¼ˆæ”¶ç›Šå¤ªå°ï¼‰
                else:
                    label = int(np.sign(ret))  # -1ï¼ˆä¸‹è·Œï¼‰æˆ– 1ï¼ˆä¸Šæ¶¨ï¼‰
            else:
                # PT/SL è§¦å‘ï¼šå›ºå®šæ ‡ç­¾ä¸º Â±1
                label = int(np.sign(ret))

            results.append({
                'ret': ret,
                'y': label,
                'tt': trigger_idx - i,
                'why': trigger_why,
                'up_p': up_barrier,
                'dn_p': dn_barrier
            })

        # ä¸ºæœ€åä¸€ä¸ªç‚¹æ·»åŠ é»˜è®¤å€¼
        if n > 0:
            results.append({
                'ret': 0.0,
                'y': 0,
                'tt': 0,
                'why': 'time',
                'up_p': close.iloc[-1],
                'dn_p': close.iloc[-1]
            })

        out = pd.DataFrame(results, index=close.index)

        global_stats["tb_success"] += 1

        return out.dropna()

    except Exception as e:
        logging.error(f"Triple-Barrier å¤±è´¥: {e}")
        raise


# ============================================================
# V5 æ ¸å¿ƒæ¨¡å—ï¼šæ ·æœ¬æƒé‡è®¡ç®—
# ============================================================

def make_sample_weight(ret: pd.Series,
                      tt: pd.Series,
                      y: pd.Series,
                      tau: float = 100.0,
                      scale: float = 10.0,
                      balance: bool = True,
                      use_log_scale: bool = True) -> pd.Series:
    """
    æ ·æœ¬æƒé‡è®¡ç®—ï¼ˆæ”¶ç›Š Ã— æ—¶é—´è¡°å‡ Ã— ç±»åˆ«å¹³è¡¡ï¼‰

    è¨­è¨ˆé‚è¼¯ï¼š
      - æ”¶ç›Šæ¬Šé‡ï¼šåæ˜ æ¨£æœ¬çš„é‡è¦æ€§ï¼ˆå¤§æ”¶ç›Šæ¨£æœ¬æ›´é‡è¦ï¼‰
      - æ™‚é–“è¡°æ¸›ï¼šæ—©è§¸ç™¼çš„æ¨£æœ¬æ›´é‡è¦ï¼ˆä¿¡è™Ÿæ›´æ˜ç¢ºï¼‰
      - é¡åˆ¥å¹³è¡¡ï¼šæ¯æ—¥ç¨ç«‹è¨ˆç®—ï¼Œåæ˜ ç•¶æ—¥çš„æ¨™ç±¤åˆ†å¸ƒç‰¹æ€§
      - è£å‰ªèˆ‡æ­¸ä¸€åŒ–ï¼šé¿å…æ¥µç«¯å€¼ï¼Œç¢ºä¿ç©©å®šæ€§

    Args:
        ret: å®é™…æ”¶ç›Šåºåˆ—
        tt: è§¦å‘æ—¶é—´æ­¥æ•°
        y: æ ‡ç­¾åºåˆ—
        tau: æ—¶é—´è¡°å‡å‚æ•°
        scale: æ”¶ç›Šç¼©æ”¾ç³»æ•°
        balance: æ˜¯å¦å¯ç”¨ç±»åˆ«å¹³è¡¡
        use_log_scale: æ˜¯å¦ä½¿ç”¨å¯¹æ•°ç¼©æ”¾ï¼ˆé¿å…å°æ”¶ç›Šè¢«è¿‡åº¦å‹åˆ¶ï¼‰

    Returns:
        w: æ ·æœ¬æƒé‡åºåˆ—ï¼ˆå½’ä¸€åŒ–åˆ°å‡å€¼ä¸º 1ï¼‰
    """
    from sklearn.utils.class_weight import compute_class_weight

    # åŸºç¡€æƒé‡ï¼šæ”¶ç›Šéƒ¨åˆ†
    ret_array = np.array(ret.values, dtype=np.float64)
    tt_array = np.array(tt.values, dtype=np.float64)

    if use_log_scale:
        # å¯¹æ•°ç¼©æ”¾ï¼šlog(1 + |return| * 1000) é¿å…å°æ”¶ç›Šè¢«å‹åˆ¶
        # ä¾‹å¦‚ï¼š0.0002 â†’ log(1.2) â‰ˆ 0.18, 0.0025 â†’ log(3.5) â‰ˆ 1.25 (æ¯”ä¾‹ç´„ 7:1 è€Œé 12:1)
        ret_weight = np.log1p(np.abs(ret_array) * 1000) * scale
        # ğŸ”§ è¨­ç½®æœ€å°æ¬Šé‡ï¼ˆé¿å…æ¥µå°æ”¶ç›Šæ¬Šé‡éä½ï¼‰- é—œéµä¿®æ­£ï¼
        ret_weight = np.maximum(ret_weight, 0.1)  # æœ€å°æ¬Šé‡ 0.1
    else:
        # çº¿æ€§ç¼©æ”¾ï¼š|return| * scale
        ret_weight = np.abs(ret_array) * scale

    # æ—¶é—´è¡°å‡
    time_decay = np.exp(-tt_array / float(tau))
    base = ret_weight * time_decay
    base = np.clip(base, 0.05, None)  # ğŸ”§ æé«˜æœ€å°å€¼ï¼ˆ1e-3 â†’ 0.05ï¼‰

    # ç±»åˆ«å¹³è¡¡ï¼ˆæŒ‰æ—¥è¨ˆç®—ï¼Œåæ˜ ç•¶æ—¥æ¨™ç±¤åˆ†å¸ƒï¼‰
    if balance:
        classes = np.array(sorted(y.unique()))
        y_array = np.array(y.values, dtype=np.int64)
        cls_w = compute_class_weight('balanced', classes=classes, y=y_array)

        # ğŸ”§ é—œéµä¿®æ­£ï¼šè£å‰ªé¡åˆ¥æ¬Šé‡æœ¬èº«ï¼ˆé¿å…å–®æ—¥æŸé¡åˆ¥æ¨£æœ¬éå°‘å°è‡´æ¥µç«¯æ¬Šé‡ï¼‰
        # ç¯„ä¾‹ï¼šç•¶ Class 1 åªæœ‰ 1 å€‹æ¨£æœ¬æ™‚ï¼Œsklearn å¯èƒ½çµ¦å‡º 56.7 çš„æ¬Šé‡
        cls_w = np.clip(cls_w, 0.5, 3.0)  # é™åˆ¶åœ¨ [0.5, 3.0] ç¯„åœ
        cls_w = cls_w / cls_w.mean()      # æ­¸ä¸€åŒ–ï¼ˆä¿æŒå‡å€¼ç‚º 1ï¼‰

        w_map = dict(zip(classes, cls_w))
        cw = np.array(y.map(w_map).values, dtype=np.float64)
        w = base * cw
    else:
        w = base

    # ğŸ”§ å…ˆæ­¸ä¸€åŒ–ï¼Œå†è£å‰ªï¼ˆé—œéµä¿®æ­£ï¼ï¼‰
    # é †åºå¾ˆé‡è¦ï¼šå¦‚æœå…ˆè£å‰ªå†æ­¸ä¸€åŒ–ï¼Œæ­¸ä¸€åŒ–æœƒç ´å£è£å‰ªæ•ˆæœ
    w = w / np.mean(w)  # å…ˆæ­¸ä¸€åŒ–ï¼ˆå‡å€¼ç‚º 1ï¼‰
    w = np.clip(w, 0.1, 5.0)  # å†è£å‰ªï¼ˆç¢ºä¿æœ€çµ‚ç¯„åœ [0.1, 5.0]ï¼‰

    return pd.Series(w, index=y.index)


# ============================================================
# V4 å…¼å®¹å‡½æ•°ï¼ˆç»§æ‰¿ï¼‰
# ============================================================

def parse_args():
    """è§£æå‘½ä»¤åˆ—å‚æ•°"""
    p = argparse.ArgumentParser(
        "extract_tw_stock_data_v5",
        description="å°è‚¡ DeepLOB èµ„æ–™å‰å¤„ç†å·¥å…· (V5 Pro ç‰ˆæœ¬)"
    )
    p.add_argument(
        "--input-dir",
        default="./data/temp",
        type=str,
        help="å«æ¯æ—¥åŸå§‹ .txt çš„èµ„æ–™å¤¹"
    )
    p.add_argument(
        "--output-dir",
        default="./data/processed_v5",
        type=str,
        help="è¾“å‡ºèµ„æ–™å¤¹"
    )
    p.add_argument(
        "--config",
        default="./configs/config_pro_v5.yaml",
        type=str,
        help="V5 é…ç½®æ–‡ä»¶è·¯å¾„"
    )
    p.add_argument(
        "--make-npz",
        action="store_true",
        default=True,
        help="è¾“å‡º 70/15/15 çš„ .npz æ–‡ä»¶"
    )
    p.add_argument(
        "--stats-only",
        action="store_true",
        default=False,
        help="åªäº§ç”Ÿéœ‡ç›ªç»Ÿè®¡æŠ¥å‘Šï¼Œä¸ç”Ÿæˆè®­ç»ƒæ•°æ®ï¼ˆå¿«é€Ÿæ¨¡å¼ï¼‰"
    )
    return p.parse_args()


def load_config(config_path: str) -> Dict[str, Any]:
    """è½½å…¥ V5 é…ç½®æ–‡ä»¶ï¼ˆä½¿ç”¨ YAMLManagerï¼‰

    Args:
        config_path: é…ç½®æ–‡ä»¶è·¯å¾„

    Returns:
        é…ç½®å­—å…¸

    Raises:
        FileNotFoundError: é…ç½®æ–‡ä»¶ä¸å­˜åœ¨æ—¶æŠ›å‡ºå¼‚å¸¸
    """
    if not os.path.exists(config_path):
        raise FileNotFoundError(
            f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}\n"
            f"è¯·ç¡®ä¿é…ç½®æ–‡ä»¶å­˜åœ¨ï¼Œæˆ–ä½¿ç”¨ --config å‚æ•°æŒ‡å®šæ­£ç¡®çš„é…ç½®æ–‡ä»¶è·¯å¾„ã€‚\n"
            f"ç¤ºä¾‹é…ç½®æ–‡ä»¶ä½äº: configs/config_pro_v5.yaml"
        )

    # ä½¿ç”¨ YAMLManager åŠ è½½é…ç½®æ–‡ä»¶ï¼ˆä¼šè‡ªåŠ¨æ£€æŸ¥æ–‡ä»¶å­˜åœ¨æ€§ï¼‰
    yaml_manager = YAMLManager(config_path)
    cfg = yaml_manager.as_dict()

    logging.info(f"å·²è½½å…¥é…ç½®æ–‡ä»¶: {config_path}")
    return cfg


def hhmmss_to_int(s: str) -> int:
    s = s.strip()
    if not s.isdigit():
        return -1
    return int(s)


def to_float(x: str, default=0.0) -> float:
    try:
        return float(x)
    except:
        return default


def is_in_trading_window(t: int) -> bool:
    return TRADING_START <= t <= TRADING_END


def spread_ok(bid1: float, ask1: float) -> bool:
    if bid1 <= 0 or ask1 <= 0:
        return False
    if bid1 >= ask1:
        return False
    mid = 0.5 * (bid1 + ask1)
    return (ask1 - bid1) / max(mid, 1e-12) <= 0.05


def within_limits(px: float, lo: float, hi: float) -> bool:
    if lo > 0 and px < lo - 1e-12:
        return False
    if hi > 0 and px > hi + 1e-12:
        return False
    return True


def parse_line(raw: str) -> Tuple[str, int, Optional[Dict[str, Any]]]:
    """è§£æå•è¡Œæ•°æ®ï¼ˆç»§æ‰¿è‡ª V4ï¼‰"""
    global global_stats
    global_stats["total_raw_events"] += 1

    parts = raw.strip().split("||")
    if len(parts) < 34:
        return ("", -1, None)

    sym = parts[1].strip()

    try:
        t = hhmmss_to_int(parts[IDX_TIME])
    except:
        t = -1

    # è¯•æ’®ç§»é™¤ï¼æ—¶é—´çª—æ£€æŸ¥
    if parts[IDX_TRIAL].strip() == "1":
        return (sym, t, None)
    if not is_in_trading_window(t):
        return (sym, t, None)

    # å–äº”æ¡£ä»·é‡
    bids_p = [to_float(parts[i], 0.0) for i in BID_P_IDX]
    bids_q = [to_float(parts[i], 0.0) for i in BID_Q_IDX]
    asks_p = [to_float(parts[i], 0.0) for i in ASK_P_IDX]
    asks_q = [to_float(parts[i], 0.0) for i in ASK_Q_IDX]

    bid1, ask1 = bids_p[0], asks_p[0]
    if not spread_ok(bid1, ask1):
        return (sym, t, None)

    # é›¶å€¼å¤„ç†
    for p, q in zip(bids_p + asks_p, bids_q + asks_q):
        if p == 0.0 and q != 0.0:
            return (sym, t, None)

    ref = to_float(parts[IDX_REF], 0.0)
    upper = to_float(parts[IDX_UPPER], 0.0)
    lower = to_float(parts[IDX_LOWER], 0.0)
    last_px = to_float(parts[IDX_LASTPRICE], 0.0)
    tv = max(0, int(to_float(parts[IDX_TV], 0.0)))

    # ä»·æ ¼é™åˆ¶æ£€æŸ¥
    prices_to_check = [p for p in bids_p + asks_p if p > 0]
    if not all(within_limits(p, lower, upper) for p in prices_to_check):
        return (sym, t, None)

    # ç»„ 20 ç»´ç‰¹å¾
    feat = np.array(bids_p + asks_p + bids_q + asks_q, dtype=np.float64)
    mid = 0.5 * (bid1 + ask1)

    rec = {
        "feat": feat,
        "mid": mid,
        "ref": ref,
        "upper": upper,
        "lower": lower,
        "last_px": last_px,
        "tv": tv,
        "raw": raw.strip()
    }

    global_stats["cleaned_events"] += 1
    return (sym, t, rec)


def dedup_by_timestamp_keep_last(rows: List[Tuple[int, Dict[str,Any]]]) -> List[Tuple[int, Dict[str,Any]]]:
    """æ—¶é—´æˆ³å»é‡ï¼Œä¿ç•™æœ€åä¸€ç¬”ï¼ˆç»§æ‰¿è‡ª V4ï¼‰"""
    if not rows:
        return rows

    dedup_dict = {}
    for idx, (t, r) in enumerate(rows):
        tv = r.get("tv", 0)
        key = (t, tv)
        dedup_dict[key] = (idx, r)

    result_with_idx = sorted(dedup_dict.values(), key=lambda x: x[0])
    result = [(rows[idx][0], r) for idx, r in result_with_idx]

    return result


def aggregate_chunks_of_10(seq: List[Tuple[int, Dict[str,Any]]]) -> Tuple[np.ndarray, np.ndarray]:
    """æ¯ 10 ç¬”å¿«ç…§ â†’ 1 æ—¶é—´ç‚¹ï¼ˆç»§æ‰¿è‡ª V4ï¼‰"""
    global global_stats

    if len(seq) < AGG_FACTOR:
        return np.zeros((0, 20), dtype=np.float64), np.zeros((0,), dtype=np.float64)

    feats, mids = [], []
    for i in range(AGG_FACTOR-1, len(seq), AGG_FACTOR):
        feat = seq[i][1]["feat"]
        mid = seq[i][1]["mid"]
        feats.append(feat)
        mids.append(mid)

    global_stats["aggregated_points"] += len(feats)
    return np.stack(feats, axis=0), np.array(mids, dtype=np.float64)


def calculate_intraday_volatility(mids: np.ndarray, date: str, symbol: str) -> Optional[Dict[str, Any]]:
    """
    è¨ˆç®—ç•¶æ—¥éœ‡ç›ªçµ±è¨ˆ

    Args:
        mids: ä¸­é–“åƒ¹åºåˆ—
        date: äº¤æ˜“æ—¥æœŸ
        symbol: è‚¡ç¥¨ä»£ç¢¼

    Returns:
        éœ‡ç›ªçµ±è¨ˆå­—å…¸ï¼ŒåŒ…å«ï¼š
        - range_pct: éœ‡ç›ªå¹…åº¦ (æœ€é«˜-æœ€ä½)/é–‹ç›¤
        - high: æœ€é«˜åƒ¹
        - low: æœ€ä½åƒ¹
        - open: é–‹ç›¤åƒ¹
        - close: æ”¶ç›¤åƒ¹
        - return_pct: æ¼²è·Œå¹…
        - n_points: æ•¸æ“šé»æ•¸
    """
    if mids.size == 0:
        return None

    open_price = mids[0]
    close_price = mids[-1]
    high_price = mids.max()
    low_price = mids.min()

    # é¿å…é™¤ä»¥é›¶
    if open_price <= 0:
        return None

    # è¨ˆç®—éœ‡ç›ªå¹…åº¦ï¼ˆç›¸å°æ–¼é–‹ç›¤åƒ¹ï¼‰
    range_pct = (high_price - low_price) / open_price

    # è¨ˆç®—æ¼²è·Œå¹…
    return_pct = (close_price - open_price) / open_price

    return {
        "date": date,
        "symbol": symbol,
        "range_pct": float(range_pct),
        "return_pct": float(return_pct),
        "high": float(high_price),
        "low": float(low_price),
        "open": float(open_price),
        "close": float(close_price),
        "n_points": len(mids)
    }


def generate_volatility_report(vol_stats: List[Dict[str, Any]], out_dir: str):
    """
    ç”Ÿæˆéœ‡ç›ªçµ±è¨ˆå ±å‘Šï¼ˆCSV + JSON + æ§åˆ¶å°è¼¸å‡ºï¼‰

    Args:
        vol_stats: éœ‡ç›ªçµ±è¨ˆåˆ—è¡¨
        out_dir: è¼¸å‡ºç›®éŒ„
    """
    if not vol_stats:
        logging.warning("æ²’æœ‰éœ‡ç›ªçµ±è¨ˆè³‡æ–™å¯ç”¢ç”Ÿå ±å‘Š")
        return

    df = pd.DataFrame(vol_stats)

    # è¨ˆç®—çµ±è¨ˆæ‘˜è¦
    range_values = df['range_pct'].values * 100  # è½‰ç‚ºç™¾åˆ†æ¯”
    return_values = df['return_pct'].values * 100

    # åˆ†ä½æ•¸çµ±è¨ˆ
    percentiles = [10, 25, 50, 75, 90, 95, 99]
    range_percentiles = np.percentile(range_values, percentiles)
    return_percentiles = np.percentile(return_values, percentiles)

    # é–¾å€¼çµ±è¨ˆ
    thresholds = [0.5, 1.0, 1.5, 2.0, 2.5, 3.0, 4.0, 5.0]
    threshold_stats = []

    for threshold in thresholds:
        count = (range_values >= threshold).sum()
        pct = count / len(range_values) * 100
        threshold_stats.append({
            'threshold_pct': float(threshold),
            'count': int(count),
            'percentage': float(pct)
        })

    # æ§åˆ¶å°è¼¸å‡º
    logging.info(f"\n{'='*60}")
    logging.info("ğŸ“Š éœ‡ç›ªå¹…åº¦çµ±è¨ˆå ±å‘Šï¼ˆIntraday Range Analysisï¼‰")
    logging.info(f"{'='*60}")
    logging.info(f"ç¸½æ¨£æœ¬æ•¸: {len(df):,} å€‹ symbol-day çµ„åˆ")
    logging.info(f"è‚¡ç¥¨æ•¸: {df['symbol'].nunique()} æª”")
    logging.info(f"äº¤æ˜“æ—¥æ•¸: {df['date'].nunique()} å¤©")
    logging.info(f"\néœ‡ç›ªå¹…åº¦ (Range %) åˆ†å¸ƒ:")
    logging.info(f"  æœ€å°å€¼: {range_values.min():.2f}%")
    logging.info(f"  æœ€å¤§å€¼: {range_values.max():.2f}%")
    logging.info(f"  å¹³å‡å€¼: {range_values.mean():.2f}%")
    logging.info(f"  ä¸­ä½æ•¸: {np.median(range_values):.2f}%")
    logging.info(f"  æ¨™æº–å·®: {range_values.std():.2f}%")

    logging.info(f"\nåˆ†ä½æ•¸åˆ†å¸ƒ:")
    for p, val in zip(percentiles, range_percentiles):
        logging.info(f"  P{p:2d}: {val:6.2f}%")

    logging.info(f"\né–¾å€¼ç¯©é¸çµ±è¨ˆï¼ˆéœ‡ç›ª â‰¥ X% çš„æ¨£æœ¬æ•¸ï¼‰:")
    logging.info(f"{'é–¾å€¼':>6} | {'æ¨£æœ¬æ•¸':>8} | {'ä½”æ¯”':>6}")
    logging.info(f"{'-'*6}-+-{'-'*8}-+-{'-'*6}")
    for ts in threshold_stats:
        logging.info(f"{ts['threshold_pct']:5.1f}% | {ts['count']:8,} | {ts['percentage']:5.1f}%")

    logging.info(f"\næ¼²è·Œå¹… (Return %) åˆ†å¸ƒ:")
    logging.info(f"  å¹³å‡å€¼: {return_values.mean():.2f}%")
    logging.info(f"  ä¸­ä½æ•¸: {np.median(return_values):.2f}%")
    logging.info(f"  æ¨™æº–å·®: {return_values.std():.2f}%")

    # æ‰¾å‡ºæ¥µç«¯æ¡ˆä¾‹
    top_volatile = df.nlargest(10, 'range_pct')[['date', 'symbol', 'range_pct', 'return_pct']]
    logging.info(f"\néœ‡ç›ªæœ€å¤§çš„ 10 å€‹æ¨£æœ¬:")
    for idx, row in top_volatile.iterrows():
        logging.info(f"  {row['symbol']} @ {row['date']}: éœ‡ç›ª {row['range_pct']*100:.2f}%, å ±é…¬ {row['return_pct']*100:.2f}%")

    logging.info(f"{'='*60}\n")

    # ä¿å­˜ CSV
    csv_path = os.path.join(out_dir, "volatility_stats.csv")
    df.to_csv(csv_path, index=False, encoding='utf-8-sig')
    logging.info(f"âœ… éœ‡ç›ªçµ±è¨ˆå·²ä¿å­˜: {csv_path}")

    # ä¿å­˜ JSON æ‘˜è¦
    summary = {
        "total_samples": int(len(df)),
        "n_stocks": int(df['symbol'].nunique()),
        "n_dates": int(df['date'].nunique()),
        "range_pct": {
            "min": float(range_values.min()),
            "max": float(range_values.max()),
            "mean": float(range_values.mean()),
            "median": float(np.median(range_values)),
            "std": float(range_values.std()),
            "percentiles": {f"P{p}": float(v) for p, v in zip(percentiles, range_percentiles)}
        },
        "return_pct": {
            "mean": float(return_values.mean()),
            "median": float(np.median(return_values)),
            "std": float(return_values.std()),
            "percentiles": {f"P{p}": float(v) for p, v in zip(percentiles, return_percentiles)}
        },
        "threshold_stats": threshold_stats,
        "top_10_volatile": [
            {
                "symbol": str(row['symbol']),
                "date": str(row['date']),
                "range_pct": float(row['range_pct'] * 100),
                "return_pct": float(row['return_pct'] * 100)
            }
            for _, row in top_volatile.iterrows()
        ]
    }

    json_path = os.path.join(out_dir, "volatility_summary.json")
    with open(json_path, "w", encoding="utf-8") as f:
        json.dump(summary, f, ensure_ascii=False, indent=2)

    logging.info(f"âœ… éœ‡ç›ªæ‘˜è¦å·²ä¿å­˜: {json_path}\n")


def zscore_fit(X: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
    """è®¡ç®— Z-Score å‚æ•°ï¼ˆç»§æ‰¿è‡ª V4ï¼‰"""
    mu = X.mean(axis=0)
    sd = X.std(axis=0, ddof=0)
    sd = np.where(sd < 1e-8, 1.0, sd)

    if np.any(np.abs(mu) > 1e6):
        logging.warning(f"ä¾¦æµ‹åˆ°å¼‚å¸¸å¤§çš„å‡å€¼: max|Î¼|={np.max(np.abs(mu)):.2f}")

    return mu, sd


def zscore_apply(X: np.ndarray, mu: np.ndarray, sd: np.ndarray) -> np.ndarray:
    """åº”ç”¨ Z-Score æ­£è§„åŒ–ï¼ˆç»§æ‰¿è‡ª V4ï¼‰"""
    return (X - mu.reshape(1, -1)) / sd.reshape(1, -1)


def extract_date_from_filename(fp: str) -> str:
    """ä»æ¡£åæŠ“å–æ—¥æœŸï¼ˆç»§æ‰¿è‡ª V4ï¼‰"""
    name = os.path.basename(fp)
    m = re.search(r"(20\d{6})", name)
    if m:
        return m.group(1)
    return name


# ============================================================
# V5 æ ¸å¿ƒï¼šæ»‘çª— + Triple-Barrier æ ‡ç­¾ + æ ·æœ¬æƒé‡
# ============================================================

def sliding_windows_v5(
    days_points: List[Tuple[str, str, np.ndarray, np.ndarray]],
    out_dir: str,
    config: Dict[str, Any]
):
    """
    V5 æ»‘çª—æµç¨‹ï¼ˆä¿®æ­£ç‰ˆï¼šæŒ‰æ—¥å¤„ç†ï¼Œé˜²æ­¢è·¨æ—¥æ±¡æŸ“ï¼‰ï¼š
    1. ä»¥ (symbol, date) ä¸ºå•ä½ç‹¬ç«‹å¤„ç†
    2. æ¯æ—¥ç‹¬ç«‹ï¼šæ ‡å‡†åŒ– â†’ æ³¢åŠ¨ç‡ â†’ triple-barrier â†’ æ»‘çª—
    3. æ³¢åŠ¨ç‡çŠ¶æ€æ¯å¤©é‡ç½®
    4. vertical barrier ç¦æ­¢è¶Šæ—¥
    5. æ»‘çª—ä¸å¾—è·¨æ—¥
    6. æ ·æœ¬æ±‡æ€»åæŒ‰è‚¡ç¥¨åˆ‡åˆ† 70/15/15
    7. ã€æ–°å¢ã€‘æ³¢åŠ¨ç‡è¿‡æ»¤ï¼šæ’é™¤æ³¢åŠ¨ç‡è¿‡ä½çš„è‚¡ç¥¨æ—¥
    """
    global global_stats

    if not days_points:
        logging.warning("æ²¡æœ‰èµ„æ–™å¯ä¾›äº§ç”Ÿ .npz æ¡£æ¡ˆ")
        return

    respect_day_boundary = config.get('respect_day_boundary', True)

    # ã€æ–°å¢ã€‘æ³¢åŠ¨ç‡è¿‡æ»¤å‚æ•°
    vol_filter_enabled = config.get('volatility_filter', {}).get('enabled', False)
    min_vol_threshold = config.get('volatility_filter', {}).get('min_daily_vol', 0.0001)  # 0.01%

    logging.info(f"\n{'='*60}")
    logging.info(f"V5 æ»‘çª—æµç¨‹å¼€å§‹ï¼ˆæŒ‰æ—¥å¤„ç†æ¨¡å¼ï¼‰ï¼Œå…± {len(days_points)} ä¸ª symbol-day ç»„åˆ")
    logging.info(f"æ—¥ç•Œçº¿ä¿æŠ¤: {'å¯ç”¨' if respect_day_boundary else 'ç¦ç”¨'}")
    logging.info(f"æ³¢åŠ¨ç‡è¿‡æ»¤: {'å¯ç”¨' if vol_filter_enabled else 'ç¦ç”¨'} (é˜ˆå€¼: {min_vol_threshold*100:.3f}%)")
    logging.info(f"{'='*60}")

    # æ­¥éª¤ 1: é‡ç»„èµ„æ–™ï¼ˆä¿ç•™æ—¥æœŸç»“æ„ï¼‰
    stock_data = defaultdict(lambda: {'day_data': []})  # [(date, X, mids)]

    for date, sym, Xd, mids in days_points:
        stock_data[sym]['day_data'].append((date, Xd, mids))

    logging.info(f"å…± {len(stock_data)} ä¸ªè‚¡ç¥¨æœ‰èµ„æ–™")

    # æ­¥éª¤ 2: å¯¹æ¯ä¸ªè‚¡ç¥¨ï¼ŒæŒ‰æ—¥æœŸæ’åºï¼ˆä½†ä¸åˆå¹¶ï¼‰
    stock_sequences = []

    for sym, data in stock_data.items():
        # æŒ‰æ—¥æœŸæ’åº
        day_data_sorted = sorted(data['day_data'], key=lambda x: x[0])

        # è®¡ç®—æ€»æ—¶é—´ç‚¹
        total_points = sum(Xd.shape[0] for _, Xd, _ in day_data_sorted)

        stock_sequences.append((sym, total_points, day_data_sorted))

    # æŒ‰æ—¶é—´ç‚¹æ•°é‡æ’åºï¼ˆä»…ç”¨äºç»Ÿè®¡ï¼‰
    stock_sequences_sorted = sorted(stock_sequences, key=lambda x: x[1], reverse=True)

    logging.info(f"\nè‚¡ç¥¨æ—¶é—´ç‚¹ç»Ÿè®¡:")
    logging.info(f"  æœ€å¤š: {stock_sequences_sorted[0][1]} ä¸ªç‚¹ ({stock_sequences_sorted[0][0]})")
    logging.info(f"  æœ€å°‘: {stock_sequences_sorted[-1][1]} ä¸ªç‚¹ ({stock_sequences_sorted[-1][0]})")

    # æ­¥éª¤ 3: è¿‡æ»¤å¤ªçŸ­çš„è‚¡ç¥¨åºåˆ—
    MIN_POINTS = SEQ_LEN + 50  # 100 + 50 = 150

    valid_stocks = [s for s in stock_sequences if s[1] >= MIN_POINTS]
    filtered_stocks = len(stock_sequences) - len(valid_stocks)

    if filtered_stocks > 0:
        logging.warning(f"è¿‡æ»¤ {filtered_stocks} æ¡£åºåˆ—å¤ªçŸ­çš„è‚¡ç¥¨ï¼ˆ< {MIN_POINTS} ä¸ªç‚¹ï¼‰")

    if not valid_stocks:
        logging.error("æ²¡æœ‰è‚¡ç¥¨æœ‰è¶³å¤Ÿçš„æ—¶é—´ç‚¹äº§ç”Ÿæ»‘çª—æ ·æœ¬ï¼")
        return

    logging.info(f"æœ‰æ•ˆè‚¡ç¥¨: {len(valid_stocks)} æ¡£")

    # æ­¥éª¤ 3: éšæœºæ‰“ä¹±åæŒ‰è‚¡ç¥¨æ•°é‡åˆ‡åˆ† 70/15/15ï¼ˆé¿å…åˆ†å¸ƒåç§»ï¼‰
    import random
    SPLIT_SEED = config.get('split', {}).get('seed', 42)
    random.Random(SPLIT_SEED).shuffle(valid_stocks)

    logging.info(f"ä½¿ç”¨éšæœºç§å­ {SPLIT_SEED} æ‰“ä¹±è‚¡ç¥¨é¡ºåºï¼ˆé¿å…é€‰æ‹©åå·®ï¼‰")

    n_stocks = len(valid_stocks)
    n_train = max(1, int(n_stocks * config['split']['train_ratio']))
    n_val = max(1, int(n_stocks * config['split']['val_ratio']))

    train_stocks = valid_stocks[:n_train]
    val_stocks = valid_stocks[n_train:n_train + n_val]
    test_stocks = valid_stocks[n_train + n_val:]

    logging.info(f"\nèµ„æ–™åˆ‡åˆ†ï¼ˆæŒ‰è‚¡ç¥¨æ•°ï¼‰:")
    logging.info(f"  Train: {len(train_stocks)} æ¡£è‚¡ç¥¨")
    logging.info(f"  Val:   {len(val_stocks)} æ¡£è‚¡ç¥¨")
    logging.info(f"  Test:  {len(test_stocks)} æ¡£è‚¡ç¥¨")

    splits = {
        'train': train_stocks,
        'val': val_stocks,
        'test': test_stocks
    }

    # æ­¥éª¤ 4: è®¡ç®—è®­ç»ƒé›†çš„ Z-Score å‚æ•°ï¼ˆåŸºäºæ‰€æœ‰è®­ç»ƒé›†æ—¥æœŸï¼‰
    logging.info(f"\n{'='*60}")
    logging.info("è®¡ç®— Z-Score å‚æ•°ï¼ˆåŸºäºè®­ç»ƒé›†ï¼‰")
    logging.info(f"{'='*60}")

    train_X_list = []
    for sym, n_points, day_data_sorted in train_stocks:
        for date, Xd, mids in day_data_sorted:
            train_X_list.append(Xd)

    Xtr = np.concatenate(train_X_list, axis=0) if train_X_list else np.zeros((0, 20))

    if Xtr.size == 0:
        mu = np.zeros((20,), dtype=np.float64)
        sd = np.ones((20,), dtype=np.float64)
        logging.warning("è®­ç»ƒé›†ä¸ºç©ºï¼Œä½¿ç”¨é¢„è®¾ Z-Score å‚æ•°")
    else:
        mu, sd = zscore_fit(Xtr)
        logging.info(f"è®­ç»ƒé›†å¤§å°: {Xtr.shape[0]:,} ä¸ªæ—¶é—´ç‚¹")

    # æ­¥éª¤ 5: äº§ç”Ÿæ»‘çª—æ ·æœ¬ï¼ˆV5 æ ¸å¿ƒï¼šæŒ‰æ—¥å¤„ç†ï¼‰
    def build_split_v5(split_name):
        """å¯¹ä¸€ä¸ª split äº§ç”Ÿ V5 æ»‘çª—æ ·æœ¬ï¼ˆæŒ‰æ—¥ç‹¬ç«‹å¤„ç†ï¼‰"""
        stock_list = splits[split_name]

        logging.info(f"\n{'='*60}")
        logging.info(f"å¤„ç† {split_name.upper()} é›†ï¼Œå…± {len(stock_list)} æ¡£è‚¡ç¥¨")
        logging.info(f"{'='*60}")

        X_windows = []
        y_labels = []
        weights = []
        stock_ids = []

        total_windows = 0
        total_days = 0
        cross_day_filtered = 0
        tb_stats = {"up": 0, "down": 0, "time": 0}
        vol_filtered_days = 0  # æ–°å¢ï¼šè®°å½•å› æ³¢åŠ¨ç‡è¿‡æ»¤è€Œç§»é™¤çš„å¤©æ•°

        for sym, n_points, day_data_sorted in stock_list:
            stock_windows = 0

            # æ ¸å¿ƒä¿®æ­£ï¼šé€æ—¥ç‹¬ç«‹å¤„ç†
            for date, Xd, mids in day_data_sorted:
                total_days += 1

                # 1. Z-score æ­£è§„åŒ–ï¼ˆæ¯æ—¥ç‹¬ç«‹ï¼‰
                Xn = zscore_apply(Xd, mu, sd)

                # 2. æ„å»º DataFrame ç”¨äº V5 æ ‡ç­¾ç”Ÿæˆ
                close = pd.Series(mids, name='close')

                # 3. è®¡ç®—æ³¢åŠ¨ç‡ï¼ˆæ¯æ—¥é‡ç½®çŠ¶æ€ï¼‰
                vol_method = config['volatility']['method']

                try:
                    if vol_method == 'ewma':
                        vol = ewma_vol(close, halflife=config['volatility']['halflife'])
                    elif vol_method == 'garch':
                        vol = garch11_vol(close)
                    else:
                        raise ValueError(f"ä¸æ”¯æ´çš„æ³¢åŠ¨ç‡æ–¹æ³•: {vol_method}")
                except Exception as e:
                    logging.warning(f"  {sym} @ {date}: æ³¢åŠ¨ç‡è®¡ç®—å¤±è´¥ ({e})ï¼Œè·³è¿‡")
                    continue

                # 4. ç”Ÿæˆ Triple-Barrier æ ‡ç­¾ï¼ˆé™åˆ¶åœ¨å½“æ—¥å†…ï¼‰
                tb_cfg = config['triple_barrier']
                day_end_idx = len(close) - 1 if respect_day_boundary else None

                try:
                    tb_df = tb_labels(
                        close=close,
                        vol=vol,
                        pt_mult=tb_cfg['pt_multiplier'],
                        sl_mult=tb_cfg['sl_multiplier'],
                        max_holding=tb_cfg['max_holding'],
                        min_return=tb_cfg['min_return'],
                        day_end_idx=day_end_idx
                    )
                except Exception as e:
                    logging.warning(f"  {sym} @ {date}: Triple-Barrier å¤±è´¥ ({e})ï¼Œè·³è¿‡")
                    continue

                # 5. è½¬æ¢æ ‡ç­¾ {-1,0,1} â†’ {0,1,2}
                y_tb = tb_df["y"].map({-1: 0, 0: 1, 1: 2})

                # 6. è®¡ç®—æ ·æœ¬æƒé‡
                if config['sample_weights']['enabled']:
                    w = make_sample_weight(
                        ret=tb_df["ret"],
                        tt=tb_df["tt"],
                        y=y_tb,
                        tau=config['sample_weights']['tau'],
                        scale=config['sample_weights']['return_scaling'],
                        balance=config['sample_weights']['balance_classes'],
                        use_log_scale=config['sample_weights'].get('use_log_scale', True)  # ğŸ”§ æ”¹é»˜èªå€¼ç‚º True
                    )
                else:
                    w = pd.Series(np.ones(len(y_tb)), index=y_tb.index)

                # 7. ç»Ÿè®¡è§¦å‘åŸå› 
                for reason in tb_df["why"].value_counts().items():
                    if reason[0] in tb_stats:
                        tb_stats[reason[0]] += reason[1]

                # 8. äº§ç”Ÿæ»‘çª—æ ·æœ¬ï¼ˆç¦æ­¢è·¨æ—¥ï¼‰
                T = Xn.shape[0]
                max_t = min(T, len(y_tb))

                if max_t < SEQ_LEN:
                    continue

                day_windows = 0

                for t in range(SEQ_LEN - 1, max_t):
                    # æ£€æŸ¥æ»‘çª—æ˜¯å¦è·¨æ—¥ï¼ˆå¦‚æœå¯ç”¨æ—¥ç•Œçº¿ä¿æŠ¤ï¼‰
                    window_start = t - SEQ_LEN + 1

                    if respect_day_boundary and window_start < 0:
                        cross_day_filtered += 1
                        continue

                    window = Xn[window_start:t + 1, :]

                    if window.shape[0] != SEQ_LEN:
                        continue

                    label = int(y_tb.iloc[t])
                    weight = float(w.iloc[t])

                    if label not in [0, 1, 2]:
                        continue

                    X_windows.append(window.astype(np.float32))
                    y_labels.append(label)
                    weights.append(weight)
                    stock_ids.append(sym)
                    day_windows += 1

                stock_windows += day_windows

            if stock_windows > 0:
                logging.info(f"  {sym}: {stock_windows:,} ä¸ªæ ·æœ¬ (å…± {n_points} ä¸ªç‚¹ï¼Œ{len(day_data_sorted)} å¤©)")

            # ã€æ–°å¢ã€‘ç»Ÿè®¡å› æ³¢åŠ¨ç‡è¿‡æ»¤è€Œç§»é™¤çš„å¤©æ•°
            if vol_filter_enabled and stock_windows == 0:
                vol_filtered_days += 1

        logging.info(f"\n{split_name.upper()} æ€»è®¡: {total_windows:,} ä¸ªæ ·æœ¬ (æ¥è‡ª {total_days} ä¸ª symbol-day)")
        logging.info(f"è§¦å‘åŸå› åˆ†å¸ƒ: {tb_stats}")

        if respect_day_boundary and cross_day_filtered > 0:
            logging.info(f"è·¨æ—¥è¿‡æ»¤: {cross_day_filtered} ä¸ªæ»‘çª—è¢«ç§»é™¤")

        # ã€æ–°å¢ã€‘æ³¢åŠ¨ç‡è¿‡æ»¤ç»Ÿè®¡
        if vol_filter_enabled and vol_filtered_days > 0:
            logging.info(f"æ³¢åŠ¨ç‡è¿‡æ»¤: {vol_filtered_days} ä¸ªè‚¡ç¥¨æ—¥è¢«ç§»é™¤ ({vol_filtered_days/total_days*100:.1f}%)")

        global_stats["valid_windows"] += total_windows

        if X_windows:
            X_array = np.stack(X_windows, axis=0)
            y_array = np.array(y_labels, dtype=np.int64)
            w_array = np.array(weights, dtype=np.float32)
            sid_array = np.array(stock_ids, dtype=object)

            # ç»Ÿè®¡èµ„è®¯
            unique_stocks = len(np.unique(sid_array))
            label_dist = np.bincount(y_array, minlength=3)
            label_pct = label_dist / label_dist.sum() * 100 if label_dist.sum() > 0 else np.zeros(3)

            logging.info(f"  å½¢çŠ¶: X={X_array.shape}, y={y_array.shape}, w={w_array.shape}")
            logging.info(f"  æ¶µç›–è‚¡ç¥¨: {unique_stocks} æ¡£")
            logging.info(f"  æ ‡ç­¾åˆ†å¸ƒ: ä¸‹è·Œ={label_dist[0]} ({label_pct[0]:.1f}%), "
                        f"æŒå¹³={label_dist[1]} ({label_pct[1]:.1f}%), "
                        f"ä¸Šæ¶¨={label_dist[2]} ({label_pct[2]:.1f}%)")
            logging.info(f"  æƒé‡ç»Ÿè®¡: mean={w_array.mean():.3f}, std={w_array.std():.3f}, max={w_array.max():.3f}")

            return X_array, y_array, w_array, sid_array
        else:
            logging.warning(f"{split_name} é›†æ²¡æœ‰äº§ç”Ÿä»»ä½•æ ·æœ¬ï¼")
            return (
                np.zeros((0, SEQ_LEN, 20), dtype=np.float32),
                np.zeros((0,), dtype=np.int64),
                np.zeros((0,), dtype=np.float32),
                np.array([], dtype=object)
            )

    # æ­¥éª¤ 6: äº§ç”Ÿå¹¶ä¿å­˜ npz æ¡£æ¡ˆ
    os.makedirs(out_dir, exist_ok=True)

    results = {}

    for split in ["train", "val", "test"]:
        X, y, w, sid = build_split_v5(split)

        npz_path = os.path.join(out_dir, f"stock_embedding_{split}.npz")
        np.savez_compressed(npz_path, X=X, y=y, weights=w, stock_ids=sid)

        logging.info(f"\nâœ… å·²ä¿å­˜: {npz_path}")

        results[split] = {
            "samples": len(y),
            "label_dist": np.bincount(y, minlength=3).tolist() if len(y) > 0 else [0, 0, 0],
            "weight_stats": {
                "mean": float(w.mean()) if len(w) > 0 else 0.0,
                "std": float(w.std()) if len(w) > 0 else 0.0,
                "max": float(w.max()) if len(w) > 0 else 0.0
            }
        }

    # æ­¥éª¤ 7: å†™å…¥ metadata
    meta = {
        "format": "deeplob_v5_pro",
        "version": VERSION,
        "creation_date": datetime.now().isoformat(),
        "seq_len": SEQ_LEN,
        "feature_dim": 20,

        "volatility": {
            "method": config['volatility']['method'],
            "halflife": config['volatility'].get('halflife', 60)
        },

        "triple_barrier": config['triple_barrier'],

        "sample_weights": {
            "enabled": config['sample_weights']['enabled'],
            "tau": config['sample_weights']['tau'],
            "return_scaling": config['sample_weights']['return_scaling'],
            "balance_classes": config['sample_weights']['balance_classes'],
            "use_log_scale": config['sample_weights'].get('use_log_scale', False)
        },

        "normalization": {
            "method": "zscore",
            "computed_on": "train_set",
            "feature_means": mu.tolist(),
            "feature_stds": sd.tolist()
        },

        "data_quality": global_stats,

        "data_split": {
            "method": "by_stock_count",
            "train_stocks": len(train_stocks),
            "val_stocks": len(val_stocks),
            "test_stocks": len(test_stocks),
            "total_stocks": len(valid_stocks),
            "filtered_stocks": filtered_stocks,
            "results": results
        },

        "note": "V5 Pro: Triple-Barrier labels + Sample weights. Labels: {0:ä¸‹è·Œ, 1:æŒå¹³, 2:ä¸Šæ¶¨}",
    }

    meta_path = os.path.join(out_dir, "normalization_meta.json")
    with open(meta_path, "w", encoding="utf-8") as f:
        json.dump(meta, f, ensure_ascii=False, indent=2)

    logging.info(f"\nâœ… Metadata å·²ä¿å­˜: {meta_path}")
    logging.info(f"\n{'='*60}")
    logging.info("NPZ æ¡£æ¡ˆäº§ç”Ÿå®Œæˆï¼")
    logging.info(f"{'='*60}")


# ============================================================
# ä¸»ç¨‹å¼
# ============================================================

def main():
    """ä¸»ç¨‹å¼"""
    try:
        args = parse_args()
        in_dir = args.input_dir
        out_dir = args.output_dir

        # è½½å…¥é…ç½®
        config = load_config(args.config)

        # éªŒè¯è¾“å…¥ç›®å½•å­˜åœ¨
        if not os.path.exists(in_dir):
            logging.error(f"è¾“å…¥ç›®å½•ä¸å­˜åœ¨: {in_dir}")
            return 1

        # å»ºç«‹è¾“å‡ºç›®å½•
        try:
            os.makedirs(out_dir, exist_ok=True)
        except Exception as e:
            logging.error(f"æ— æ³•å»ºç«‹è¾“å‡ºç›®å½• {out_dir}: {e}")
            return 1

        logging.info(f"{'='*60}")
        logging.info(f"V5 Pro èµ„æ–™æµæ°´çº¿å¯åŠ¨")
        logging.info(f"{'='*60}")
        logging.info(f"è¾“å…¥ç›®å½•: {in_dir}")
        logging.info(f"è¾“å‡ºç›®å½•: {out_dir}")
        logging.info(f"é…ç½®ç‰ˆæœ¬: {config['version']}")
        logging.info(f"æ³¢åŠ¨ç‡æ–¹æ³•: {config['volatility']['method']}")
        logging.info(f"Triple-Barrier: PT={config['triple_barrier']['pt_multiplier']}Ïƒ, "
                    f"SL={config['triple_barrier']['sl_multiplier']}Ïƒ, "
                    f"MaxHold={config['triple_barrier']['max_holding']}")
        logging.info(f"{'='*60}\n")

        # è¯»å–æ‰€æœ‰ .txt æ¡£
        files = sorted(glob.glob(os.path.join(in_dir, "*.txt")))
        if not files:
            logging.error(f"åœ¨ {in_dir} æ‰¾ä¸åˆ° .txt æ¡£æ¡ˆ")
            return 1

        logging.info(f"æ‰¾åˆ° {len(files)} ä¸ªæ¡£æ¡ˆå¾…å¤„ç†")

        # å…ˆæŒ‰æ¡£åæ—¥æœŸåˆ†ç»„
        day_map: Dict[str, List[str]] = defaultdict(list)
        for fp in files:
            day = extract_date_from_filename(fp)
            day_map[day].append(fp)

        # é€æ—¥é€æ¡£æ¡ˆè¯»å– â†’ é€ symbol æ±‡æ•´
        per_day_symbol_points = []  # [(date, symbol, X_points, mids)]
        day_keys = sorted(day_map.keys())

        logging.info(f"å¼€å§‹å¤„ç† {len(day_keys)} ä¸ªäº¤æ˜“æ—¥çš„èµ„æ–™")

        for day in day_keys:
            fps = sorted(day_map[day])
            logging.info(f"å¤„ç†æ—¥æœŸ {day}ï¼Œå…± {len(fps)} ä¸ªæ¡£æ¡ˆ")

            # è¯»æœ¬æ—¥æ‰€æœ‰è¡Œ
            per_symbol_raw: Dict[str, List[Tuple[int, Dict[str,Any]]]] = defaultdict(list)

            for fp in fps:
                try:
                    with open(fp, "r", encoding="utf-8", errors="ignore") as f:
                        for raw in f:
                            sym, t, rec = parse_line(raw)
                            if rec is None or sym == "" or t < 0:
                                continue
                            per_symbol_raw[sym].append((t, rec))
                except Exception as e:
                    logging.warning(f"è¯»å–æ¡£æ¡ˆ {fp} æ—¶å‘ç”Ÿé”™è¯¯: {e}")
                    continue

            # å¯¹æ¯ä¸ª symbolï¼šå»é‡ã€èšåˆã€ä¿å­˜ä¸­é—´ä»·
            for sym, rows in per_symbol_raw.items():
                if not rows:
                    continue

                # æ—¶é—´æ’åº
                rows.sort(key=lambda x: x[0])

                # å»é‡
                rows = dedup_by_timestamp_keep_last(rows)
                if not rows:
                    continue

                # 10 äº‹ä»¶èšåˆ
                Xp, mids = aggregate_chunks_of_10(rows)
                if Xp.shape[0] == 0:
                    continue

                # V5: è¨ˆç®—ä¸¦ä¿å­˜éœ‡ç›ªçµ±è¨ˆ
                vol_stats = calculate_intraday_volatility(mids, day, sym)
                if vol_stats is not None:
                    global_stats["volatility_stats"].append(vol_stats)

                    # V5: éœ‡ç›ªç¯©é¸ï¼ˆå¦‚æœå•Ÿç”¨ï¼‰
                    if config.get('intraday_volatility_filter', {}).get('enabled', False):
                        min_range = config['intraday_volatility_filter'].get('min_range_pct', 0.0)
                        max_range = config['intraday_volatility_filter'].get('max_range_pct', 1.0)

                        range_pct = vol_stats['range_pct']

                        if range_pct < min_range:
                            logging.debug(f"  {sym} @ {day}: éœ‡ç›ªéå° ({range_pct*100:.2f}% < {min_range*100:.1f}%)ï¼Œè·³é")
                            global_stats["volatility_filtered"] += 1
                            continue

                        if range_pct > max_range:
                            logging.debug(f"  {sym} @ {day}: éœ‡ç›ªéå¤§ ({range_pct*100:.2f}% > {max_range*100:.1f}%)ï¼Œè·³é")
                            global_stats["volatility_filtered"] += 1
                            continue

                # V5: ä¿å­˜ midsï¼ˆç”¨äºåç»­æ ‡ç­¾ç”Ÿæˆï¼‰
                per_day_symbol_points.append((day, sym, Xp, mids))

        # è‹¥æ²¡æœ‰å¯ç”¨èµ„æ–™
        if not per_day_symbol_points:
            logging.error("æ¸…æ´—æˆ–èšåˆåæ²¡æœ‰å¯ç”¨èµ„æ–™")
            return 1

        logging.info(f"å…±å¤„ç† {len(per_day_symbol_points)} ä¸ª symbol-day ç»„åˆ")

        # ç”¢ç”Ÿéœ‡ç›ªçµ±è¨ˆå ±å‘Š
        if global_stats["volatility_stats"]:
            generate_volatility_report(global_stats["volatility_stats"], out_dir)

        # äº§å‡º 70/15/15 çš„ .npzï¼ˆV5 æ»‘çª—æµç¨‹ï¼‰
        if args.stats_only:
            logging.info("\n" + "="*60)
            logging.info("âš¡ å¿«é€Ÿæ¨¡å¼ï¼šå·²å®Œæˆéœ‡ç›ªçµ±è¨ˆï¼Œè·³éè¨“ç·´æ•¸æ“šç”Ÿæˆ")
            logging.info("="*60)
            logging.info("å¦‚éœ€ç”Ÿæˆè¨“ç·´æ•¸æ“šï¼Œè«‹ç§»é™¤ --stats-only åƒæ•¸")
            logging.info("="*60 + "\n")
        elif args.make_npz:
            logging.info("å¼€å§‹äº§ç”Ÿ V5 .npz æ¡£æ¡ˆ")
            sliding_windows_v5(
                per_day_symbol_points,
                os.path.join(out_dir, "npz"),
                config
            )

        logging.info(f"\n{'='*60}")
        if args.stats_only:
            logging.info(f"[å®Œæˆ] éœ‡ç›ªçµ±è¨ˆæˆåŠŸï¼Œè¼¸å‡ºè³‡æ–™å¤¾: {out_dir}")
        else:
            logging.info(f"[å®Œæˆ] V5 è½¬æ¢æˆåŠŸï¼Œè¾“å‡ºèµ„æ–™å¤¹: {out_dir}")
        logging.info(f"{'='*60}")
        logging.info(f"ç»Ÿè®¡èµ„æ–™:")
        logging.info(f"  åŸå§‹äº‹ä»¶æ•°: {global_stats['total_raw_events']:,}")
        logging.info(f"  æ¸…æ´—å: {global_stats['cleaned_events']:,}")
        logging.info(f"  èšåˆåæ—¶é—´ç‚¹: {global_stats['aggregated_points']:,}")
        if not args.stats_only:
            logging.info(f"  æœ‰æ•ˆçª—å£: {global_stats['valid_windows']:,}")
            logging.info(f"  Triple-Barrier æˆåŠŸ: {global_stats['tb_success']:,}")
        logging.info(f"  éœ‡ç›ªçµ±è¨ˆæ¨£æœ¬: {len(global_stats['volatility_stats']):,}")
        logging.info(f"{'='*60}\n")

        return 0

    except Exception as e:
        logging.error(f"ç¨‹å¼æ‰§è¡Œå¤±è´¥: {e}")
        traceback.print_exc()
        return 1


if __name__ == "__main__":
    import sys
    sys.exit(main())
