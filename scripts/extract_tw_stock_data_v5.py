# -*- coding: utf-8 -*-
"""
extract_tw_stock_data_v5.py - V5 Pro ä¸“ä¸šåŒ–èµ„æ–™æµæ°´çº¿
=============================================================================
ã€æ›´æ–°æ—¥æœŸã€‘2025-10-18
ã€ç‰ˆæœ¬è¯´æ˜ã€‘v5.0 - ä¸“ä¸šçº§æ ‡ç­¾ä¸æ³¢åŠ¨ç‡ï¼ˆtriple-barrier + archï¼‰

åŸºäºã€ŠV5_Pro_NoMLFinLab_Guide.mdã€‹å®ç°ï¼Œå°† V4 çš„å›ºå®š k æ­¥æ ‡ç­¾å…¨é¢å‡çº§ä¸ºï¼š
  1) ä¸“ä¸šæ³¢åŠ¨ç‡ä¼°è®¡ï¼ˆEWMA / Yang-Zhang / GARCHï¼‰
  2) Triple-Barrier äº‹ä»¶æ‰“æ ‡ï¼ˆæ­¢ç›ˆ/æ­¢æŸ/åˆ°æœŸï¼‰
  3) æ ·æœ¬æƒé‡ï¼ˆæ”¶ç›Š Ã— æ—¶é—´è¡°å‡ Ã— ç±»åˆ«å¹³è¡¡ï¼‰

=============================================================================
æ ¸å¿ƒæ”¹è¿›ï¼ˆV4 â†’ V5ï¼‰
=============================================================================
âœ… æ³¢åŠ¨ç‡ä¼°è®¡ï¼š
  - V4: ç®€å•ç›¸å¯¹æ³¢åŠ¨ç‡ std(mid) / mean(mid)
  - V5: arch åº“ä¸“ä¸šä¼°è®¡ï¼ˆEWMA / Yang-Zhang / GARCHï¼‰

âœ… æ ‡ç­¾ç”Ÿæˆï¼š
  - V4: å›ºå®š k æ­¥ä»·æ ¼å˜åŠ¨ï¼ˆalpha=0.002 é˜ˆå€¼ï¼‰
  - V5: Triple-Barrier è‡ªé€‚åº”æ ‡ç­¾ï¼ˆåŸºäºæ³¢åŠ¨ç‡å€æ•°ï¼‰

âœ… æ ·æœ¬æƒé‡ï¼š
  - V4: æ— æƒé‡ï¼ˆæˆ–ç®€å•è¿‡æ»¤ï¼‰
  - V5: æ”¶ç›ŠåŠ æƒ + æ—¶é—´è¡°å‡ + ç±»åˆ«å¹³è¡¡

âœ… è¾“å‡ºæ ¼å¼ï¼š
  - V4: X (N,100,20), y {0,1,2}
  - V5: X (N,100,20), y {0,1,2}, w (N,) + è¯¦ç»† metadata

=============================================================================
ä½¿ç”¨æ–¹å¼
=============================================================================

ã€åŸºæœ¬ä½¿ç”¨ã€‘ï¼ˆä½¿ç”¨é¢„è®¾é…ç½®ï¼‰ï¼š
  python scripts/extract_tw_stock_data_v5.py \
      --input-dir ./data/temp \
      --output-dir ./data/processed_v5 \
      --config configs/config_pro_v5.yaml

ã€è¾“å‡ºç»“æœã€‘ï¼š
  1. NPZ æ–‡ä»¶ï¼ˆæ–°å¢æ ·æœ¬æƒé‡ï¼‰ï¼š
     - ./data/processed_v5/npz/stock_embedding_train.npz
       å†…å«ï¼šX, y, stock_ids, weights (æ–°å¢)
     - ./data/processed_v5/npz/stock_embedding_val.npz
     - ./data/processed_v5/npz/stock_embedding_test.npz

  2. Metadataï¼ˆåŒ…å«å®Œæ•´ V5 é…ç½®ï¼‰ï¼š
     - ./data/processed_v5/npz/normalization_meta.json
       {
         "version": "5.0.0",
         "volatility_method": "ewma",
         "triple_barrier": {...},
         "sample_weights": {...},
         "label_distribution": {...}
       }

=============================================================================
å›ºå®šè®¾å®šï¼ˆç»§æ‰¿è‡ª V4ï¼‰
=============================================================================
- NoAuctionï¼šç§»é™¤ IsTrialMatch=='1'ï¼Œå¹¶é™åˆ¶ 09:00:00â€“13:30:00
- 10 äº‹ä»¶èšåˆï¼ˆä¸é‡å ï¼‰ï¼šä»·æ ¼/æ•°é‡çš†å–è§†çª—æœ«ç«¯
- æ ‡å‡†åŒ–ï¼šZ-Scoreï¼ˆåŸºäºè®­ç»ƒé›†ï¼‰
- æ—¶é—´åºåˆ—çª—å£ï¼š100 timesteps
- è¾“å‡ºç»´åº¦ï¼š20 ç»´ LOB ç‰¹å¾ï¼ˆ5 æ¡£ LOBï¼‰

=============================================================================
ä¾èµ–å¥—ä»¶ï¼ˆæ–°å¢ï¼‰
=============================================================================
pip install triple-barrier arch ruamel.yaml pandas scikit-learn

ã€å¯é€‰ã€‘ï¼š
pip install skfolio vectorbt nautilus-trader  # ç”¨äºåç»­ CV/å›æµ‹/æ¡å½¢èšåˆ

ç‰ˆæœ¬ï¼šv5.0
æ›´æ–°ï¼š2025-10-18
"""

import os
import re
import json
import argparse
import glob
import logging
import traceback
import warnings
from collections import defaultdict
from datetime import datetime
from typing import List, Tuple, Dict, Any, Optional

import numpy as np
import pandas as pd
import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° Python è·¯å¾„
project_root = Path(__file__).parent.parent
if str(project_root) not in sys.path:
    sys.path.insert(0, str(project_root))

from src.utils.yaml_manager import YAMLManager

# è®¾å®šç‰ˆæœ¬å·
VERSION = "5.0.0"

# è®¾å®šæ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)

# å¿½ç•¥éƒ¨åˆ†ç¬¬ä¸‰æ–¹åº“è­¦å‘Š
warnings.filterwarnings('ignore', category=FutureWarning)
warnings.filterwarnings('ignore', category=RuntimeWarning)

# å›ºå®šå¸¸æ•°ï¼ˆç»§æ‰¿è‡ª V4ï¼‰
AGG_FACTOR = 10
SEQ_LEN = 100
TRADING_START = 90000   # 09:00:00
TRADING_END   = 133000  # 13:30:00

# æ¬„ä½ç´¢å¼•ï¼ˆè¼¸å…¥ï¼‰
IDX_REF = 3
IDX_UPPER = 4
IDX_LOWER = 5
IDX_LASTPRICE = 9
IDX_LASTVOL = 10
IDX_TV = 11
IDX_TIME = 32
IDX_TRIAL = 33

# äº”æª”åƒ¹é‡ç´¢å¼•å°æ‡‰
BID_P_IDX = [12, 14, 16, 18, 20]
BID_Q_IDX = [13, 15, 17, 19, 21]
ASK_P_IDX = [22, 24, 26, 28, 30]
ASK_Q_IDX = [23, 25, 27, 29, 31]

# å…¨åŸŸçµ±è¨ˆè®Šæ•¸
global_stats = {
    "total_raw_events": 0,
    "cleaned_events": 0,
    "aggregated_points": 0,
    "valid_windows": 0,
    "tb_success": 0,
    "volatility_stats": [],  # æ–°å¢ï¼šå„²å­˜æ¯å€‹ symbol-day çš„éœ‡ç›ªçµ±è¨ˆ
    "volatility_filtered": 0  # æ–°å¢ï¼šè¢«éœ‡ç›ªç¯©é¸éæ¿¾çš„æ¨£æœ¬æ•¸
}


# ============================================================
# V5 æ ¸å¿ƒæ¨¡å—ï¼šä¸“ä¸šæ³¢åŠ¨ç‡ä¼°è®¡ï¼ˆarchï¼‰
# ============================================================

def ewma_vol(close: pd.Series, halflife: int = 60) -> pd.Series:
    """
    EWMA æ³¢åŠ¨ç‡ä¼°è®¡

    Args:
        close: æ”¶ç›˜ä»·åºåˆ—
        halflife: EWMA åŠè¡°æœŸï¼ˆbarsï¼‰

    Returns:
        vol: æ³¢åŠ¨ç‡åºåˆ—
    """
    ret = np.log(close).diff()
    var = ret.ewm(halflife=halflife, adjust=False).var()
    vol = np.sqrt(var).bfill()
    return vol


def yang_zhang_vol(ohlc: pd.DataFrame, window: int = 60) -> pd.Series:
    """
    Yang-Zhang æ³¢åŠ¨ç‡ä¼°è®¡ï¼ˆåˆ©ç”¨ OHLCï¼‰

    Args:
        ohlc: åŒ…å« open, high, low, close çš„ DataFrame
        window: æ»šåŠ¨çª—å£å¤§å°

    Returns:
        vol: Yang-Zhang æ³¢åŠ¨ç‡åºåˆ—
    """
    o = ohlc['open'].astype(float)
    h = ohlc['high'].astype(float)
    l = ohlc['low'].astype(float)
    c = ohlc['close'].astype(float)

    k = 0.34 / (1.34 + (window + 1) / (window - 1))

    log_oc = np.log(o / c.shift(1))
    log_co = np.log(c / o)
    rs = (np.log(h / l)) ** 2

    sigma_o = log_oc.rolling(window).var()
    sigma_c = log_co.rolling(window).var()
    sigma_rs = rs.rolling(window).mean()

    yz = sigma_o + k * sigma_c + (1 - k) * sigma_rs
    vol = np.sqrt(yz).replace([np.inf, -np.inf], np.nan).bfill()

    return vol


def garch11_vol(close: pd.Series) -> pd.Series:
    """
    GARCH(1,1) æ³¢åŠ¨ç‡ä¼°è®¡

    Args:
        close: æ”¶ç›˜ä»·åºåˆ—

    Returns:
        vol: GARCH æ³¢åŠ¨ç‡åºåˆ—
    """
    try:
        from arch import arch_model

        ret = 100 * np.log(close).diff().dropna()

        if len(ret) < 50:
            logging.warning("GARCH: èµ„æ–™ç‚¹ä¸è¶³ï¼Œå›é€€åˆ° EWMA")
            return ewma_vol(close, halflife=60)

        am = arch_model(ret, vol='GARCH', p=1, q=1, dist='normal')
        res = am.fit(disp='off', show_warning=False)
        fcast = res.forecast(horizon=1, reindex=True).variance
        vol = np.sqrt(fcast.squeeze()) / 100.0

        return vol.reindex(close.index).bfill()

    except Exception as e:
        logging.warning(f"GARCH å¤±è´¥: {e}ï¼Œå›é€€åˆ° EWMA")
        return ewma_vol(close, halflife=60)


# ============================================================
# V5 æ ¸å¿ƒæ¨¡å—ï¼šTriple-Barrier æ ‡ç­¾ç”Ÿæˆ
# ============================================================

def tb_labels(close: pd.Series,
              vol: pd.Series,
              pt_mult: float = 2.0,
              sl_mult: float = 2.0,
              max_holding: int = 200,
              min_return: float = 0.0001) -> pd.DataFrame:
    """
    Triple-Barrier æ ‡ç­¾ç”Ÿæˆï¼ˆè‡ªå®šä¹‰å®ç°ï¼‰

    Args:
        close: æ”¶ç›˜ä»·åºåˆ—
        vol: æ³¢åŠ¨ç‡åºåˆ—
        pt_mult: æ­¢ç›ˆå€æ•°
        sl_mult: æ­¢æŸå€æ•°
        max_holding: æœ€å¤§æŒæœ‰æœŸï¼ˆbarsï¼‰
        min_return: æœ€å°æŠ¥é…¬é˜ˆå€¼

    Returns:
        DataFrame åŒ…å«:
            - y: {-1, 0, 1} æ ‡ç­¾
            - ret: å®é™…æ”¶ç›Š
            - tt: è§¦å‘æ—¶é—´æ­¥æ•°
            - why: è§¦å‘åŸå›  {'up', 'down', 'time'}
            - up_p: æ­¢ç›ˆä»·æ ¼
            - dn_p: æ­¢æŸä»·æ ¼
    """
    try:
        n = len(close)
        results = []

        for i in range(n - 1):
            entry_price = close.iloc[i]
            entry_vol = vol.iloc[i]

            # è®¡ç®—æ­¢ç›ˆæ­¢æŸä»·æ ¼
            up_barrier = entry_price * (1 + pt_mult * entry_vol)
            dn_barrier = entry_price * (1 - sl_mult * entry_vol)

            # æŸ¥æ‰¾è§¦å‘ç‚¹
            end_idx = min(i + max_holding, n)
            triggered = False
            trigger_idx = end_idx - 1
            trigger_why = 'time'

            for j in range(i + 1, end_idx):
                future_price = close.iloc[j]

                # æ£€æŸ¥æ˜¯å¦è§¦åŠæ­¢ç›ˆ
                if future_price >= up_barrier:
                    trigger_idx = j
                    trigger_why = 'up'
                    triggered = True
                    break

                # æ£€æŸ¥æ˜¯å¦è§¦åŠæ­¢æŸ
                if future_price <= dn_barrier:
                    trigger_idx = j
                    trigger_why = 'down'
                    triggered = True
                    break

            # è®¡ç®—å®é™…æ”¶ç›Š
            exit_price = close.iloc[trigger_idx]
            ret = (exit_price - entry_price) / entry_price

            # åº”ç”¨æœ€å°æŠ¥é…¬é˜ˆå€¼ï¼ˆæ ¸å¿ƒä¿®æ­£ï¼šæ‰€æœ‰æ ·æœ¬éƒ½è¦æ£€æŸ¥é˜ˆå€¼ï¼‰
            if np.abs(ret) < min_return:
                label = 0  # æŒå¹³ï¼ˆæ”¶ç›Šå¤ªå°ï¼Œè§†ä¸ºæ— è¶‹åŠ¿ï¼‰
            else:
                label = int(np.sign(ret))  # -1ï¼ˆä¸‹è·Œï¼‰æˆ– 1ï¼ˆä¸Šæ¶¨ï¼‰

            results.append({
                'ret': ret,
                'y': label,
                'tt': trigger_idx - i,
                'why': trigger_why,
                'up_p': up_barrier,
                'dn_p': dn_barrier
            })

        # ä¸ºæœ€åä¸€ä¸ªç‚¹æ·»åŠ é»˜è®¤å€¼
        if n > 0:
            results.append({
                'ret': 0.0,
                'y': 0,
                'tt': 0,
                'why': 'time',
                'up_p': close.iloc[-1],
                'dn_p': close.iloc[-1]
            })

        out = pd.DataFrame(results, index=close.index)

        global_stats["tb_success"] += 1

        return out.dropna()

    except Exception as e:
        logging.error(f"Triple-Barrier å¤±è´¥: {e}")
        raise


# ============================================================
# V5 æ ¸å¿ƒæ¨¡å—ï¼šæ ·æœ¬æƒé‡è®¡ç®—
# ============================================================

def make_sample_weight(ret: pd.Series,
                      tt: pd.Series,
                      y: pd.Series,
                      tau: float = 100.0,
                      scale: float = 10.0,
                      balance: bool = True) -> pd.Series:
    """
    æ ·æœ¬æƒé‡è®¡ç®—ï¼ˆæ”¶ç›Š Ã— æ—¶é—´è¡°å‡ Ã— ç±»åˆ«å¹³è¡¡ï¼‰

    Args:
        ret: å®é™…æ”¶ç›Šåºåˆ—
        tt: è§¦å‘æ—¶é—´æ­¥æ•°
        y: æ ‡ç­¾åºåˆ—
        tau: æ—¶é—´è¡°å‡å‚æ•°
        scale: æ”¶ç›Šç¼©æ”¾ç³»æ•°
        balance: æ˜¯å¦å¯ç”¨ç±»åˆ«å¹³è¡¡

    Returns:
        w: æ ·æœ¬æƒé‡åºåˆ—ï¼ˆå½’ä¸€åŒ–åˆ°å‡å€¼ä¸º 1ï¼‰
    """
    from sklearn.utils.class_weight import compute_class_weight

    # åŸºç¡€æƒé‡ï¼š|æ”¶ç›Š| Ã— æ—¶é—´è¡°å‡
    base = np.abs(ret.values) * scale * np.exp(-tt.values / float(tau))
    base = np.clip(base, 1e-3, None)

    # ç±»åˆ«å¹³è¡¡
    if balance:
        classes = np.array(sorted(y.unique()))
        cls_w = compute_class_weight('balanced', classes=classes, y=y.values)
        w_map = dict(zip(classes, cls_w))
        cw = y.map(w_map).values
        w = base * cw
    else:
        w = base

    # å½’ä¸€åŒ–ï¼ˆå‡å€¼ä¸º 1ï¼‰
    w = w / np.mean(w)

    return pd.Series(w, index=y.index)


# ============================================================
# V4 å…¼å®¹å‡½æ•°ï¼ˆç»§æ‰¿ï¼‰
# ============================================================

def parse_args():
    """è§£æå‘½ä»¤åˆ—å‚æ•°"""
    p = argparse.ArgumentParser(
        "extract_tw_stock_data_v5",
        description="å°è‚¡ DeepLOB èµ„æ–™å‰å¤„ç†å·¥å…· (V5 Pro ç‰ˆæœ¬)"
    )
    p.add_argument(
        "--input-dir",
        default="./data/temp",
        type=str,
        help="å«æ¯æ—¥åŸå§‹ .txt çš„èµ„æ–™å¤¹"
    )
    p.add_argument(
        "--output-dir",
        default="./data/processed_v5",
        type=str,
        help="è¾“å‡ºèµ„æ–™å¤¹"
    )
    p.add_argument(
        "--config",
        default="./configs/config_pro_v5.yaml",
        type=str,
        help="V5 é…ç½®æ–‡ä»¶è·¯å¾„"
    )
    p.add_argument(
        "--make-npz",
        action="store_true",
        default=True,
        help="è¾“å‡º 70/15/15 çš„ .npz æ–‡ä»¶"
    )
    p.add_argument(
        "--stats-only",
        action="store_true",
        default=False,
        help="åªäº§ç”Ÿéœ‡ç›ªç»Ÿè®¡æŠ¥å‘Šï¼Œä¸ç”Ÿæˆè®­ç»ƒæ•°æ®ï¼ˆå¿«é€Ÿæ¨¡å¼ï¼‰"
    )
    return p.parse_args()


def load_config(config_path: str) -> Dict[str, Any]:
    """è½½å…¥ V5 é…ç½®æ–‡ä»¶ï¼ˆä½¿ç”¨ YAMLManagerï¼‰

    Args:
        config_path: é…ç½®æ–‡ä»¶è·¯å¾„

    Returns:
        é…ç½®å­—å…¸

    Raises:
        FileNotFoundError: é…ç½®æ–‡ä»¶ä¸å­˜åœ¨æ—¶æŠ›å‡ºå¼‚å¸¸
    """
    if not os.path.exists(config_path):
        raise FileNotFoundError(
            f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}\n"
            f"è¯·ç¡®ä¿é…ç½®æ–‡ä»¶å­˜åœ¨ï¼Œæˆ–ä½¿ç”¨ --config å‚æ•°æŒ‡å®šæ­£ç¡®çš„é…ç½®æ–‡ä»¶è·¯å¾„ã€‚\n"
            f"ç¤ºä¾‹é…ç½®æ–‡ä»¶ä½äº: configs/config_pro_v5.yaml"
        )

    # ä½¿ç”¨ YAMLManager åŠ è½½é…ç½®æ–‡ä»¶ï¼ˆä¼šè‡ªåŠ¨æ£€æŸ¥æ–‡ä»¶å­˜åœ¨æ€§ï¼‰
    yaml_manager = YAMLManager(config_path)
    cfg = yaml_manager.as_dict()

    logging.info(f"å·²è½½å…¥é…ç½®æ–‡ä»¶: {config_path}")
    return cfg


def hhmmss_to_int(s: str) -> int:
    s = s.strip()
    if not s.isdigit():
        return -1
    return int(s)


def to_float(x: str, default=0.0) -> float:
    try:
        return float(x)
    except:
        return default


def is_in_trading_window(t: int) -> bool:
    return TRADING_START <= t <= TRADING_END


def spread_ok(bid1: float, ask1: float) -> bool:
    if bid1 <= 0 or ask1 <= 0:
        return False
    if bid1 >= ask1:
        return False
    mid = 0.5 * (bid1 + ask1)
    return (ask1 - bid1) / max(mid, 1e-12) <= 0.05


def within_limits(px: float, lo: float, hi: float) -> bool:
    if lo > 0 and px < lo - 1e-12:
        return False
    if hi > 0 and px > hi + 1e-12:
        return False
    return True


def parse_line(raw: str) -> Tuple[str, int, Optional[Dict[str, Any]]]:
    """è§£æå•è¡Œæ•°æ®ï¼ˆç»§æ‰¿è‡ª V4ï¼‰"""
    global global_stats
    global_stats["total_raw_events"] += 1

    parts = raw.strip().split("||")
    if len(parts) < 34:
        return ("", -1, None)

    sym = parts[1].strip()

    try:
        t = hhmmss_to_int(parts[IDX_TIME])
    except:
        t = -1

    # è¯•æ’®ç§»é™¤ï¼æ—¶é—´çª—æ£€æŸ¥
    if parts[IDX_TRIAL].strip() == "1":
        return (sym, t, None)
    if not is_in_trading_window(t):
        return (sym, t, None)

    # å–äº”æ¡£ä»·é‡
    bids_p = [to_float(parts[i], 0.0) for i in BID_P_IDX]
    bids_q = [to_float(parts[i], 0.0) for i in BID_Q_IDX]
    asks_p = [to_float(parts[i], 0.0) for i in ASK_P_IDX]
    asks_q = [to_float(parts[i], 0.0) for i in ASK_Q_IDX]

    bid1, ask1 = bids_p[0], asks_p[0]
    if not spread_ok(bid1, ask1):
        return (sym, t, None)

    # é›¶å€¼å¤„ç†
    for p, q in zip(bids_p + asks_p, bids_q + asks_q):
        if p == 0.0 and q != 0.0:
            return (sym, t, None)

    ref = to_float(parts[IDX_REF], 0.0)
    upper = to_float(parts[IDX_UPPER], 0.0)
    lower = to_float(parts[IDX_LOWER], 0.0)
    last_px = to_float(parts[IDX_LASTPRICE], 0.0)
    tv = max(0, int(to_float(parts[IDX_TV], 0.0)))

    # ä»·æ ¼é™åˆ¶æ£€æŸ¥
    prices_to_check = [p for p in bids_p + asks_p if p > 0]
    if not all(within_limits(p, lower, upper) for p in prices_to_check):
        return (sym, t, None)

    # ç»„ 20 ç»´ç‰¹å¾
    feat = np.array(bids_p + asks_p + bids_q + asks_q, dtype=np.float64)
    mid = 0.5 * (bid1 + ask1)

    rec = {
        "feat": feat,
        "mid": mid,
        "ref": ref,
        "upper": upper,
        "lower": lower,
        "last_px": last_px,
        "tv": tv,
        "raw": raw.strip()
    }

    global_stats["cleaned_events"] += 1
    return (sym, t, rec)


def dedup_by_timestamp_keep_last(rows: List[Tuple[int, Dict[str,Any]]]) -> List[Tuple[int, Dict[str,Any]]]:
    """æ—¶é—´æˆ³å»é‡ï¼Œä¿ç•™æœ€åä¸€ç¬”ï¼ˆç»§æ‰¿è‡ª V4ï¼‰"""
    if not rows:
        return rows

    dedup_dict = {}
    for idx, (t, r) in enumerate(rows):
        tv = r.get("tv", 0)
        key = (t, tv)
        dedup_dict[key] = (idx, r)

    result_with_idx = sorted(dedup_dict.values(), key=lambda x: x[0])
    result = [(rows[idx][0], r) for idx, r in result_with_idx]

    return result


def aggregate_chunks_of_10(seq: List[Tuple[int, Dict[str,Any]]]) -> Tuple[np.ndarray, np.ndarray]:
    """æ¯ 10 ç¬”å¿«ç…§ â†’ 1 æ—¶é—´ç‚¹ï¼ˆç»§æ‰¿è‡ª V4ï¼‰"""
    global global_stats

    if len(seq) < AGG_FACTOR:
        return np.zeros((0, 20), dtype=np.float64), np.zeros((0,), dtype=np.float64)

    feats, mids = [], []
    for i in range(AGG_FACTOR-1, len(seq), AGG_FACTOR):
        feat = seq[i][1]["feat"]
        mid = seq[i][1]["mid"]
        feats.append(feat)
        mids.append(mid)

    global_stats["aggregated_points"] += len(feats)
    return np.stack(feats, axis=0), np.array(mids, dtype=np.float64)


def calculate_intraday_volatility(mids: np.ndarray, date: str, symbol: str) -> Optional[Dict[str, Any]]:
    """
    è¨ˆç®—ç•¶æ—¥éœ‡ç›ªçµ±è¨ˆ

    Args:
        mids: ä¸­é–“åƒ¹åºåˆ—
        date: äº¤æ˜“æ—¥æœŸ
        symbol: è‚¡ç¥¨ä»£ç¢¼

    Returns:
        éœ‡ç›ªçµ±è¨ˆå­—å…¸ï¼ŒåŒ…å«ï¼š
        - range_pct: éœ‡ç›ªå¹…åº¦ (æœ€é«˜-æœ€ä½)/é–‹ç›¤
        - high: æœ€é«˜åƒ¹
        - low: æœ€ä½åƒ¹
        - open: é–‹ç›¤åƒ¹
        - close: æ”¶ç›¤åƒ¹
        - return_pct: æ¼²è·Œå¹…
        - n_points: æ•¸æ“šé»æ•¸
    """
    if mids.size == 0:
        return None

    open_price = mids[0]
    close_price = mids[-1]
    high_price = mids.max()
    low_price = mids.min()

    # é¿å…é™¤ä»¥é›¶
    if open_price <= 0:
        return None

    # è¨ˆç®—éœ‡ç›ªå¹…åº¦ï¼ˆç›¸å°æ–¼é–‹ç›¤åƒ¹ï¼‰
    range_pct = (high_price - low_price) / open_price

    # è¨ˆç®—æ¼²è·Œå¹…
    return_pct = (close_price - open_price) / open_price

    return {
        "date": date,
        "symbol": symbol,
        "range_pct": float(range_pct),
        "return_pct": float(return_pct),
        "high": float(high_price),
        "low": float(low_price),
        "open": float(open_price),
        "close": float(close_price),
        "n_points": len(mids)
    }


def generate_volatility_report(vol_stats: List[Dict[str, Any]], out_dir: str):
    """
    ç”Ÿæˆéœ‡ç›ªçµ±è¨ˆå ±å‘Šï¼ˆCSV + JSON + æ§åˆ¶å°è¼¸å‡ºï¼‰

    Args:
        vol_stats: éœ‡ç›ªçµ±è¨ˆåˆ—è¡¨
        out_dir: è¼¸å‡ºç›®éŒ„
    """
    if not vol_stats:
        logging.warning("æ²’æœ‰éœ‡ç›ªçµ±è¨ˆè³‡æ–™å¯ç”¢ç”Ÿå ±å‘Š")
        return

    df = pd.DataFrame(vol_stats)

    # è¨ˆç®—çµ±è¨ˆæ‘˜è¦
    range_values = df['range_pct'].values * 100  # è½‰ç‚ºç™¾åˆ†æ¯”
    return_values = df['return_pct'].values * 100

    # åˆ†ä½æ•¸çµ±è¨ˆ
    percentiles = [10, 25, 50, 75, 90, 95, 99]
    range_percentiles = np.percentile(range_values, percentiles)
    return_percentiles = np.percentile(return_values, percentiles)

    # é–¾å€¼çµ±è¨ˆ
    thresholds = [0.5, 1.0, 1.5, 2.0, 2.5, 3.0, 4.0, 5.0]
    threshold_stats = []

    for threshold in thresholds:
        count = (range_values >= threshold).sum()
        pct = count / len(range_values) * 100
        threshold_stats.append({
            'threshold_pct': float(threshold),
            'count': int(count),
            'percentage': float(pct)
        })

    # æ§åˆ¶å°è¼¸å‡º
    logging.info(f"\n{'='*60}")
    logging.info("ğŸ“Š éœ‡ç›ªå¹…åº¦çµ±è¨ˆå ±å‘Šï¼ˆIntraday Range Analysisï¼‰")
    logging.info(f"{'='*60}")
    logging.info(f"ç¸½æ¨£æœ¬æ•¸: {len(df):,} å€‹ symbol-day çµ„åˆ")
    logging.info(f"è‚¡ç¥¨æ•¸: {df['symbol'].nunique()} æª”")
    logging.info(f"äº¤æ˜“æ—¥æ•¸: {df['date'].nunique()} å¤©")
    logging.info(f"\néœ‡ç›ªå¹…åº¦ (Range %) åˆ†å¸ƒ:")
    logging.info(f"  æœ€å°å€¼: {range_values.min():.2f}%")
    logging.info(f"  æœ€å¤§å€¼: {range_values.max():.2f}%")
    logging.info(f"  å¹³å‡å€¼: {range_values.mean():.2f}%")
    logging.info(f"  ä¸­ä½æ•¸: {np.median(range_values):.2f}%")
    logging.info(f"  æ¨™æº–å·®: {range_values.std():.2f}%")

    logging.info(f"\nåˆ†ä½æ•¸åˆ†å¸ƒ:")
    for p, val in zip(percentiles, range_percentiles):
        logging.info(f"  P{p:2d}: {val:6.2f}%")

    logging.info(f"\né–¾å€¼ç¯©é¸çµ±è¨ˆï¼ˆéœ‡ç›ª â‰¥ X% çš„æ¨£æœ¬æ•¸ï¼‰:")
    logging.info(f"{'é–¾å€¼':>6} | {'æ¨£æœ¬æ•¸':>8} | {'ä½”æ¯”':>6}")
    logging.info(f"{'-'*6}-+-{'-'*8}-+-{'-'*6}")
    for ts in threshold_stats:
        logging.info(f"{ts['threshold_pct']:5.1f}% | {ts['count']:8,} | {ts['percentage']:5.1f}%")

    logging.info(f"\næ¼²è·Œå¹… (Return %) åˆ†å¸ƒ:")
    logging.info(f"  å¹³å‡å€¼: {return_values.mean():.2f}%")
    logging.info(f"  ä¸­ä½æ•¸: {np.median(return_values):.2f}%")
    logging.info(f"  æ¨™æº–å·®: {return_values.std():.2f}%")

    # æ‰¾å‡ºæ¥µç«¯æ¡ˆä¾‹
    top_volatile = df.nlargest(10, 'range_pct')[['date', 'symbol', 'range_pct', 'return_pct']]
    logging.info(f"\néœ‡ç›ªæœ€å¤§çš„ 10 å€‹æ¨£æœ¬:")
    for idx, row in top_volatile.iterrows():
        logging.info(f"  {row['symbol']} @ {row['date']}: éœ‡ç›ª {row['range_pct']*100:.2f}%, å ±é…¬ {row['return_pct']*100:.2f}%")

    logging.info(f"{'='*60}\n")

    # ä¿å­˜ CSV
    csv_path = os.path.join(out_dir, "volatility_stats.csv")
    df.to_csv(csv_path, index=False, encoding='utf-8-sig')
    logging.info(f"âœ… éœ‡ç›ªçµ±è¨ˆå·²ä¿å­˜: {csv_path}")

    # ä¿å­˜ JSON æ‘˜è¦
    summary = {
        "total_samples": int(len(df)),
        "n_stocks": int(df['symbol'].nunique()),
        "n_dates": int(df['date'].nunique()),
        "range_pct": {
            "min": float(range_values.min()),
            "max": float(range_values.max()),
            "mean": float(range_values.mean()),
            "median": float(np.median(range_values)),
            "std": float(range_values.std()),
            "percentiles": {f"P{p}": float(v) for p, v in zip(percentiles, range_percentiles)}
        },
        "return_pct": {
            "mean": float(return_values.mean()),
            "median": float(np.median(return_values)),
            "std": float(return_values.std()),
            "percentiles": {f"P{p}": float(v) for p, v in zip(percentiles, return_percentiles)}
        },
        "threshold_stats": threshold_stats,
        "top_10_volatile": [
            {
                "symbol": str(row['symbol']),
                "date": str(row['date']),
                "range_pct": float(row['range_pct'] * 100),
                "return_pct": float(row['return_pct'] * 100)
            }
            for _, row in top_volatile.iterrows()
        ]
    }

    json_path = os.path.join(out_dir, "volatility_summary.json")
    with open(json_path, "w", encoding="utf-8") as f:
        json.dump(summary, f, ensure_ascii=False, indent=2)

    logging.info(f"âœ… éœ‡ç›ªæ‘˜è¦å·²ä¿å­˜: {json_path}\n")


def zscore_fit(X: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
    """è®¡ç®— Z-Score å‚æ•°ï¼ˆç»§æ‰¿è‡ª V4ï¼‰"""
    mu = X.mean(axis=0)
    sd = X.std(axis=0, ddof=0)
    sd = np.where(sd < 1e-8, 1.0, sd)

    if np.any(np.abs(mu) > 1e6):
        logging.warning(f"ä¾¦æµ‹åˆ°å¼‚å¸¸å¤§çš„å‡å€¼: max|Î¼|={np.max(np.abs(mu)):.2f}")

    return mu, sd


def zscore_apply(X: np.ndarray, mu: np.ndarray, sd: np.ndarray) -> np.ndarray:
    """åº”ç”¨ Z-Score æ­£è§„åŒ–ï¼ˆç»§æ‰¿è‡ª V4ï¼‰"""
    return (X - mu.reshape(1, -1)) / sd.reshape(1, -1)


def extract_date_from_filename(fp: str) -> str:
    """ä»æ¡£åæŠ“å–æ—¥æœŸï¼ˆç»§æ‰¿è‡ª V4ï¼‰"""
    name = os.path.basename(fp)
    m = re.search(r"(20\d{6})", name)
    if m:
        return m.group(1)
    return name


# ============================================================
# V5 æ ¸å¿ƒï¼šæ»‘çª— + Triple-Barrier æ ‡ç­¾ + æ ·æœ¬æƒé‡
# ============================================================

def sliding_windows_v5(
    days_points: List[Tuple[str, str, np.ndarray, np.ndarray]],
    out_dir: str,
    config: Dict[str, Any]
):
    """
    V5 æ»‘çª—æµç¨‹ï¼š
    1. ä»¥è‚¡ç¥¨ä¸ºå•ä½ä¸²æ¥èµ„æ–™
    2. è®¡ç®—æ³¢åŠ¨ç‡ï¼ˆEWMA/YZ/GARCHï¼‰
    3. ç”Ÿæˆ Triple-Barrier æ ‡ç­¾
    4. è®¡ç®—æ ·æœ¬æƒé‡
    5. äº§ç”Ÿæ»‘çª—æ ·æœ¬
    6. 70/15/15 åˆ‡åˆ†
    """
    global global_stats

    if not days_points:
        logging.warning("æ²¡æœ‰èµ„æ–™å¯ä¾›äº§ç”Ÿ .npz æ¡£æ¡ˆ")
        return

    logging.info(f"\n{'='*60}")
    logging.info(f"V5 æ»‘çª—æµç¨‹å¼€å§‹ï¼Œå…± {len(days_points)} ä¸ª symbol-day ç»„åˆ")
    logging.info(f"{'='*60}")

    # æ­¥éª¤ 1: ä»¥è‚¡ç¥¨ä¸ºå•ä½é‡ç»„èµ„æ–™
    stock_data = defaultdict(lambda: {'dates': [], 'X': [], 'mids': []})

    for date, sym, Xd, mids in days_points:
        stock_data[sym]['dates'].append(date)
        stock_data[sym]['X'].append(Xd)
        stock_data[sym]['mids'].append(mids)

    logging.info(f"å…± {len(stock_data)} ä¸ªè‚¡ç¥¨æœ‰èµ„æ–™")

    # å¯¹æ¯ä¸ªè‚¡ç¥¨ï¼ŒæŒ‰æ—¥æœŸæ’åºå¹¶ä¸²æ¥
    stock_sequences = []

    for sym, data in stock_data.items():
        sorted_indices = np.argsort(data['dates'])

        X_concat = np.concatenate([data['X'][i] for i in sorted_indices], axis=0)
        mids_concat = np.concatenate([data['mids'][i] for i in sorted_indices], axis=0)

        stock_sequences.append((sym, X_concat.shape[0], X_concat, mids_concat))

    # æŒ‰æ—¶é—´ç‚¹æ•°é‡æ’åºï¼ˆä»…ç”¨äºç»Ÿè®¡ï¼‰
    stock_sequences_sorted = sorted(stock_sequences, key=lambda x: x[1], reverse=True)

    logging.info(f"\nè‚¡ç¥¨æ—¶é—´ç‚¹ç»Ÿè®¡:")
    logging.info(f"  æœ€å¤š: {stock_sequences_sorted[0][1]} ä¸ªç‚¹ ({stock_sequences_sorted[0][0]})")
    logging.info(f"  æœ€å°‘: {stock_sequences_sorted[-1][1]} ä¸ªç‚¹ ({stock_sequences_sorted[-1][0]})")

    # æ­¥éª¤ 2: è¿‡æ»¤å¤ªçŸ­çš„è‚¡ç¥¨åºåˆ—
    MIN_POINTS = SEQ_LEN + 50  # 100 + 50 = 150

    valid_stocks = [s for s in stock_sequences if s[1] >= MIN_POINTS]
    filtered_stocks = len(stock_sequences) - len(valid_stocks)

    if filtered_stocks > 0:
        logging.warning(f"è¿‡æ»¤ {filtered_stocks} æ¡£åºåˆ—å¤ªçŸ­çš„è‚¡ç¥¨ï¼ˆ< {MIN_POINTS} ä¸ªç‚¹ï¼‰")

    if not valid_stocks:
        logging.error("æ²¡æœ‰è‚¡ç¥¨æœ‰è¶³å¤Ÿçš„æ—¶é—´ç‚¹äº§ç”Ÿæ»‘çª—æ ·æœ¬ï¼")
        return

    logging.info(f"æœ‰æ•ˆè‚¡ç¥¨: {len(valid_stocks)} æ¡£")

    # æ­¥éª¤ 3: éšæœºæ‰“ä¹±åæŒ‰è‚¡ç¥¨æ•°é‡åˆ‡åˆ† 70/15/15ï¼ˆé¿å…åˆ†å¸ƒåç§»ï¼‰
    import random
    SPLIT_SEED = config.get('split', {}).get('seed', 42)
    random.Random(SPLIT_SEED).shuffle(valid_stocks)

    logging.info(f"ä½¿ç”¨éšæœºç§å­ {SPLIT_SEED} æ‰“ä¹±è‚¡ç¥¨é¡ºåºï¼ˆé¿å…é€‰æ‹©åå·®ï¼‰")

    n_stocks = len(valid_stocks)
    n_train = max(1, int(n_stocks * config['split']['train_ratio']))
    n_val = max(1, int(n_stocks * config['split']['val_ratio']))

    train_stocks = valid_stocks[:n_train]
    val_stocks = valid_stocks[n_train:n_train + n_val]
    test_stocks = valid_stocks[n_train + n_val:]

    logging.info(f"\nèµ„æ–™åˆ‡åˆ†ï¼ˆæŒ‰è‚¡ç¥¨æ•°ï¼‰:")
    logging.info(f"  Train: {len(train_stocks)} æ¡£è‚¡ç¥¨")
    logging.info(f"  Val:   {len(val_stocks)} æ¡£è‚¡ç¥¨")
    logging.info(f"  Test:  {len(test_stocks)} æ¡£è‚¡ç¥¨")

    splits = {
        'train': train_stocks,
        'val': val_stocks,
        'test': test_stocks
    }

    # æ­¥éª¤ 4: è®¡ç®—è®­ç»ƒé›†çš„ Z-Score å‚æ•°
    logging.info(f"\n{'='*60}")
    logging.info("è®¡ç®— Z-Score å‚æ•°ï¼ˆåŸºäºè®­ç»ƒé›†ï¼‰")
    logging.info(f"{'='*60}")

    train_X_list = [stock[2] for stock in train_stocks]
    Xtr = np.concatenate(train_X_list, axis=0) if train_X_list else np.zeros((0, 20))

    if Xtr.size == 0:
        mu = np.zeros((20,), dtype=np.float64)
        sd = np.ones((20,), dtype=np.float64)
        logging.warning("è®­ç»ƒé›†ä¸ºç©ºï¼Œä½¿ç”¨é¢„è®¾ Z-Score å‚æ•°")
    else:
        mu, sd = zscore_fit(Xtr)
        logging.info(f"è®­ç»ƒé›†å¤§å°: {Xtr.shape[0]:,} ä¸ªæ—¶é—´ç‚¹")

    # æ­¥éª¤ 5: äº§ç”Ÿæ»‘çª—æ ·æœ¬ï¼ˆV5 æ ¸å¿ƒï¼‰
    def build_split_v5(split_name):
        """å¯¹ä¸€ä¸ª split äº§ç”Ÿ V5 æ»‘çª—æ ·æœ¬"""
        stock_list = splits[split_name]

        logging.info(f"\n{'='*60}")
        logging.info(f"å¤„ç† {split_name.upper()} é›†ï¼Œå…± {len(stock_list)} æ¡£è‚¡ç¥¨")
        logging.info(f"{'='*60}")

        X_windows = []
        y_labels = []
        weights = []
        stock_ids = []

        total_windows = 0
        tb_stats = {"up": 0, "down": 0, "time": 0}

        for sym, n_points, Xd, mids in stock_list:
            # Z-score æ­£è§„åŒ–
            Xn = zscore_apply(Xd, mu, sd)

            # æ„å»º DataFrame ç”¨äº V5 æ ‡ç­¾ç”Ÿæˆ
            close = pd.Series(mids, name='close')

            # è®¡ç®—æ³¢åŠ¨ç‡
            vol_method = config['volatility']['method']

            # è®¡ç®—æ³¢åŠ¨ç‡ï¼ˆå¤±è´¥åˆ™æŠ›å‡ºå¼‚å¸¸ï¼‰
            if vol_method == 'ewma':
                vol = ewma_vol(close, halflife=config['volatility']['halflife'])
            elif vol_method == 'garch':
                vol = garch11_vol(close)
            else:
                raise ValueError(f"ä¸æ”¯æ´çš„æ³¢åŠ¨ç‡æ–¹æ³•: {vol_method}ï¼Œè¯·ä½¿ç”¨ 'ewma' æˆ– 'garch'")

            # ç”Ÿæˆ Triple-Barrier æ ‡ç­¾ï¼ˆå¤±è´¥åˆ™åœæ­¢æµç¨‹ï¼‰
            tb_cfg = config['triple_barrier']
            tb_df = tb_labels(
                close=close,
                vol=vol,
                pt_mult=tb_cfg['pt_multiplier'],
                sl_mult=tb_cfg['sl_multiplier'],
                max_holding=tb_cfg['max_holding'],
                min_return=tb_cfg['min_return']
            )

            # è½¬æ¢æ ‡ç­¾ {-1,0,1} â†’ {0,1,2}
            y_tb = tb_df["y"].map({-1: 0, 0: 1, 1: 2})

            # è®¡ç®—æ ·æœ¬æƒé‡
            if config['sample_weights']['enabled']:
                w = make_sample_weight(
                    ret=tb_df["ret"],
                    tt=tb_df["tt"],
                    y=y_tb,
                    tau=config['sample_weights']['tau'],
                    scale=config['sample_weights']['return_scaling'],
                    balance=config['sample_weights']['balance_classes']
                )
            else:
                w = pd.Series(np.ones(len(y_tb)), index=y_tb.index)

            # ç»Ÿè®¡è§¦å‘åŸå› 
            for reason in tb_df["why"].value_counts().items():
                if reason[0] in tb_stats:
                    tb_stats[reason[0]] += reason[1]

            # äº§ç”Ÿæ»‘çª—æ ·æœ¬
            T = Xn.shape[0]
            max_t = min(T, len(y_tb))

            if max_t < SEQ_LEN:
                logging.warning(f"  {sym}: è·³è¿‡ï¼ˆåªæœ‰ {max_t} ä¸ªç‚¹ï¼‰")
                continue

            windows_count = 0

            for t in range(SEQ_LEN - 1, max_t):
                window = Xn[t - SEQ_LEN + 1:t + 1, :]

                if window.shape[0] != SEQ_LEN:
                    continue

                label = int(y_tb.iloc[t])
                weight = float(w.iloc[t])

                if label not in [0, 1, 2]:
                    continue

                X_windows.append(window.astype(np.float32))
                y_labels.append(label)
                weights.append(weight)
                stock_ids.append(sym)
                windows_count += 1

            if windows_count > 0:
                logging.info(f"  {sym}: {windows_count:,} ä¸ªæ ·æœ¬ (å…± {T} ä¸ªç‚¹)")

            total_windows += windows_count

        logging.info(f"\n{split_name.upper()} æ€»è®¡: {total_windows:,} ä¸ªæ ·æœ¬")
        logging.info(f"è§¦å‘åŸå› åˆ†å¸ƒ: {tb_stats}")

        global_stats["valid_windows"] += total_windows

        if X_windows:
            X_array = np.stack(X_windows, axis=0)
            y_array = np.array(y_labels, dtype=np.int64)
            w_array = np.array(weights, dtype=np.float32)
            sid_array = np.array(stock_ids, dtype=object)

            # ç»Ÿè®¡èµ„è®¯
            unique_stocks = len(np.unique(sid_array))
            label_dist = np.bincount(y_array, minlength=3)

            logging.info(f"  å½¢çŠ¶: X={X_array.shape}, y={y_array.shape}, w={w_array.shape}")
            logging.info(f"  æ¶µç›–è‚¡ç¥¨: {unique_stocks} æ¡£")
            logging.info(f"  æ ‡ç­¾åˆ†å¸ƒ: ä¸Šæ¶¨={label_dist[0]}, æŒå¹³={label_dist[1]}, ä¸‹è·Œ={label_dist[2]}")
            logging.info(f"  æƒé‡ç»Ÿè®¡: mean={w_array.mean():.3f}, std={w_array.std():.3f}, max={w_array.max():.3f}")

            return X_array, y_array, w_array, sid_array
        else:
            logging.warning(f"{split_name} é›†æ²¡æœ‰äº§ç”Ÿä»»ä½•æ ·æœ¬ï¼")
            return (
                np.zeros((0, SEQ_LEN, 20), dtype=np.float32),
                np.zeros((0,), dtype=np.int64),
                np.zeros((0,), dtype=np.float32),
                np.array([], dtype=object)
            )

    # æ­¥éª¤ 6: äº§ç”Ÿå¹¶ä¿å­˜ npz æ¡£æ¡ˆ
    os.makedirs(out_dir, exist_ok=True)

    results = {}

    for split in ["train", "val", "test"]:
        X, y, w, sid = build_split_v5(split)

        npz_path = os.path.join(out_dir, f"stock_embedding_{split}.npz")
        np.savez_compressed(npz_path, X=X, y=y, weights=w, stock_ids=sid)

        logging.info(f"\nâœ… å·²ä¿å­˜: {npz_path}")

        results[split] = {
            "samples": len(y),
            "label_dist": np.bincount(y, minlength=3).tolist() if len(y) > 0 else [0, 0, 0],
            "weight_stats": {
                "mean": float(w.mean()) if len(w) > 0 else 0.0,
                "std": float(w.std()) if len(w) > 0 else 0.0,
                "max": float(w.max()) if len(w) > 0 else 0.0
            }
        }

    # æ­¥éª¤ 7: å†™å…¥ metadata
    meta = {
        "format": "deeplob_v5_pro",
        "version": VERSION,
        "creation_date": datetime.now().isoformat(),
        "seq_len": SEQ_LEN,
        "feature_dim": 20,

        "volatility": {
            "method": config['volatility']['method'],
            "halflife": config['volatility'].get('halflife', 60)
        },

        "triple_barrier": config['triple_barrier'],

        "sample_weights": {
            "enabled": config['sample_weights']['enabled'],
            "tau": config['sample_weights']['tau'],
            "return_scaling": config['sample_weights']['return_scaling'],
            "balance_classes": config['sample_weights']['balance_classes']
        },

        "normalization": {
            "method": "zscore",
            "computed_on": "train_set",
            "feature_means": mu.tolist(),
            "feature_stds": sd.tolist()
        },

        "data_quality": global_stats,

        "data_split": {
            "method": "by_stock_count",
            "train_stocks": len(train_stocks),
            "val_stocks": len(val_stocks),
            "test_stocks": len(test_stocks),
            "total_stocks": len(valid_stocks),
            "filtered_stocks": filtered_stocks,
            "results": results
        },

        "note": "V5 Pro: Triple-Barrier labels + Sample weights. Labels: {0:ä¸‹è·Œ, 1:æŒå¹³, 2:ä¸Šæ¶¨}",
    }

    meta_path = os.path.join(out_dir, "normalization_meta.json")
    with open(meta_path, "w", encoding="utf-8") as f:
        json.dump(meta, f, ensure_ascii=False, indent=2)

    logging.info(f"\nâœ… Metadata å·²ä¿å­˜: {meta_path}")
    logging.info(f"\n{'='*60}")
    logging.info("NPZ æ¡£æ¡ˆäº§ç”Ÿå®Œæˆï¼")
    logging.info(f"{'='*60}")


# ============================================================
# ä¸»ç¨‹å¼
# ============================================================

def main():
    """ä¸»ç¨‹å¼"""
    try:
        args = parse_args()
        in_dir = args.input_dir
        out_dir = args.output_dir

        # è½½å…¥é…ç½®
        config = load_config(args.config)

        # éªŒè¯è¾“å…¥ç›®å½•å­˜åœ¨
        if not os.path.exists(in_dir):
            logging.error(f"è¾“å…¥ç›®å½•ä¸å­˜åœ¨: {in_dir}")
            return 1

        # å»ºç«‹è¾“å‡ºç›®å½•
        try:
            os.makedirs(out_dir, exist_ok=True)
        except Exception as e:
            logging.error(f"æ— æ³•å»ºç«‹è¾“å‡ºç›®å½• {out_dir}: {e}")
            return 1

        logging.info(f"{'='*60}")
        logging.info(f"V5 Pro èµ„æ–™æµæ°´çº¿å¯åŠ¨")
        logging.info(f"{'='*60}")
        logging.info(f"è¾“å…¥ç›®å½•: {in_dir}")
        logging.info(f"è¾“å‡ºç›®å½•: {out_dir}")
        logging.info(f"é…ç½®ç‰ˆæœ¬: {config['version']}")
        logging.info(f"æ³¢åŠ¨ç‡æ–¹æ³•: {config['volatility']['method']}")
        logging.info(f"Triple-Barrier: PT={config['triple_barrier']['pt_multiplier']}Ïƒ, "
                    f"SL={config['triple_barrier']['sl_multiplier']}Ïƒ, "
                    f"MaxHold={config['triple_barrier']['max_holding']}")
        logging.info(f"{'='*60}\n")

        # è¯»å–æ‰€æœ‰ .txt æ¡£
        files = sorted(glob.glob(os.path.join(in_dir, "*.txt")))
        if not files:
            logging.error(f"åœ¨ {in_dir} æ‰¾ä¸åˆ° .txt æ¡£æ¡ˆ")
            return 1

        logging.info(f"æ‰¾åˆ° {len(files)} ä¸ªæ¡£æ¡ˆå¾…å¤„ç†")

        # å…ˆæŒ‰æ¡£åæ—¥æœŸåˆ†ç»„
        day_map: Dict[str, List[str]] = defaultdict(list)
        for fp in files:
            day = extract_date_from_filename(fp)
            day_map[day].append(fp)

        # é€æ—¥é€æ¡£æ¡ˆè¯»å– â†’ é€ symbol æ±‡æ•´
        per_day_symbol_points = []  # [(date, symbol, X_points, mids)]
        day_keys = sorted(day_map.keys())

        logging.info(f"å¼€å§‹å¤„ç† {len(day_keys)} ä¸ªäº¤æ˜“æ—¥çš„èµ„æ–™")

        for day in day_keys:
            fps = sorted(day_map[day])
            logging.info(f"å¤„ç†æ—¥æœŸ {day}ï¼Œå…± {len(fps)} ä¸ªæ¡£æ¡ˆ")

            # è¯»æœ¬æ—¥æ‰€æœ‰è¡Œ
            per_symbol_raw: Dict[str, List[Tuple[int, Dict[str,Any]]]] = defaultdict(list)

            for fp in fps:
                try:
                    with open(fp, "r", encoding="utf-8", errors="ignore") as f:
                        for raw in f:
                            sym, t, rec = parse_line(raw)
                            if rec is None or sym == "" or t < 0:
                                continue
                            per_symbol_raw[sym].append((t, rec))
                except Exception as e:
                    logging.warning(f"è¯»å–æ¡£æ¡ˆ {fp} æ—¶å‘ç”Ÿé”™è¯¯: {e}")
                    continue

            # å¯¹æ¯ä¸ª symbolï¼šå»é‡ã€èšåˆã€ä¿å­˜ä¸­é—´ä»·
            for sym, rows in per_symbol_raw.items():
                if not rows:
                    continue

                # æ—¶é—´æ’åº
                rows.sort(key=lambda x: x[0])

                # å»é‡
                rows = dedup_by_timestamp_keep_last(rows)
                if not rows:
                    continue

                # 10 äº‹ä»¶èšåˆ
                Xp, mids = aggregate_chunks_of_10(rows)
                if Xp.shape[0] == 0:
                    continue

                # V5: è¨ˆç®—ä¸¦ä¿å­˜éœ‡ç›ªçµ±è¨ˆ
                vol_stats = calculate_intraday_volatility(mids, day, sym)
                if vol_stats is not None:
                    global_stats["volatility_stats"].append(vol_stats)

                    # V5: éœ‡ç›ªç¯©é¸ï¼ˆå¦‚æœå•Ÿç”¨ï¼‰
                    if config.get('intraday_volatility_filter', {}).get('enabled', False):
                        min_range = config['intraday_volatility_filter'].get('min_range_pct', 0.0)
                        max_range = config['intraday_volatility_filter'].get('max_range_pct', 1.0)

                        range_pct = vol_stats['range_pct']

                        if range_pct < min_range:
                            logging.debug(f"  {sym} @ {day}: éœ‡ç›ªéå° ({range_pct*100:.2f}% < {min_range*100:.1f}%)ï¼Œè·³é")
                            global_stats["volatility_filtered"] += 1
                            continue

                        if range_pct > max_range:
                            logging.debug(f"  {sym} @ {day}: éœ‡ç›ªéå¤§ ({range_pct*100:.2f}% > {max_range*100:.1f}%)ï¼Œè·³é")
                            global_stats["volatility_filtered"] += 1
                            continue

                # V5: ä¿å­˜ midsï¼ˆç”¨äºåç»­æ ‡ç­¾ç”Ÿæˆï¼‰
                per_day_symbol_points.append((day, sym, Xp, mids))

        # è‹¥æ²¡æœ‰å¯ç”¨èµ„æ–™
        if not per_day_symbol_points:
            logging.error("æ¸…æ´—æˆ–èšåˆåæ²¡æœ‰å¯ç”¨èµ„æ–™")
            return 1

        logging.info(f"å…±å¤„ç† {len(per_day_symbol_points)} ä¸ª symbol-day ç»„åˆ")

        # ç”¢ç”Ÿéœ‡ç›ªçµ±è¨ˆå ±å‘Š
        if global_stats["volatility_stats"]:
            generate_volatility_report(global_stats["volatility_stats"], out_dir)

        # äº§å‡º 70/15/15 çš„ .npzï¼ˆV5 æ»‘çª—æµç¨‹ï¼‰
        if args.stats_only:
            logging.info("\n" + "="*60)
            logging.info("âš¡ å¿«é€Ÿæ¨¡å¼ï¼šå·²å®Œæˆéœ‡ç›ªçµ±è¨ˆï¼Œè·³éè¨“ç·´æ•¸æ“šç”Ÿæˆ")
            logging.info("="*60)
            logging.info("å¦‚éœ€ç”Ÿæˆè¨“ç·´æ•¸æ“šï¼Œè«‹ç§»é™¤ --stats-only åƒæ•¸")
            logging.info("="*60 + "\n")
        elif args.make_npz:
            logging.info("å¼€å§‹äº§ç”Ÿ V5 .npz æ¡£æ¡ˆ")
            sliding_windows_v5(
                per_day_symbol_points,
                os.path.join(out_dir, "npz"),
                config
            )

        logging.info(f"\n{'='*60}")
        if args.stats_only:
            logging.info(f"[å®Œæˆ] éœ‡ç›ªçµ±è¨ˆæˆåŠŸï¼Œè¼¸å‡ºè³‡æ–™å¤¾: {out_dir}")
        else:
            logging.info(f"[å®Œæˆ] V5 è½¬æ¢æˆåŠŸï¼Œè¾“å‡ºèµ„æ–™å¤¹: {out_dir}")
        logging.info(f"{'='*60}")
        logging.info(f"ç»Ÿè®¡èµ„æ–™:")
        logging.info(f"  åŸå§‹äº‹ä»¶æ•°: {global_stats['total_raw_events']:,}")
        logging.info(f"  æ¸…æ´—å: {global_stats['cleaned_events']:,}")
        logging.info(f"  èšåˆåæ—¶é—´ç‚¹: {global_stats['aggregated_points']:,}")
        if not args.stats_only:
            logging.info(f"  æœ‰æ•ˆçª—å£: {global_stats['valid_windows']:,}")
            logging.info(f"  Triple-Barrier æˆåŠŸ: {global_stats['tb_success']:,}")
        logging.info(f"  éœ‡ç›ªçµ±è¨ˆæ¨£æœ¬: {len(global_stats['volatility_stats']):,}")
        logging.info(f"{'='*60}\n")

        return 0

    except Exception as e:
        logging.error(f"ç¨‹å¼æ‰§è¡Œå¤±è´¥: {e}")
        traceback.print_exc()
        return 1


if __name__ == "__main__":
    import sys
    sys.exit(main())
