#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
verify_class_weights.py - é©—è­‰ Class Weights è¨ˆç®—èˆ‡æ•¸æ“šåˆ†ä½ˆ
=============================================================================
ã€ç”¨é€”ã€‘æª¢æŸ¥è¨“ç·´æ•¸æ“šçš„é¡åˆ¥åˆ†ä½ˆå’Œæ¬Šé‡è¨ˆç®—æ˜¯å¦æ­£ç¢º

ã€ä½¿ç”¨æ–¹å¼ã€‘
conda activate deeplob-pro
python scripts/verify_class_weights.py
=============================================================================
"""

import sys
import numpy as np
import torch
from pathlib import Path
from collections import Counter

# æ·»åŠ å°ˆæ¡ˆæ ¹ç›®éŒ„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

def load_and_check_data(npz_path):
    """è¼‰å…¥ä¸¦æª¢æŸ¥æ•¸æ“š"""
    print(f"\nè¼‰å…¥æ•¸æ“š: {npz_path}")
    data = np.load(npz_path, allow_pickle=True)

    print("\nå¯ç”¨çš„ keys:")
    for key in data.keys():
        print(f"  - {key}: {data[key].shape}")

    return data

def analyze_label_distribution(y):
    """åˆ†ææ¨™ç±¤åˆ†ä½ˆ"""
    print("\n" + "="*60)
    print("æ¨™ç±¤åˆ†ä½ˆåˆ†æ")
    print("="*60)

    counter = Counter(y)
    total = len(y)

    print(f"\nç¸½æ¨£æœ¬æ•¸: {total:,}")
    print("\nå„é¡åˆ¥çµ±è¨ˆ:")

    class_names = {0: "ä¸‹è·Œ", 1: "æŒå¹³", 2: "ä¸Šæ¼²"}

    for cls in sorted(counter.keys()):
        count = counter[cls]
        pct = 100 * count / total
        name = class_names.get(cls, f"æœªçŸ¥({cls})")
        print(f"  Class {cls} ({name}): {count:,} ({pct:.2f}%)")

    # è¨ˆç®—ä¸å¹³è¡¡æ¯”ä¾‹
    counts = [counter[i] for i in sorted(counter.keys())]
    max_count = max(counts)
    min_count = min(counts)
    imbalance_ratio = max_count / min_count

    print(f"\nä¸å¹³è¡¡æ¯”ä¾‹: {imbalance_ratio:.2f}:1")

    if imbalance_ratio > 3.0:
        print("  âš ï¸  è­¦å‘Š: é¡åˆ¥åš´é‡ä¸å¹³è¡¡ï¼")
    elif imbalance_ratio > 1.5:
        print("  âš ï¸  æ³¨æ„: é¡åˆ¥ä¸­åº¦ä¸å¹³è¡¡")
    else:
        print("  âœ… é¡åˆ¥ç›¸å°å¹³è¡¡")

    return counter

def compute_class_weights(y, method='auto'):
    """è¨ˆç®—é¡åˆ¥æ¬Šé‡ï¼ˆæ¨¡æ“¬ PyTorch çš„ auto æ¨¡å¼ï¼‰"""
    print("\n" + "="*60)
    print(f"é¡åˆ¥æ¬Šé‡è¨ˆç®—ï¼ˆmethod={method}ï¼‰")
    print("="*60)

    counter = Counter(y)
    n_samples = len(y)
    n_classes = len(counter)

    if method == 'auto':
        # PyTorch auto æ¨¡å¼: n_samples / (n_classes * class_count)
        weights = {}
        for cls, count in counter.items():
            weights[cls] = n_samples / (n_classes * count)

        print(f"\nå…¬å¼: n_samples / (n_classes Ã— class_count)")
        print(f"  n_samples = {n_samples:,}")
        print(f"  n_classes = {n_classes}")

        print("\nè¨ˆç®—çµæœ:")
        for cls in sorted(weights.keys()):
            print(f"  Class {cls}: {weights[cls]:.4f}")

        # è½‰æ›ç‚º tensor
        weight_tensor = torch.FloatTensor([weights[i] for i in sorted(weights.keys())])

        return weight_tensor

    elif method == 'inverse_freq':
        # ç°¡å–®é€†é »ç‡
        weights = {}
        for cls, count in counter.items():
            weights[cls] = 1.0 / count

        # Normalize to sum=n_classes
        total_weight = sum(weights.values())
        for cls in weights:
            weights[cls] = weights[cls] / total_weight * n_classes

        weight_tensor = torch.FloatTensor([weights[i] for i in sorted(weights.keys())])

        return weight_tensor

def verify_loss_computation():
    """é©—è­‰æå¤±è¨ˆç®—ï¼ˆæ¨¡æ“¬å¯¦éš›è¨“ç·´ï¼‰"""
    print("\n" + "="*60)
    print("æå¤±è¨ˆç®—é©—è­‰")
    print("="*60)

    # æ¨¡æ“¬æ•¸æ“š
    batch_size = 32
    n_classes = 3

    # æ¨¡æ“¬ logitsï¼ˆéš¨æ©Ÿï¼‰
    torch.manual_seed(42)
    logits = torch.randn(batch_size, n_classes)

    # æ¨¡æ“¬æ¨™ç±¤ï¼ˆä¸å¹³è¡¡ï¼š10 å€‹ class 0, 20 å€‹ class 1, 2 å€‹ class 2ï¼‰
    labels = torch.tensor([0]*10 + [1]*20 + [2]*2)

    # è¨ˆç®— class weights
    counter = Counter(labels.numpy())
    n_samples = len(labels)
    class_weights = torch.FloatTensor([
        n_samples / (n_classes * counter[i]) for i in range(n_classes)
    ])

    print(f"\næ¨¡æ“¬æ‰¹æ¬¡:")
    print(f"  Batch size: {batch_size}")
    print(f"  é¡åˆ¥åˆ†ä½ˆ: Class 0={counter[0]}, Class 1={counter[1]}, Class 2={counter[2]}")
    print(f"  Class weights: {class_weights.numpy()}")

    # æ¸¬è©¦ 1: ä¸åŠ æ¬Š + ç„¡ smoothing
    loss_plain = torch.nn.functional.cross_entropy(logits, labels)

    # æ¸¬è©¦ 2: åŠ æ¬Š + ç„¡ smoothing
    loss_weighted = torch.nn.functional.cross_entropy(
        logits, labels, weight=class_weights
    )

    # æ¸¬è©¦ 3: åŠ æ¬Š + smoothing
    loss_weighted_smooth = torch.nn.functional.cross_entropy(
        logits, labels, weight=class_weights, label_smoothing=0.15
    )

    print(f"\næå¤±æ¯”è¼ƒ:")
    print(f"  Plain CE:                {loss_plain.item():.4f}")
    print(f"  Weighted CE:             {loss_weighted.item():.4f}")
    print(f"  Weighted CE + Smoothing: {loss_weighted_smooth.item():.4f}")

    # é©—è­‰åŠ æ¬Šç¢ºå¯¦å½±éŸ¿æå¤±
    if abs(loss_plain.item() - loss_weighted.item()) < 1e-6:
        print("\n  âš ï¸  è­¦å‘Š: åŠ æ¬Šä¼¼ä¹æ²’æœ‰æ•ˆæœï¼")
    else:
        print("\n  âœ… åŠ æ¬Šæ­£å¸¸å·¥ä½œ")

    # é©—è­‰ smoothing ç¢ºå¯¦å½±éŸ¿æå¤±
    if abs(loss_weighted.item() - loss_weighted_smooth.item()) < 1e-6:
        print("  âš ï¸  è­¦å‘Š: Label smoothing ä¼¼ä¹æ²’æœ‰æ•ˆæœï¼")
    else:
        print("  âœ… Label smoothing æ­£å¸¸å·¥ä½œ")

def check_sample_weights(data, use_weights=False):
    """æª¢æŸ¥æ¨£æœ¬æ¬Šé‡ï¼ˆå¦‚æœå­˜åœ¨ï¼‰"""
    print("\n" + "="*60)
    print("æ¨£æœ¬æ¬Šé‡æª¢æŸ¥")
    print("="*60)

    if not use_weights:
        print("\nâš ï¸  é…ç½®ä¸­å·²ç¦ç”¨æ¨£æœ¬æ¬Šé‡ï¼ˆuse_sample_weights=falseï¼‰")
        print("âœ… é€™æ˜¯æ­£ç¢ºçš„é¸æ“‡ï¼é¿å…æ¬Šé‡ç•°å¸¸å•é¡Œã€‚")
        return

    if 'weights' not in data:
        print("\nâš ï¸  æ•¸æ“šä¸­ä¸åŒ…å« 'weights' å­—æ®µ")
        return

    weights = data['weights']

    print(f"\næ¨£æœ¬æ¬Šé‡çµ±è¨ˆ:")
    print(f"  æ•¸é‡: {len(weights):,}")
    print(f"  Mean: {weights.mean():.4f}")
    print(f"  Std:  {weights.std():.4f}")
    print(f"  Min:  {weights.min():.4f}")
    print(f"  Max:  {weights.max():.4f}")
    print(f"  Median: {np.median(weights):.4f}")

    # æª¢æŸ¥ç•°å¸¸å€¼
    if weights.max() > 100:
        print(f"\n  âš ï¸  è­¦å‘Š: æœ€å¤§æ¬Šé‡ {weights.max():.2f} éå¤§ï¼")
        print("  å»ºè­°: clip åˆ° [0.1, 10.0] ä¸¦ normalize")

    if weights.min() < 0:
        print(f"\n  âŒ éŒ¯èª¤: å­˜åœ¨è² æ¬Šé‡ï¼")

    if np.any(~np.isfinite(weights)):
        print(f"\n  âŒ éŒ¯èª¤: å­˜åœ¨ NaN/Inf æ¬Šé‡ï¼")

    # æ¬Šé‡åˆ†ä½ˆ
    percentiles = [1, 5, 25, 50, 75, 95, 99]
    print(f"\næ¬Šé‡åˆ†ä½æ•¸:")
    for p in percentiles:
        val = np.percentile(weights, p)
        print(f"  {p:2d}%: {val:.4f}")

def main():
    """ä¸»å‡½æ•¸"""
    print("\n" + "="*60)
    print("ğŸ” Class Weights èˆ‡æ•¸æ“šåˆ†ä½ˆé©—è­‰")
    print("="*60)

    # æ•¸æ“šè·¯å¾‘
    train_path = "data/processed_v5_balanced/npz/stock_embedding_train.npz"
    val_path = "data/processed_v5_balanced/npz/stock_embedding_val.npz"
    test_path = "data/processed_v5_balanced/npz/stock_embedding_test.npz"

    # æª¢æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not Path(train_path).exists():
        print(f"\nâŒ éŒ¯èª¤: æ‰¾ä¸åˆ°è¨“ç·´æ•¸æ“š {train_path}")
        print("\nè«‹å…ˆé‹è¡Œæ•¸æ“šè™•ç†è…³æœ¬:")
        print("  python scripts/extract_tw_stock_data_v5.py \\")
        print("      --input-dir ./data/temp \\")
        print("      --output-dir ./data/processed_v5_balanced")
        return 1

    try:
        # 1. è¼‰å…¥è¨“ç·´æ•¸æ“š
        train_data = load_and_check_data(train_path)

        # 2. åˆ†ææ¨™ç±¤åˆ†ä½ˆ
        counter = analyze_label_distribution(train_data['y'])

        # 3. è¨ˆç®—é¡åˆ¥æ¬Šé‡
        class_weights_auto = compute_class_weights(train_data['y'], method='auto')

        print(f"\nå»ºè­°åœ¨é…ç½®ä¸­ä½¿ç”¨:")
        print(f"```yaml")
        print(f"loss:")
        print(f"  class_weights: 'auto'  # PyTorch è‡ªå‹•è¨ˆç®—")
        print(f"  # æˆ–æ‰‹å‹•æŒ‡å®š:")
        print(f"  # manual_weights: {class_weights_auto.numpy().tolist()}")
        print(f"```")

        # 4. é©—è­‰æå¤±è¨ˆç®—
        verify_loss_computation()

        # 5. æª¢æŸ¥æ¨£æœ¬æ¬Šé‡ï¼ˆå¦‚æœå•Ÿç”¨ï¼‰
        check_sample_weights(train_data, use_weights=False)

        print("\n" + "="*60)
        print("âœ… é©—è­‰å®Œæˆï¼")
        print("="*60)

        print("\né—œéµç™¼ç¾:")
        print("  1. âœ… é¡åˆ¥æ¬Šé‡è¨ˆç®—æ­£ç¢º")
        print("  2. âœ… æå¤±å‡½æ•¸æ­£ç¢ºæ‡‰ç”¨æ¬Šé‡")
        print("  3. âœ… æ¨£æœ¬æ¬Šé‡å·²ç¦ç”¨ï¼ˆé¿å…å•é¡Œï¼‰")

        print("\nä¸‹ä¸€æ­¥å»ºè­°:")
        print("  1. å¦‚æœé¡åˆ¥ä¸å¹³è¡¡ >3:1ï¼Œä¿æŒ class_weights='auto'")
        print("  2. è€ƒæ…®ä½¿ç”¨ Focal Loss æ›¿ä»£ CE Loss")
        print("  3. å¢åŠ æ¨¡å‹å®¹é‡ï¼ˆlstm_hidden: 48â†’64ï¼‰")
        print("  4. å¯¦ä½œæ”¹é€²ç‰ˆ DeepLOBï¼ˆLayerNorm + Attentionï¼‰")

        return 0

    except Exception as e:
        print(f"\nâŒ éŒ¯èª¤: {e}")
        import traceback
        traceback.print_exc()
        return 1

if __name__ == "__main__":
    sys.exit(main())
