#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
verify_no_cross_file.py - é©—è­‰ V7.1 ä¿®å¾©ï¼šæ»‘å‹•çª—å£ä¸è·¨æ–‡ä»¶

ä½¿ç”¨æ–¹å¼ï¼š
    python scripts/verify_no_cross_file.py --data-dir data/processed_v7_test/npz

æª¢æŸ¥é …ç›®ï¼š
    1. æ¨£æœ¬æ•¸é‡æ˜¯å¦åˆç†
    2. æ¨™ç±¤åˆ†å¸ƒæ˜¯å¦æ­£å¸¸
    3. stock_ids æ˜¯å¦æœ‰ç•°å¸¸
"""
import os
import argparse
import numpy as np
import json
from collections import Counter


def main():
    parser = argparse.ArgumentParser(description='é©—è­‰ V7.1 ä¿®å¾©')
    parser.add_argument('--data-dir', type=str, required=True, help='NPZ ç›®éŒ„')
    args = parser.parse_args()

    npz_dir = args.data_dir

    # æª¢æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    train_path = os.path.join(npz_dir, 'stock_embedding_train.npz')
    val_path = os.path.join(npz_dir, 'stock_embedding_val.npz')
    test_path = os.path.join(npz_dir, 'stock_embedding_test.npz')
    meta_path = os.path.join(npz_dir, 'normalization_meta.json')

    if not os.path.exists(train_path):
        print(f"âŒ æ‰¾ä¸åˆ°è¨“ç·´æ•¸æ“š: {train_path}")
        return

    print("=" * 80)
    print("V7.1 é©—è­‰å ±å‘Šï¼šæ»‘å‹•çª—å£ä¸è·¨æ–‡ä»¶")
    print("=" * 80)

    # è¼‰å…¥æ•¸æ“š
    train = np.load(train_path)
    val = np.load(val_path)
    test = np.load(test_path)

    # 1. æ¨£æœ¬æ•¸é‡æª¢æŸ¥
    print("\nã€1. æ¨£æœ¬æ•¸é‡æª¢æŸ¥ã€‘")
    print(f"Train: {len(train['X']):,} æ¨£æœ¬")
    print(f"Val:   {len(val['X']):,} æ¨£æœ¬")
    print(f"Test:  {len(test['X']):,} æ¨£æœ¬")
    print(f"Total: {len(train['X']) + len(val['X']) + len(test['X']):,} æ¨£æœ¬")

    # 2. æ•¸æ“šå½¢ç‹€æª¢æŸ¥
    print("\nã€2. æ•¸æ“šå½¢ç‹€æª¢æŸ¥ã€‘")
    print(f"Train X shape: {train['X'].shape}")  # (N, 100, 20)
    print(f"Train y shape: {train['y'].shape}")  # (N,)
    print(f"Window size: {train['X'].shape[1]} timesteps")
    print(f"Features: {train['X'].shape[2]} dimensions")

    # 3. æ¨™ç±¤åˆ†å¸ƒæª¢æŸ¥
    print("\nã€3. æ¨™ç±¤åˆ†å¸ƒæª¢æŸ¥ã€‘")
    all_labels = np.concatenate([train['y'], val['y'], test['y']])
    label_counts = Counter(all_labels)
    total_samples = len(all_labels)

    print(f"æ¨™ç±¤ 0 (ä¸‹è·Œ): {label_counts[0]:,} ({label_counts[0]/total_samples*100:.2f}%)")
    print(f"æ¨™ç±¤ 1 (æŒå¹³): {label_counts[1]:,} ({label_counts[1]/total_samples*100:.2f}%)")
    print(f"æ¨™ç±¤ 2 (ä¸Šæ¼²): {label_counts[2]:,} ({label_counts[2]/total_samples*100:.2f}%)")

    # æª¢æŸ¥æ˜¯å¦æ¥è¿‘ç›®æ¨™åˆ†å¸ƒ 30/40/30
    target_dist = [0.30, 0.40, 0.30]
    actual_dist = [label_counts[i]/total_samples for i in range(3)]
    print(f"\nç›®æ¨™åˆ†å¸ƒ: {target_dist}")
    print(f"å¯¦éš›åˆ†å¸ƒ: {[f'{d:.3f}' for d in actual_dist]}")

    # 4. Stock IDs æª¢æŸ¥
    print("\nã€4. Stock IDs æª¢æŸ¥ã€‘")
    if 'stock_ids' in train:
        train_stocks = set(train['stock_ids'])
        val_stocks = set(val['stock_ids'])
        test_stocks = set(test['stock_ids'])

        print(f"Train è‚¡ç¥¨æ•¸: {len(train_stocks)}")
        print(f"Val è‚¡ç¥¨æ•¸:   {len(val_stocks)}")
        print(f"Test è‚¡ç¥¨æ•¸:  {len(test_stocks)}")

        # æª¢æŸ¥æ˜¯å¦æœ‰é‡ç–Šï¼ˆæ‡‰è©²æ²’æœ‰ï¼‰
        overlap_train_val = train_stocks & val_stocks
        overlap_train_test = train_stocks & test_stocks
        overlap_val_test = val_stocks & test_stocks

        if overlap_train_val or overlap_train_test or overlap_val_test:
            print("\nâš ï¸ è­¦å‘Šï¼šç™¼ç¾æ•¸æ“šé›†é‡ç–Šï¼")
            print(f"Train-Val é‡ç–Š: {len(overlap_train_val)} æª”")
            print(f"Train-Test é‡ç–Š: {len(overlap_train_test)} æª”")
            print(f"Val-Test é‡ç–Š: {len(overlap_val_test)} æª”")
        else:
            print("\nâœ… æ•¸æ“šé›†ç„¡é‡ç–Šï¼ˆæ­£ç¢ºï¼‰")

    # 5. æ¬Šé‡æª¢æŸ¥ï¼ˆå¦‚æœæœ‰ï¼‰
    print("\nã€5. æ¬Šé‡æª¢æŸ¥ã€‘")
    if 'weights' in train:
        weights = train['weights']
        print(f"æ¬Šé‡ç¯„åœ: [{weights.min():.3f}, {weights.max():.3f}]")
        print(f"æ¬Šé‡å¹³å‡: {weights.mean():.3f}")
        print(f"æ¬Šé‡æ¨™æº–å·®: {weights.std():.3f}")
    else:
        print("âš ï¸ ç„¡æ¬Šé‡æ•¸æ“š")

    # 6. Metadata æª¢æŸ¥
    print("\nã€6. Metadata æª¢æŸ¥ã€‘")
    if os.path.exists(meta_path):
        with open(meta_path, 'r', encoding='utf-8') as f:
            meta = json.load(f)

        print(f"ç‰ˆæœ¬: {meta.get('version', 'N/A')}")
        print(f"ç”Ÿæˆæ™‚é–“: {meta.get('timestamp', 'N/A')}")

        if 'label_distribution' in meta:
            print(f"\næ¨™ç±¤åˆ†å¸ƒï¼ˆä¾†è‡ª metadataï¼‰:")
            for label, dist in meta['label_distribution'].items():
                print(f"  {label}: {dist}")
    else:
        print("âš ï¸ æ‰¾ä¸åˆ° normalization_meta.json")

    # 7. åƒ¹æ ¼å’Œæˆäº¤é‡æª¢æŸ¥ï¼ˆV7.1 æ–°å¢ï¼‰
    print("\nã€7. åƒ¹æ ¼å’Œæˆäº¤é‡æª¢æŸ¥ã€‘")
    if 'prices' in train:
        print(f"âœ… Train åŒ…å«åƒ¹æ ¼æ•¸æ“š: {train['prices'].shape}")
        print(f"   åƒ¹æ ¼ç¯„åœ: [{train['prices'].min():.2f}, {train['prices'].max():.2f}]")
    else:
        print("âš ï¸ Train ç„¡åƒ¹æ ¼æ•¸æ“š")

    if 'volumes' in train:
        print(f"âœ… Train åŒ…å«æˆäº¤é‡æ•¸æ“š: {train['volumes'].shape}")
    else:
        print("âš ï¸ Train ç„¡æˆäº¤é‡æ•¸æ“š")

    # 8. ç¸½çµ
    print("\n" + "=" * 80)
    print("ã€ç¸½çµã€‘")
    print("=" * 80)

    issues = []

    # æª¢æŸ¥æ¨£æœ¬æ•¸é‡æ˜¯å¦å¤ªå°‘
    if total_samples < 100000:
        issues.append(f"âš ï¸ æ¨£æœ¬æ•¸é‡åå°‘ ({total_samples:,} < 100,000)")
    else:
        print(f"âœ… æ¨£æœ¬æ•¸é‡å……è¶³ ({total_samples:,} æ¨£æœ¬)")

    # æª¢æŸ¥æ¨™ç±¤åˆ†å¸ƒ
    if abs(actual_dist[1] - 0.40) > 0.10:  # æŒå¹³é¡åå·®è¶…é 10%
        issues.append(f"âš ï¸ æ¨™ç±¤åˆ†å¸ƒåé›¢ç›®æ¨™ï¼ˆæŒå¹³é¡: {actual_dist[1]:.2f} vs ç›®æ¨™ 0.40ï¼‰")
    else:
        print(f"âœ… æ¨™ç±¤åˆ†å¸ƒæ¥è¿‘ç›®æ¨™")

    # æª¢æŸ¥çª—å£å¤§å°
    if train['X'].shape[1] != 100:
        issues.append(f"âš ï¸ çª—å£å¤§å°ç•°å¸¸ ({train['X'].shape[1]} != 100)")
    else:
        print(f"âœ… çª—å£å¤§å°æ­£ç¢º (100 timesteps)")

    # è¼¸å‡ºå•é¡Œ
    if issues:
        print(f"\nç™¼ç¾ {len(issues)} å€‹å•é¡Œ:")
        for issue in issues:
            print(f"  {issue}")
    else:
        print(f"\nğŸ‰ æ‰€æœ‰æª¢æŸ¥é€šéï¼")

    print("\n" + "=" * 80)


if __name__ == '__main__':
    main()
