# -*- coding: utf-8 -*-
"""
preprocess_single_day.py - å–®æª”é€æ—¥é è™•ç†è…³æœ¬ï¼ˆåƒ…ä½¿ç”¨ Trend Stable æ¨™ç±¤ï¼‰
=============================================================================
ã€æ›´æ–°æ—¥æœŸã€‘2025-10-23
ã€ç‰ˆæœ¬èªªæ˜ã€‘v2.1 - ç°¡åŒ–ç‰ˆï¼ˆåƒ…ä¿ç•™ Trend Stable æ¨™ç±¤æ–¹æ³•ï¼‰

åŠŸèƒ½ï¼š
  1. è®€å–å–®ä¸€å¤©çš„ TXT æª”æ¡ˆ
  2. è§£æã€æ¸…æ´—ã€èšåˆï¼ˆç¹¼æ‰¿ V5 é‚è¼¯ï¼‰
  3. è¨ˆç®—æ¯å€‹ symbol çš„æ—¥å…§çµ±è¨ˆ
  4. ã€æ ¸å¿ƒã€‘å‹•æ…‹æ±ºå®šç•¶å¤©çš„éæ¿¾é–¾å€¼ï¼ˆåŸºæ–¼ç›®æ¨™æ¨™ç±¤åˆ†å¸ƒï¼‰
  5. ã€æ¨™ç±¤ã€‘åƒ…ä½¿ç”¨ Trend Stable æ¨™ç±¤æ–¹æ³•ï¼ˆæ—¥å…§æ³¢æ®µäº¤æ˜“ï¼‰
  6. æ‡‰ç”¨éæ¿¾ä¸¦ä¿å­˜ç‚ºä¸­é–“æ ¼å¼ï¼ˆNPZï¼‰
  7. ç”Ÿæˆç•¶å¤©æ‘˜è¦å ±å‘Š

æ¨™ç±¤æ–¹æ³•ï¼š
  âœ… Trend Stable - ç©©å®šè¶¨å‹¢æ¨™ç±¤ï¼ˆæ¨è–¦ï¼Œé©åˆæ—¥å…§æ³¢æ®µäº¤æ˜“ï¼‰
  âŒ Triple-Barrier - å·²æ£„ç”¨ï¼ˆé«˜é »äº¤æ˜“ï¼‰
  âŒ Trend Adaptive - å·²æ£„ç”¨ï¼ˆéœ‡ç›ªå€é–“ä¸ç©©å®šï¼‰

è¼¸å‡ºï¼š
  - data/preprocessed_v5/daily/{date}/{symbol}.npz
  - data/preprocessed_v5/daily/{date}/summary.json

ä½¿ç”¨æ–¹å¼ï¼š
  python scripts/preprocess_single_day.py \
      --input ./data/temp/20250901.txt \
      --output-dir ./data/preprocessed_v5 \
      --config configs/config_pro_v5_ml_optimal.yaml

æ‰¹æ¬¡è™•ç†ï¼š
  bash scripts/batch_preprocess.sh

=============================================================================
ğŸ“š ç›¸é—œæŠ€è¡“æ–‡ä»¶ï¼ˆdocs/ ç›®éŒ„ï¼‰
=============================================================================

ã€å¿…è®€ã€‘æ ¸å¿ƒæ–‡æª”ï¼š
  1. docs/PROFESSIONAL_PACKAGES_MIGRATION.md
     â†’ å°ˆæ¥­é‡‘èå·¥ç¨‹å¥—ä»¶é·ç§»æŒ‡å—ï¼ˆæœ¬è…³æœ¬ä½¿ç”¨çš„å¥—ä»¶èªªæ˜ï¼‰
     â†’ åŒ…å«ï¼šæ€§èƒ½å°æ¯”ã€API ä½¿ç”¨ã€æ¸¬è©¦çµæœ

  2. docs/V6_TWO_STAGE_PIPELINE_GUIDE.md
     â†’ V6 é›™éšæ®µè³‡æ–™è™•ç†æµç¨‹æŒ‡å—
     â†’ æœ¬è…³æœ¬æ˜¯ã€éšæ®µ1ï¼šé è™•ç†ã€‘

  3. docs/PREPROCESSED_DATA_SPECIFICATION.md
     â†’ é è™•ç†æ•¸æ“šæ ¼å¼è¦ç¯„
     â†’ NPZ æª”æ¡ˆçµæ§‹ã€metadata èªªæ˜

ã€åƒè€ƒã€‘é€²éšæ–‡æª”ï¼š
  4. docs/LABEL_PREVIEW_GUIDE.md
     â†’ æ¨™ç±¤é è¦½åŠŸèƒ½ä½¿ç”¨æŒ‡å—
     â†’ Triple-Barrier åƒæ•¸èª¿æ•´

  5. docs/V5_Pro_NoMLFinLab_Guide.md
     â†’ V5 å°ˆæ¥­ç‰ˆæŠ€è¡“è¦ç¯„ï¼ˆåŸå§‹è¨­è¨ˆæ–‡æª”ï¼‰

ã€é…ç½®ã€‘ç›¸é—œé…ç½®ï¼š
  - configs/config_pro_v5_ml_optimal.yaml
    â†’ æœ¬è…³æœ¬ä½¿ç”¨çš„é…ç½®æª”æ¡ˆ

ã€å·¥å…·ã€‘æ¨™ç±¤æŸ¥çœ‹å™¨ï¼š
  - label_viewer/app_preprocessed.py
    â†’ é è™•ç†æ•¸æ“šè¦–è¦ºåŒ–å·¥å…·

=============================================================================
ğŸ”§ å°ˆæ¥­å¥—ä»¶ä¾è³´ï¼ˆsrc/utils/financial_engineering.pyï¼‰
=============================================================================

æœ¬è…³æœ¬ä½¿ç”¨ä»¥ä¸‹å°ˆæ¥­é‡‘èå·¥ç¨‹å‡½æ•¸ï¼š
  - ewma_volatility_professional()      â†’ Pandas å„ªåŒ– EWMAï¼ˆ100x åŠ é€Ÿï¼‰
  - triple_barrier_labels_professional() â†’ NumPy å‘é‡åŒ– TBï¼ˆ10x åŠ é€Ÿï¼‰
  - compute_sample_weights_professional()â†’ Sklearn é¡åˆ¥å¹³è¡¡

æŠ€è¡“ç´°ç¯€è«‹åƒé–±ï¼š
  - src/utils/financial_engineering.pyï¼ˆå‡½æ•¸å¯¦ç¾ï¼‰
  - docs/PROFESSIONAL_PACKAGES_MIGRATION.mdï¼ˆé·ç§»æŒ‡å—ï¼‰

=============================================================================

ç‰ˆæœ¬ï¼šv2.0
æ›´æ–°ï¼š2025-10-23
è®Šæ›´ï¼šé·ç§»åˆ°å°ˆæ¥­é‡‘èå·¥ç¨‹å¥—ä»¶å¯¦ç¾
"""

import os
import re
import json
import argparse
import logging
import warnings
from collections import defaultdict
from datetime import datetime
from typing import List, Tuple, Dict, Any, Optional

import numpy as np
import pandas as pd
import sys
from pathlib import Path

# æ·»åŠ é …ç›®æ ¹ç›®éŒ„åˆ°è·¯å¾‘
project_root = Path(__file__).parent.parent
if str(project_root) not in sys.path:
    sys.path.insert(0, str(project_root))

from src.utils.yaml_manager import YAMLManager

# ã€æ–°å¢ã€‘å°å…¥å°ˆæ¥­é‡‘èå·¥ç¨‹å‡½æ•¸åº«ï¼ˆ2025-10-23ï¼‰
# ã€2025-10-23 æ›´æ–°ã€‘åƒ…ä¿ç•™ Trend Stable ç›¸é—œå‡½æ•¸
from src.utils.financial_engineering import (
    ewma_volatility_professional,
    # triple_barrier_labels_professional,  # å·²æ£„ç”¨ï¼šåƒ…ä¿ç•™ Trend Stable
    # trend_labels_adaptive,               # å·²æ£„ç”¨ï¼šåƒ…ä¿ç•™ Trend Stable
    trend_labels_stable,
    compute_sample_weights_professional
)

# è¨­å®šæ—¥èªŒ
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)

warnings.filterwarnings('ignore', category=FutureWarning)
warnings.filterwarnings('ignore', category=RuntimeWarning)

# å›ºå®šå¸¸æ•¸ï¼ˆç¹¼æ‰¿è‡ª V5ï¼‰
AGG_FACTOR = 10
SEQ_LEN = 100
TRADING_START = 90000
TRADING_END = 133000

# æ¬„ä½ç´¢å¼•
IDX_REF = 3
IDX_UPPER = 4
IDX_LOWER = 5
IDX_LASTPRICE = 9
IDX_LASTVOL = 10
IDX_TV = 11
IDX_TIME = 32
IDX_TRIAL = 33

# äº”æª”åƒ¹é‡ç´¢å¼•
BID_P_IDX = [12, 14, 16, 18, 20]
BID_Q_IDX = [13, 15, 17, 19, 21]
ASK_P_IDX = [22, 24, 26, 28, 30]
ASK_Q_IDX = [23, 25, 27, 29, 31]

# çµ±è¨ˆè®Šæ•¸
stats = {
    "total_raw_events": 0,
    "cleaned_events": 0,
    "aggregated_points": 0,
    "symbols_processed": 0,
    "symbols_passed_filter": 0,
    "symbols_filtered_out": 0
}


# ============================================================
# ç¹¼æ‰¿è‡ª V5 çš„å·¥å…·å‡½æ•¸
# ============================================================

def hhmmss_to_int(s: str) -> int:
    s = s.strip()
    if not s.isdigit():
        return -1
    return int(s)


def to_float(x: str, default=0.0) -> float:
    try:
        return float(x)
    except:
        return default


def is_in_trading_window(t: int) -> bool:
    return TRADING_START <= t <= TRADING_END


def spread_ok(bid1: float, ask1: float) -> bool:
    if bid1 <= 0 or ask1 <= 0:
        return False
    if bid1 >= ask1:
        return False
    mid = 0.5 * (bid1 + ask1)
    return (ask1 - bid1) / max(mid, 1e-12) <= 0.05


def within_limits(px: float, lo: float, hi: float) -> bool:
    if lo > 0 and px < lo - 1e-12:
        return False
    if hi > 0 and px > hi + 1e-12:
        return False
    return True


def extract_date_from_filename(fp: str) -> str:
    """å¾æª”åæŠ“å–æ—¥æœŸ"""
    name = os.path.basename(fp)
    m = re.search(r"(20\d{6})", name)
    if m:
        return m.group(1)
    return name


def parse_line(raw: str) -> Tuple[str, int, Optional[Dict[str, Any]]]:
    """è§£æå–®è¡Œæ•¸æ“šï¼ˆç¹¼æ‰¿è‡ª V5ï¼‰"""
    global stats
    stats["total_raw_events"] += 1

    parts = raw.strip().split("||")
    if len(parts) < 34:
        return ("", -1, None)

    sym = parts[1].strip()

    try:
        t = hhmmss_to_int(parts[IDX_TIME])
    except:
        t = -1

    # è©¦æ’®ç§»é™¤ï¼æ™‚é–“çª—æª¢æŸ¥
    if parts[IDX_TRIAL].strip() == "1":
        return (sym, t, None)
    if not is_in_trading_window(t):
        return (sym, t, None)

    # å–äº”æª”åƒ¹é‡
    bids_p = [to_float(parts[i], 0.0) for i in BID_P_IDX]
    bids_q = [to_float(parts[i], 0.0) for i in BID_Q_IDX]
    asks_p = [to_float(parts[i], 0.0) for i in ASK_P_IDX]
    asks_q = [to_float(parts[i], 0.0) for i in ASK_Q_IDX]

    bid1, ask1 = bids_p[0], asks_p[0]
    if not spread_ok(bid1, ask1):
        return (sym, t, None)

    # é›¶å€¼è™•ç†
    for p, q in zip(bids_p + asks_p, bids_q + asks_q):
        if p == 0.0 and q != 0.0:
            return (sym, t, None)

    ref = to_float(parts[IDX_REF], 0.0)
    upper = to_float(parts[IDX_UPPER], 0.0)
    lower = to_float(parts[IDX_LOWER], 0.0)
    last_px = to_float(parts[IDX_LASTPRICE], 0.0)
    tv = max(0, int(to_float(parts[IDX_TV], 0.0)))

    # åƒ¹æ ¼é™åˆ¶æª¢æŸ¥
    prices_to_check = [p for p in bids_p + asks_p if p > 0]
    if not all(within_limits(p, lower, upper) for p in prices_to_check):
        return (sym, t, None)

    # çµ„ 20 ç¶­ç‰¹å¾µ
    feat = np.array(bids_p + asks_p + bids_q + asks_q, dtype=np.float64)
    mid = 0.5 * (bid1 + ask1)

    rec = {
        "feat": feat,
        "mid": mid,
        "ref": ref,
        "upper": upper,
        "lower": lower,
        "last_px": last_px,
        "tv": tv,
        "raw": raw.strip()
    }

    stats["cleaned_events"] += 1
    return (sym, t, rec)


def dedup_by_timestamp_keep_last(rows: List[Tuple[int, Dict[str,Any]]]) -> List[Tuple[int, Dict[str,Any]]]:
    """æ™‚é–“æˆ³å»é‡ï¼Œä¿ç•™æœ€å¾Œä¸€ç­†"""
    if not rows:
        return rows

    dedup_dict = {}
    for idx, (t, r) in enumerate(rows):
        tv = r.get("tv", 0)
        key = (t, tv)
        dedup_dict[key] = (idx, r)

    result_with_idx = sorted(dedup_dict.values(), key=lambda x: x[0])
    result = [(rows[idx][0], r) for idx, r in result_with_idx]

    return result


def aggregate_to_1hz(
    seq: List[Tuple[int, Dict[str,Any]]],
    reducer: str = 'last',
    ffill_limit: int = 120
) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:
    """
    1Hz æ™‚é–“èšåˆï¼ˆç§’ç´šï¼‰

    Args:
        seq: [(timestamp_int, record)] å·²æ’åºçš„äº‹ä»¶åºåˆ—
        reducer: å¤šäº‹ä»¶èšåˆç­–ç•¥ {'last', 'median', 'vwap-mid'}
        ffill_limit: å‰å€¼å¡«è£œæœ€å¤§é–“éš”ï¼ˆç§’ï¼‰

    Returns:
        features: (T, 20) LOB ç‰¹å¾µ
        mids: (T,) ä¸­é–“åƒ¹
        bucket_event_count: (T,) æ¯ç§’äº‹ä»¶æ•¸
        bucket_mask: (T,) æ¨™è¨˜ {0: å–®äº‹ä»¶, 1: ffill, 2: ç¼ºå¤±, 3: å¤šäº‹ä»¶èšåˆ}
    """
    global stats

    if not seq:
        return (np.zeros((0, 20), dtype=np.float64),
                np.zeros((0,), dtype=np.float64),
                np.zeros((0,), dtype=np.int32),
                np.zeros((0,), dtype=np.int32))

    # è½‰æ›ç‚ºç§’ç´¢å¼•ï¼ˆå¾äº¤æ˜“é–‹å§‹ç®—èµ·ï¼‰
    # ä¾‹å¦‚ 90000 (09:00:00) â†’ idx 0, 90001 â†’ idx 1
    def time_to_sec_idx(t):
        """HHMMSS è½‰ç§’ç´¢å¼•"""
        hh = t // 10000
        mm = (t // 100) % 100
        ss = t % 100
        total_sec = hh * 3600 + mm * 60 + ss
        start_total_sec = 9 * 3600  # 09:00:00
        return total_sec - start_total_sec

    # è¨ˆç®—ç¸½ç§’æ•¸
    n_seconds = (13 * 3600 + 30 * 60) - (9 * 3600) + 1  # 09:00:00 ~ 13:30:00

    # åˆå§‹åŒ–æ¡¶ï¼ˆæ¯ç§’ä¸€å€‹ï¼‰
    buckets = [[] for _ in range(n_seconds)]

    # å°‡äº‹ä»¶åˆ†é…åˆ°æ¡¶
    for t, rec in seq:
        sec_idx = time_to_sec_idx(t)
        if 0 <= sec_idx < n_seconds:
            buckets[sec_idx].append(rec)

    # èšåˆæ¯å€‹æ¡¶
    features_list = []
    mids_list = []
    event_counts = []
    masks = []

    last_valid_feat = None
    last_valid_mid = None
    last_valid_idx = -1

    for sec_idx, bucket in enumerate(buckets):
        event_count = len(bucket)
        event_counts.append(event_count)

        if event_count == 0:
            # ç„¡äº‹ä»¶ï¼šæª¢æŸ¥æ˜¯å¦å¯ä»¥ ffill
            gap = sec_idx - last_valid_idx if last_valid_idx >= 0 else ffill_limit + 1

            if gap <= ffill_limit and last_valid_feat is not None:
                # ffill
                features_list.append(last_valid_feat)
                mids_list.append(last_valid_mid)
                masks.append(1)  # ffill
            else:
                # ç¼ºå¤±
                features_list.append(np.zeros(20, dtype=np.float64))
                mids_list.append(0.0)
                masks.append(2)  # missing

        elif event_count == 1:
            # å–®äº‹ä»¶ï¼šç›´æ¥å–
            rec = bucket[0]
            feat = rec['feat']
            mid = rec['mid']

            features_list.append(feat)
            mids_list.append(mid)
            masks.append(0)  # native-single

            last_valid_feat = feat
            last_valid_mid = mid
            last_valid_idx = sec_idx

        else:
            # å¤šäº‹ä»¶ï¼šæ ¹æ“š reducer èšåˆ
            if reducer == 'last':
                rec = bucket[-1]  # å–æœ€å¾Œä¸€ç­†
                feat = rec['feat']
                mid = rec['mid']

            elif reducer == 'median':
                # é€æ¬„ä¸­ä½æ•¸
                feats_array = np.array([r['feat'] for r in bucket])
                mids_array = np.array([r['mid'] for r in bucket])
                feat = np.median(feats_array, axis=0)
                mid = float(np.median(mids_array))

            elif reducer == 'vwap-mid':
                # VWAPï¼ˆè‹¥æœ‰æˆäº¤é‡ï¼‰
                has_volume = any(r.get('tv', 0) > 0 for r in bucket)

                if has_volume:
                    total_vol = sum(r.get('tv', 0) for r in bucket)
                    if total_vol > 0:
                        mid = sum(r['mid'] * r.get('tv', 0) for r in bucket) / total_vol
                        # ç‰¹å¾µä»ç”¨ lastï¼ˆVWAP åƒ…ç”¨æ–¼ midï¼‰
                        feat = bucket[-1]['feat']
                    else:
                        # å›é€€åˆ° last
                        rec = bucket[-1]
                        feat = rec['feat']
                        mid = rec['mid']
                else:
                    # ç„¡æˆäº¤é‡ï¼Œå›é€€åˆ° last
                    rec = bucket[-1]
                    feat = rec['feat']
                    mid = rec['mid']
            else:
                raise ValueError(f"Unknown reducer: {reducer}")

            features_list.append(feat)
            mids_list.append(mid)
            masks.append(3)  # multi-event aggregated

            last_valid_feat = feat
            last_valid_mid = mid
            last_valid_idx = sec_idx

    # ç§»é™¤é¦–å°¾é€£çºŒç¼ºå¤± + ç§»é™¤ä¸­é–“çš„ç¼ºå¤±æ¡¶ï¼ˆä¿®å¾© mids=0 å•é¡Œï¼‰
    # âš ï¸ ç­–ç•¥è®Šæ›´ï¼šä¸ä¿ç•™ mask=2 çš„ç¼ºå¤±æ¡¶ï¼Œé¿å…ç”¢ç”Ÿ mids=0
    # åŸå› ï¼šmids=0 æœƒå°è‡´ log(0) = -infï¼Œé€²è€Œå°è‡´ Triple-Barrier å¤±æ•—

    first_valid = -1
    last_valid = -1

    for i, mask in enumerate(masks):
        if mask != 2:  # éç¼ºå¤±
            if first_valid == -1:
                first_valid = i
            last_valid = i

    if first_valid == -1:
        # å…¨éƒ¨ç¼ºå¤±
        return (np.zeros((0, 20), dtype=np.float64),
                np.zeros((0,), dtype=np.float64),
                np.zeros((0,), dtype=np.int32),
                np.zeros((0,), dtype=np.int32))

    # æˆªå–æœ‰æ•ˆç¯„åœï¼ˆé¦–å°¾ï¼‰
    features_list = features_list[first_valid:last_valid+1]
    mids_list = mids_list[first_valid:last_valid+1]
    event_counts = event_counts[first_valid:last_valid+1]
    masks = masks[first_valid:last_valid+1]

    # é€²ä¸€æ­¥ç§»é™¤ä¸­é–“çš„ç¼ºå¤±æ¡¶ï¼ˆmask=2ï¼Œmids=0ï¼‰
    # ä¿ç•™ mask=0 (å–®äº‹ä»¶), mask=1 (ffill), mask=3 (å¤šäº‹ä»¶)
    valid_indices = [i for i, m in enumerate(masks) if m != 2]

    if len(valid_indices) == 0:
        return (np.zeros((0, 20), dtype=np.float64),
                np.zeros((0,), dtype=np.float64),
                np.zeros((0,), dtype=np.int32),
                np.zeros((0,), dtype=np.int32))

    features_list = [features_list[i] for i in valid_indices]
    mids_list = [mids_list[i] for i in valid_indices]
    event_counts = [event_counts[i] for i in valid_indices]
    masks = [masks[i] for i in valid_indices]

    features = np.stack(features_list, axis=0)
    mids = np.array(mids_list, dtype=np.float64)
    bucket_event_count = np.array(event_counts, dtype=np.int32)
    bucket_mask = np.array(masks, dtype=np.int32)

    # é©—è­‰ï¼šç¢ºä¿æ²’æœ‰ mids=0ï¼ˆé™¤éçœŸå¯¦åƒ¹æ ¼å°±æ˜¯ 0ï¼Œä½†é€™ä¸å¤ªå¯èƒ½ï¼‰
    if (mids == 0).any():
        logging.warning(f"è­¦å‘Šï¼šç™¼ç¾ {(mids == 0).sum()} å€‹ mids=0 çš„é»ï¼ˆå¯èƒ½æ˜¯æ•¸æ“šå•é¡Œï¼‰")
        # ç§»é™¤ mids=0 çš„é»
        valid_mids = mids > 0
        if valid_mids.sum() == 0:
            return (np.zeros((0, 20), dtype=np.float64),
                    np.zeros((0,), dtype=np.float64),
                    np.zeros((0,), dtype=np.int32),
                    np.zeros((0,), dtype=np.int32))

        features = features[valid_mids]
        mids = mids[valid_mids]
        bucket_event_count = bucket_event_count[valid_mids]
        bucket_mask = bucket_mask[valid_mids]

    stats["aggregated_points"] += len(mids)

    return features, mids, bucket_event_count, bucket_mask


def calculate_intraday_volatility(mids: np.ndarray, date: str, symbol: str) -> Optional[Dict[str, Any]]:
    """è¨ˆç®—ç•¶æ—¥éœ‡ç›ªçµ±è¨ˆ"""
    if mids.size == 0:
        return None

    open_price = mids[0]
    close_price = mids[-1]
    high_price = mids.max()
    low_price = mids.min()

    if open_price <= 0:
        return None

    range_pct = (high_price - low_price) / open_price
    return_pct = (close_price - open_price) / open_price

    return {
        "date": date,
        "symbol": symbol,
        "range_pct": float(range_pct),
        "return_pct": float(return_pct),
        "high": float(high_price),
        "low": float(low_price),
        "open": float(open_price),
        "close": float(close_price),
        "n_points": len(mids)
    }


# ============================================================
# æ¨™ç±¤è¨ˆç®—åŒ…è£å‡½æ•¸ï¼ˆèª¿ç”¨å°ˆæ¥­é‡‘èå·¥ç¨‹å¥—ä»¶ï¼‰
# ============================================================

def ewma_vol(close: pd.Series, halflife: int = 60) -> pd.Series:
    """EWMA æ³¢å‹•ç‡ä¼°è¨ˆ â†’ ewma_volatility_professional()"""
    return ewma_volatility_professional(close, halflife=halflife, min_periods=20)


# ============================================================
# å·²æ£„ç”¨ï¼šTriple-Barrier æ¨™ç±¤æ–¹æ³•ï¼ˆåƒ…ä¿ç•™ Trend Stableï¼‰
# ============================================================
# def tb_labels(close: pd.Series,
#               vol: pd.Series,
#               pt_mult: float = 2.0,
#               sl_mult: float = 2.0,
#               max_holding: int = 200,
#               min_return: float = 0.0001,
#               day_end_idx: Optional[int] = None) -> pd.DataFrame:
#     """Triple-Barrier æ¨™ç±¤ç”Ÿæˆ â†’ triple_barrier_labels_professional()"""
#     return triple_barrier_labels_professional(
#         close=close,
#         volatility=vol,
#         pt_multiplier=pt_mult,
#         sl_multiplier=sl_mult,
#         max_holding=max_holding,
#         min_return=min_return,
#         day_end_idx=day_end_idx
#     )


def trend_labels(close: pd.Series,
                 vol: pd.Series,
                 lookforward: int = 150,
                 vol_multiplier: float = 2.0,
                 use_stable: bool = True,  # é è¨­ä½¿ç”¨ç©©å®šç‰ˆ
                 hysteresis_ratio: float = 0.6,
                 smooth_window: int = 15,
                 min_trend_duration: int = 30) -> pd.Series:
    """
    è¶¨å‹¢æ¨™ç±¤ç”Ÿæˆï¼ˆåƒ…ä½¿ç”¨ç©©å®šç‰ˆ Trend Stableï¼‰

    ã€2025-10-23 æ›´æ–°ã€‘
    - åƒ…ä¿ç•™ Trend Stable æ–¹æ³•ï¼ˆuse_stable å›ºå®šç‚º Trueï¼‰
    - å·²æ£„ç”¨ï¼šTriple-Barrier å’Œ Trend Adaptive

    Args:
        close: æ”¶ç›¤åƒ¹åºåˆ—
        vol: æ³¢å‹•ç‡åºåˆ—
        lookforward: å‰ç»çª—å£ï¼ˆç§’ï¼‰
        vol_multiplier: è¶¨å‹¢é–€æª»å€æ•¸
        use_stable: å›ºå®šç‚º Trueï¼ˆåƒ…ä¿ç•™ç©©å®šç‰ˆï¼‰
        hysteresis_ratio: é€€å‡ºé–€æª»æ¯”ä¾‹ï¼ˆ0.6 = é€€å‡ºé–€æª»ç‚ºé€²å…¥çš„60%ï¼‰
        smooth_window: å¤šæ•¸ç¥¨å¹³æ»‘çª—å£ï¼ˆç§’ï¼Œå¥‡æ•¸ï¼‰
        min_trend_duration: æœ€çŸ­è¶¨å‹¢æŒçºŒæ™‚é–“ï¼ˆç§’ï¼‰

    Returns:
        æ¨™ç±¤åºåˆ—ï¼ˆ-1: Down, 0: Neutral, 1: Upï¼‰
    """
    # åƒ…ä½¿ç”¨ç©©å®šç‰ˆï¼šæ¸›å°‘éœ‡ç›ªå€é–“çš„é »ç¹ç¿»è½‰
    return trend_labels_stable(
        close=close,
        volatility=vol,
        lookforward=lookforward,
        vol_multiplier=vol_multiplier,
        hysteresis_ratio=hysteresis_ratio,
        smooth_window=smooth_window,
        min_trend_duration=min_trend_duration
    )

# ============================================================
# å·²æ£„ç”¨ï¼šTrend Adaptive æ–¹æ³•ï¼ˆéœ‡ç›ªå€é–“ä¸ç©©å®šï¼‰
# ============================================================
# def trend_labels_adaptive_wrapper(close: pd.Series,
#                                   vol: pd.Series,
#                                   lookforward: int = 150,
#                                   vol_multiplier: float = 2.0) -> pd.Series:
#     """Trend Adaptive æ¨™ç±¤ç”Ÿæˆï¼ˆå·²æ£„ç”¨ï¼‰"""
#     return trend_labels_adaptive(
#         close=close,
#         volatility=vol,
#         lookforward=lookforward,
#         vol_multiplier=vol_multiplier
#     )


def compute_label_preview(
    mids: np.ndarray,
    tb_config: Dict,
    return_labels: bool = False
) -> Optional[Dict[str, Any]]:
    """
    è¨ˆç®—æ¨™ç±¤åˆ†å¸ƒï¼ˆåƒ…ä½¿ç”¨ Trend Stable æ¨™ç±¤æ–¹æ³•ï¼‰

    ã€é‡è¦æ›´æ–° 2025-10-23ã€‘
    1. åƒ…ä¿ç•™ Trend Stable æ¨™ç±¤æ–¹æ³•
    2. å·²æ£„ç”¨ï¼šTriple-Barrier å’Œ Trend Adaptive
    3. ä½¿ç”¨å°ˆæ¥­é‡‘èå·¥ç¨‹å¥—ä»¶ï¼ˆpandas + NumPy + sklearnï¼‰

    Args:
        mids: ä¸­é–“åƒ¹åºåˆ—
        tb_config: æ¨™ç±¤é…ç½®ï¼ˆåƒ…è®€å– trend_labeling åƒæ•¸ï¼‰
        return_labels: æ˜¯å¦è¿”å›å®Œæ•´æ¨™ç±¤é™£åˆ—ï¼ˆé è¨­ Falseï¼Œåªè¿”å›çµ±è¨ˆï¼‰

    Returns:
        æ¨™ç±¤çµ±è¨ˆå­—å…¸ï¼ŒåŒ…å«ï¼š
        - label_counts: {-1: count, 0: count, 1: count}
        - label_dist: {-1: ratio, 0: ratio, 1: ratio}
        - total_labels: ç¸½æ¨™ç±¤æ•¸
        - labels_array: å®Œæ•´æ¨™ç±¤é™£åˆ—ï¼ˆåƒ…ç•¶ return_labels=Trueï¼‰
        - labeling_method: å›ºå®šç‚º 'trend_stable'
        - å¦‚æœè¨ˆç®—å¤±æ•—è¿”å› None
    """
    try:
        # è½‰ç‚º Series
        close = pd.Series(mids, name='close')

        # è¨ˆç®—æ³¢å‹•ç‡
        halflife = tb_config.get('ewma_halflife', 60)
        vol = ewma_vol(close, halflife=halflife)

        # ========== åƒ…ä½¿ç”¨ Trend Stable æ¨™ç±¤æ–¹æ³• ==========
        labeling_method = 'trend_stable'  # å›ºå®šä½¿ç”¨ Trend Stable

        trend_config = tb_config.get('trend_labeling', {})
        lookforward = trend_config.get('lookforward', 150)
        vol_multiplier = trend_config.get('vol_multiplier', 2.0)
        hysteresis_ratio = trend_config.get('hysteresis_ratio', 0.6)
        smooth_window = trend_config.get('smooth_window', 15)
        min_trend_duration = trend_config.get('min_trend_duration', 30)

        # è¨ˆç®—è¶¨å‹¢æ¨™ç±¤ï¼ˆTrend Stableï¼‰
        labels_series = trend_labels(
            close=close,
            vol=vol,
            lookforward=lookforward,
            vol_multiplier=vol_multiplier,
            use_stable=True,  # å›ºå®šä½¿ç”¨ç©©å®šç‰ˆ
            hysteresis_ratio=hysteresis_ratio,
            smooth_window=smooth_window,
            min_trend_duration=min_trend_duration
        )

        labels_array = labels_series.values

        # çµ±è¨ˆæ¨™ç±¤åˆ†å¸ƒ
        if len(labels_array) == 0:
            return None

        unique, counts = np.unique(labels_array, return_counts=True)
        total = len(labels_array)

        label_counts = {int(k): int(v) for k, v in zip(unique, counts)}
        label_dist = {int(k): float(v / total) for k, v in zip(unique, counts)}

        # è£œé½Šç¼ºå¤±çš„é¡åˆ¥ï¼ˆ-1, 0, 1ï¼‰
        for label_val in [-1, 0, 1]:
            if label_val not in label_counts:
                label_counts[label_val] = 0
                label_dist[label_val] = 0.0

        result = {
            'label_counts': label_counts,
            'label_dist': label_dist,
            'total_labels': total,
            'down_pct': label_dist.get(-1, 0.0),
            'neutral_pct': label_dist.get(0, 0.0),
            'up_pct': label_dist.get(1, 0.0),
            'labeling_method': labeling_method  # è¨˜éŒ„ä½¿ç”¨çš„æ¨™ç±¤æ–¹æ³•
        }

        # å¦‚æœéœ€è¦è¿”å›å®Œæ•´æ¨™ç±¤é™£åˆ—
        if return_labels:
            # å‰µå»ºèˆ‡ mids ç­‰é•·çš„æ¨™ç±¤é™£åˆ—ï¼ˆå¡«å…… NaN ç”¨æ–¼æœªè¨ˆç®—çš„ä½ç½®ï¼‰
            full_labels = np.full(len(mids), np.nan, dtype=np.float32)
            full_labels[:len(labels_array)] = labels_array
            result['labels_array'] = full_labels

        return result

    except Exception as e:
        logging.warning(f"æ¨™ç±¤é è¦½è¨ˆç®—å¤±æ•—: {e}")
        return None


# ============================================================
# æ¬Šé‡ç­–ç•¥è¨ˆç®—ï¼ˆå¤šç¨®ç­–ç•¥é å…ˆè¨ˆç®—ï¼‰
# ============================================================

def compute_all_weight_strategies(labels: np.ndarray) -> Dict[str, Dict]:
    """
    è¨ˆç®—é¡åˆ¥æ¬Šé‡ç­–ç•¥ï¼ˆæ“´å±•ç‰ˆ - æä¾› 5 ç¨®ç­–ç•¥ï¼‰

    ã€2025-10-23æ›´æ–°ã€‘æä¾›å¤šç¨®æ¬Šé‡ç­–ç•¥ä¾›è¨“ç·´æ™‚é¸æ“‡
    è©³ç´°ç­–ç•¥è«‹åƒè€ƒ src/utils/financial_engineering.py

    Args:
        labels: æ¨™ç±¤é™£åˆ— (-1, 0, 1, NaN)

    Returns:
        æ¬Šé‡ç­–ç•¥å­—å…¸ï¼ŒåŒ…å« 5 ç¨®ç­–ç•¥ï¼š
        - uniform: ç„¡æ¬Šé‡ï¼ˆå…¨éƒ¨ 1.0ï¼‰
        - balanced: sklearn æ¨™æº–å¹³è¡¡æ¬Šé‡
        - balanced_sqrt: å¹³è¡¡æ¬Šé‡çš„å¹³æ–¹æ ¹ï¼ˆè¼ƒæº«å’Œï¼‰
        - inverse_freq: åé »ç‡æ¬Šé‡ï¼ˆæ¥µç«¯å¹³è¡¡ï¼‰
        - focal_alpha: Focal Loss é¢¨æ ¼æ¬Šé‡ï¼ˆå¼·èª¿å°‘æ•¸é¡ï¼‰
    """
    from sklearn.utils.class_weight import compute_class_weight

    strategies = {}

    # éæ¿¾ NaN
    valid_labels = labels[~np.isnan(labels)]

    if len(valid_labels) == 0:
        return {}

    # è¨ˆç®—é¡åˆ¥é »ç‡
    unique_classes, counts = np.unique(valid_labels, return_counts=True)
    total = len(valid_labels)
    freqs = {int(cls): count / total for cls, count in zip(unique_classes, counts)}

    # ç¢ºä¿åŒ…å«æ‰€æœ‰é¡åˆ¥ï¼ˆ-1, 0, 1ï¼‰
    for label in [-1, 0, 1]:
        if label not in freqs:
            freqs[label] = 0.0

    # ========== ç­–ç•¥ 1: ç„¡æ¬Šé‡ ==========
    strategies['uniform'] = {
        'class_weights': {-1: 1.0, 0: 1.0, 1: 1.0},
        'description': 'No weighting (all classes equal)',
        'type': 'class_weight'
    }

    # ========== ç­–ç•¥ 2: sklearn æ¨™æº–å¹³è¡¡æ¬Šé‡ ==========
    class_weights_arr = compute_class_weight(
        'balanced',
        classes=unique_classes,
        y=valid_labels
    )
    class_weights_dict = dict(zip(unique_classes.astype(int), class_weights_arr))

    # è£œé½Šç¼ºå¤±é¡åˆ¥
    for label in [-1, 0, 1]:
        if label not in class_weights_dict:
            class_weights_dict[label] = 1.0

    strategies['balanced'] = {
        'class_weights': class_weights_dict,
        'description': 'Sklearn balanced weights (n_samples / (n_classes * n_samples_per_class))',
        'type': 'class_weight'
    }

    # ========== ç­–ç•¥ 3: å¹³è¡¡æ¬Šé‡çš„å¹³æ–¹æ ¹ï¼ˆè¼ƒæº«å’Œï¼‰==========
    balanced_sqrt = {
        label: np.sqrt(class_weights_dict[label])
        for label in [-1, 0, 1]
    }
    # æ­¸ä¸€åŒ–ï¼ˆå‡å€¼ = 1.0ï¼‰
    mean_weight = np.mean(list(balanced_sqrt.values()))
    balanced_sqrt = {k: v / mean_weight for k, v in balanced_sqrt.items()}

    strategies['balanced_sqrt'] = {
        'class_weights': balanced_sqrt,
        'description': 'Square root of balanced weights (milder than balanced)',
        'type': 'class_weight'
    }

    # ========== ç­–ç•¥ 4: åé »ç‡æ¬Šé‡ï¼ˆæ¥µç«¯å¹³è¡¡ï¼‰==========
    inverse_freq = {}
    for label in [-1, 0, 1]:
        if freqs[label] > 0:
            inverse_freq[label] = 1.0 / freqs[label]
        else:
            inverse_freq[label] = 1.0

    # æ­¸ä¸€åŒ–
    mean_weight = np.mean(list(inverse_freq.values()))
    inverse_freq = {k: v / mean_weight for k, v in inverse_freq.items()}

    strategies['inverse_freq'] = {
        'class_weights': inverse_freq,
        'description': 'Inverse frequency (1 / freq), extreme balancing',
        'type': 'class_weight'
    }

    # ========== ç­–ç•¥ 5: Focal Loss é¢¨æ ¼æ¬Šé‡ ==========
    # alpha = 1 - freqï¼ˆå°‘æ•¸é¡æ¬Šé‡æ›´é«˜ï¼‰
    focal_alpha = {label: 1.0 - freqs[label] for label in [-1, 0, 1]}
    # æ­¸ä¸€åŒ–
    mean_weight = np.mean(list(focal_alpha.values()))
    focal_alpha = {k: v / mean_weight for k, v in focal_alpha.items()}

    strategies['focal_alpha'] = {
        'class_weights': focal_alpha,
        'description': 'Focal Loss style (alpha = 1 - freq), emphasizes minority classes',
        'type': 'class_weight'
    }

    return strategies


# ============================================================
# æ ¸å¿ƒåŠŸèƒ½ï¼šå‹•æ…‹é–¾å€¼æ±ºç­–
# ============================================================

def estimate_label_distribution(stats_list: List[Dict], tb_config: Dict) -> Dict[str, float]:
    """
    ä¼°è¨ˆæ¨™ç±¤åˆ†å¸ƒï¼ˆç°¡åŒ–ç‰ˆï¼‰

    åŸºæ–¼å•Ÿç™¼å¼è¦å‰‡ï¼š
    - è‹¥ |return| < min_return â†’ neutral
    - è‹¥ return > 0 â†’ up
    - è‹¥ return < 0 â†’ down

    æ³¨æ„ï¼šé€™æ˜¯ç°¡åŒ–ä¼°è¨ˆï¼Œå¯¦éš›æ¨™ç±¤ç”± Triple-Barrier æ±ºå®š
    """
    down_count = 0
    neutral_count = 0
    up_count = 0

    min_return = tb_config.get('min_return', 0.0015)

    for s in stats_list:
        return_pct = s['return_pct']

        if abs(return_pct) < min_return:
            neutral_count += 1
        elif return_pct > 0:
            up_count += 1
        else:
            down_count += 1

    total = down_count + neutral_count + up_count

    if total == 0:
        return {'down': 0.33, 'neutral': 0.34, 'up': 0.33}

    return {
        'down': down_count / total,
        'neutral': neutral_count / total,
        'up': up_count / total
    }


def calculate_distribution_distance(pred: Dict[str, float], target: Dict[str, float]) -> float:
    """è¨ˆç®—åˆ†å¸ƒè·é›¢ï¼ˆå¹³æ–¹å·®ï¼‰"""
    return sum((pred[k] - target[k])**2 for k in target.keys())


def determine_adaptive_threshold(
    daily_stats: List[Dict],
    config: Dict,
    target_label_dist: Optional[Dict[str, float]] = None
) -> Tuple[float, str, Dict]:
    """
    å‹•æ…‹æ±ºå®šç•¶å¤©çš„éæ¿¾é–¾å€¼

    âš ï¸ è­¦å‘Šï¼šæ­¤å‡½æ•¸å­˜åœ¨ã€Œå¾Œè¦‹ä¹‹æ˜æ´©æ¼ã€(Hindsight Bias)
    ---------------------------------------------------------
    å•é¡Œï¼šä½¿ç”¨ç•¶æ—¥**æ”¶ç›¤å¾Œ**çš„å®Œæ•´çµ±è¨ˆé‡ä¾†æ±ºå®šéæ¿¾é–¾å€¼
    å½±éŸ¿ï¼šå¯¦ç›¤æ™‚ç„¡æ³•åœ¨ç›¤ä¸­è¤‡è£½æ­¤æ±ºç­–ï¼ˆéœ€ç­‰æ”¶ç›¤æ‰çŸ¥é“åˆ†å¸ƒï¼‰

    å¯¦ç›¤æ›¿ä»£æ–¹æ¡ˆï¼š
    1. ä½¿ç”¨å‰ N å¤©çš„æ»¾å‹•çµ±è¨ˆé‡ï¼ˆå¦‚å‰ 5 æ—¥ P50ï¼‰
    2. å›ºå®šé–¾å€¼ç­–ç•¥ï¼ˆå¦‚å§‹çµ‚ä½¿ç”¨ P50 = 1.005ï¼‰
    3. ä½¿ç”¨ç›¤ä¸­å‰ 1 å°æ™‚çš„çµ±è¨ˆé‡ï¼ˆå¯åœ¨ç›¤ä¸­ç²å¾—ï¼‰

    ç•¶å‰åƒ…ç”¨æ–¼**é›¢ç·šå›æ¸¬**ï¼Œéœ€æ”¹é€²å¾Œæ‰å¯ç”¨æ–¼å¯¦ç›¤ã€‚
    ---------------------------------------------------------

    ç­–ç•¥ï¼š
    1. è¨ˆç®—ç•¶å¤©æ³¢å‹•ç‡åˆ†ä½æ•¸ï¼ˆP10, P25, P50, P75ï¼‰
    2. å°æ¯å€‹å€™é¸é–¾å€¼ï¼Œæ¨¡æ“¬éæ¿¾å¾Œçš„æ¨™ç±¤åˆ†å¸ƒ
    3. é¸æ“‡æœ€æ¥è¿‘ç›®æ¨™åˆ†å¸ƒçš„é–¾å€¼

    Args:
        daily_stats: ç•¶å¤©æ‰€æœ‰ symbol çš„çµ±è¨ˆ
        config: é…ç½®åƒæ•¸
        target_label_dist: ç›®æ¨™æ¨™ç±¤åˆ†å¸ƒï¼Œé è¨­ {'down': 0.30, 'neutral': 0.40, 'up': 0.30}

    Returns:
        (threshold, method_name, predicted_dist)
    """
    if target_label_dist is None:
        target_label_dist = {'down': 0.30, 'neutral': 0.40, 'up': 0.30}

    range_values = [s['range_pct'] for s in daily_stats]

    if len(range_values) == 0:
        logging.warning("ç•¶å¤©ç„¡æœ‰æ•ˆæ•¸æ“šï¼Œä½¿ç”¨é è¨­é–¾å€¼ 0.005")
        return 0.005, "default", target_label_dist

    # å€™é¸é–¾å€¼ï¼ˆåŸºæ–¼åˆ†ä½æ•¸ + çµ•å°å€¼ï¼‰
    # ä¿®æ”¹ï¼šå¢åŠ æ›´å¤šç´°ç²’åº¦å€™é¸ï¼Œå¹³è¡¡æ•¸æ“šä¿ç•™èˆ‡æ¨™ç±¤è³ªé‡
    candidates = {
        'P10': np.percentile(range_values, 10),
        'P15': np.percentile(range_values, 15),
        'P20': np.percentile(range_values, 20),
        'P25': np.percentile(range_values, 25),
        'P30': np.percentile(range_values, 30),
        'P50': np.percentile(range_values, 50),
        # çµ•å°å€¼å€™é¸ï¼ˆé©ç”¨æ–¼æ‰€æœ‰æ—¥æœŸï¼‰
        'fixed_0.5%': 0.005,
        'fixed_1.0%': 0.010,
        'fixed_1.5%': 0.015,
    }

    tb_config = config.get('triple_barrier', {})

    best_threshold = None
    best_method = None
    best_score = float('inf')
    best_predicted = None

    for name, threshold in candidates.items():
        # æ¨¡æ“¬éæ¿¾
        if threshold > 0:
            filtered_stats = [s for s in daily_stats if s['range_pct'] >= threshold]
        else:
            filtered_stats = daily_stats

        if len(filtered_stats) == 0:
            continue

        # ä¼°è¨ˆæ¨™ç±¤åˆ†å¸ƒ
        predicted_dist = estimate_label_distribution(filtered_stats, tb_config)

        # è¨ˆç®—è·é›¢
        score = calculate_distribution_distance(predicted_dist, target_label_dist)

        if score < best_score:
            best_score = score
            best_threshold = threshold
            best_method = name
            best_predicted = predicted_dist

    logging.info(f"é¸æ“‡é–¾å€¼: {best_method} = {best_threshold:.4f} (åˆ†æ•¸: {best_score:.4f})")
    logging.info(f"é æ¸¬æ¨™ç±¤åˆ†å¸ƒ: Down={best_predicted['down']:.1%}, "
                f"Neutral={best_predicted['neutral']:.1%}, Up={best_predicted['up']:.1%}")

    return best_threshold, best_method, best_predicted


# ============================================================
# ä¿å­˜èˆ‡å ±å‘Š
# ============================================================

def save_preprocessed_npz(
    output_dir: str,
    date: str,
    symbol: str,
    features: np.ndarray,
    mids: np.ndarray,
    bucket_event_count: np.ndarray,
    bucket_mask: np.ndarray,
    vol_stats: Dict,
    pass_filter: bool,
    filter_threshold: float,
    filter_method: str,
    label_preview: Optional[Dict] = None,
    labels: Optional[np.ndarray] = None
):
    """ä¿å­˜é è™•ç†å¾Œçš„ NPZ æª”æ¡ˆï¼ˆ1Hz ç‰ˆæœ¬ï¼‰"""
    day_dir = os.path.join(output_dir, "daily", date)
    os.makedirs(day_dir, exist_ok=True)

    # æº–å‚™ metadata
    metadata = {
        "symbol": symbol,
        "date": date,
        "n_points": int(features.shape[0]),

        # æ—¥å…§çµ±è¨ˆ
        "range_pct": float(vol_stats['range_pct']),
        "return_pct": float(vol_stats['return_pct']),
        "high": float(vol_stats['high']),
        "low": float(vol_stats['low']),
        "open": float(vol_stats['open']),
        "close": float(vol_stats['close']),

        # éæ¿¾è³‡è¨Š
        "pass_filter": bool(pass_filter),
        "filter_threshold": float(filter_threshold),
        "filter_method": filter_method,
        "filter_reason": None if pass_filter else "range_too_low",

        # è™•ç†è³‡è¨Š
        "processed_at": datetime.now().isoformat(),
        "raw_events": vol_stats.get('raw_events', 0),
        "aggregated_points": int(features.shape[0]),

        # 1Hz èšåˆè³‡è¨Š
        "sampling_mode": "time",
        "bucket_seconds": 1,
        "ffill_limit": 120,
        "agg_reducer": "last",
        "n_seconds": vol_stats.get('n_seconds', 0),
        "ffill_ratio": vol_stats.get('ffill_ratio', 0.0),
        "missing_ratio": vol_stats.get('missing_ratio', 0.0),
        "multi_event_ratio": vol_stats.get('multi_event_ratio', 0.0),
        "max_gap_sec": vol_stats.get('max_gap_sec', 0)
    }

    # ğŸ†• åŠ å…¥æ¨™ç±¤é è¦½è³‡è¨Š
    if label_preview is not None:
        metadata["label_preview"] = {
            "total_labels": label_preview['total_labels'],
            "down_count": label_preview['label_counts'].get(-1, 0),
            "neutral_count": label_preview['label_counts'].get(0, 0),
            "up_count": label_preview['label_counts'].get(1, 0),
            "down_pct": label_preview['down_pct'],
            "neutral_pct": label_preview['neutral_pct'],
            "up_pct": label_preview['up_pct']
        }

        # ğŸ†• å¦‚æœæœ‰æ¨™ç±¤é™£åˆ—ï¼Œè¨ˆç®—æ‰€æœ‰æ¬Šé‡ç­–ç•¥
        if 'labels_array' in label_preview and labels is not None:
            try:
                weight_strategies = compute_all_weight_strategies(labels)

                if weight_strategies:
                    # è½‰æ›ç‚º JSON å¯åºåˆ—åŒ–çš„æ ¼å¼
                    metadata["weight_strategies"] = {
                        name: {
                            'class_weights': {
                                str(k): float(v) for k, v in strategy.get('class_weights', {}).items()
                            },
                            'description': strategy.get('description', ''),
                            'type': strategy.get('type', 'class_weight')  # 'class_weight' æˆ– 'focal'
                        }
                        for name, strategy in weight_strategies.items()
                    }
                    logging.info(f"  è¨ˆç®—äº† {len(weight_strategies)} ç¨®æ¬Šé‡ç­–ç•¥")
                else:
                    metadata["weight_strategies"] = None
            except Exception as e:
                logging.warning(f"  æ¬Šé‡ç­–ç•¥è¨ˆç®—å¤±æ•—: {e}")
                metadata["weight_strategies"] = None
        else:
            metadata["weight_strategies"] = None
    else:
        metadata["label_preview"] = None
        metadata["weight_strategies"] = None

    # ä¿å­˜
    npz_path = os.path.join(day_dir, f"{symbol}.npz")

    # æº–å‚™ä¿å­˜çš„æ•¸æ“šå­—å…¸
    save_data = {
        'features': features.astype(np.float32),
        'mids': mids.astype(np.float64),
        'bucket_event_count': bucket_event_count.astype(np.int32),
        'bucket_mask': bucket_mask.astype(np.int32),
        'metadata': json.dumps(metadata, ensure_ascii=False)
    }

    # å¦‚æœæœ‰æ¨™ç±¤æ•¸æ“šï¼Œæ·»åŠ åˆ°ä¿å­˜å­—å…¸ä¸­
    if labels is not None:
        save_data['labels'] = labels.astype(np.float32)

    np.savez_compressed(npz_path, **save_data)

    return npz_path


def generate_daily_summary(
    output_dir: str,
    date: str,
    daily_stats: List[Dict],
    filter_threshold: float,
    filter_method: str,
    predicted_dist: Dict,
    symbols_passed: int,
    symbols_filtered: int,
    label_stats: Optional[Dict] = None
):
    """ç”Ÿæˆç•¶å¤©æ‘˜è¦å ±å‘Š"""
    day_dir = os.path.join(output_dir, "daily", date)

    range_values = [s['range_pct'] for s in daily_stats]

    summary = {
        "date": date,
        "total_symbols": len(daily_stats),
        "passed_filter": symbols_passed,
        "filtered_out": symbols_filtered,
        "filter_threshold": float(filter_threshold),
        "filter_method": filter_method,

        "volatility_distribution": {
            "min": float(np.min(range_values)) if range_values else 0.0,
            "max": float(np.max(range_values)) if range_values else 0.0,
            "mean": float(np.mean(range_values)) if range_values else 0.0,
            "P10": float(np.percentile(range_values, 10)) if range_values else 0.0,
            "P25": float(np.percentile(range_values, 25)) if range_values else 0.0,
            "P50": float(np.percentile(range_values, 50)) if range_values else 0.0,
            "P75": float(np.percentile(range_values, 75)) if range_values else 0.0,
        },

        "predicted_label_dist": predicted_dist,

        # ğŸ†• å¯¦éš›æ¨™ç±¤çµ±è¨ˆï¼ˆåŸºæ–¼ TB è¨ˆç®—ï¼‰
        "actual_label_stats": label_stats if label_stats else None,

        "top_volatile": sorted(
            [{"symbol": s['symbol'], "range_pct": s['range_pct']} for s in daily_stats],
            key=lambda x: x['range_pct'],
            reverse=True
        )[:10]
    }

    summary_path = os.path.join(day_dir, "summary.json")
    with open(summary_path, "w", encoding="utf-8") as f:
        json.dump(summary, f, ensure_ascii=False, indent=2)

    logging.info(f"âœ… ç•¶å¤©æ‘˜è¦å·²ä¿å­˜: {summary_path}")

    return summary_path


# ============================================================
# ä¸»è™•ç†æµç¨‹
# ============================================================

def process_single_day(txt_file: str, output_dir: str, config: Dict) -> Dict:
    """
    è™•ç†å–®ä¸€å¤©çš„ TXT æª”æ¡ˆ

    Returns:
        è™•ç†çµ±è¨ˆå­—å…¸
    """
    global stats

    # é‡ç½®çµ±è¨ˆ
    stats = {
        "total_raw_events": 0,
        "cleaned_events": 0,
        "aggregated_points": 0,
        "symbols_processed": 0,
        "symbols_passed_filter": 0,
        "symbols_filtered_out": 0
    }

    date = extract_date_from_filename(txt_file)

    logging.info(f"\n{'='*60}")
    logging.info(f"è™•ç†æ—¥æœŸ: {date}")
    logging.info(f"è¼¸å…¥æª”æ¡ˆ: {txt_file}")
    logging.info(f"{'='*60}\n")

    # Step 1: è®€å–ä¸¦è§£æ
    per_symbol_raw: Dict[str, List[Tuple[int, Dict[str,Any]]]] = defaultdict(list)

    try:
        with open(txt_file, "r", encoding="utf-8", errors="ignore") as f:
            for raw in f:
                sym, t, rec = parse_line(raw)
                if rec is None or sym == "" or t < 0:
                    continue
                per_symbol_raw[sym].append((t, rec))
    except Exception as e:
        logging.error(f"è®€å–æª”æ¡ˆå¤±æ•—: {e}")
        return stats

    logging.info(f"æ¸…æ´—å¾Œ: {stats['cleaned_events']:,} äº‹ä»¶ï¼Œå…± {len(per_symbol_raw)} å€‹è‚¡ç¥¨")

    # Step 2: èšåˆä¸¦è¨ˆç®—çµ±è¨ˆ
    symbol_data = {}  # {symbol: (features, mids, vol_stats)}
    daily_stats = []

    for sym, rows in per_symbol_raw.items():
        if not rows:
            continue

        # æ’åºã€å»é‡
        rows.sort(key=lambda x: x[0])
        rows = dedup_by_timestamp_keep_last(rows)

        # 1Hz èšåˆï¼ˆæ–°ç‰ˆï¼‰
        features, mids, bucket_event_count, bucket_mask = aggregate_to_1hz(
            rows,
            reducer='last',  # å¯é…ç½®ï¼š'last', 'median', 'vwap-mid'
            ffill_limit=60  # A.1: é™ä½è‡³ 60 ç§’ï¼ˆé¿å…é•·æœŸ ffill é€ æˆå‡è¨Šè™Ÿï¼‰
        )
        if features.shape[0] == 0:
            continue

        # è¨ˆç®—çµ±è¨ˆ
        vol_stats = calculate_intraday_volatility(mids, date, sym)
        if vol_stats is None:
            continue

        vol_stats['raw_events'] = len(rows)

        # æ–°å¢ï¼š1Hz èšåˆçµ±è¨ˆ
        vol_stats['n_seconds'] = len(mids)
        vol_stats['ffill_ratio'] = float((bucket_mask == 1).sum() / len(bucket_mask))
        vol_stats['missing_ratio'] = float((bucket_mask == 2).sum() / len(bucket_mask))
        vol_stats['multi_event_ratio'] = float((bucket_mask == 3).sum() / len(bucket_mask))
        vol_stats['max_gap_sec'] = int(np.max(np.diff(np.where(bucket_mask != 2)[0]))) if (bucket_mask != 2).sum() > 1 else 0

        symbol_data[sym] = (features, mids, bucket_event_count, bucket_mask, vol_stats)
        daily_stats.append(vol_stats)
        stats["symbols_processed"] += 1

    logging.info(f"èšåˆå¾Œ: {stats['aggregated_points']:,} æ™‚é–“é»ï¼Œ{stats['symbols_processed']} å€‹è‚¡ç¥¨")

    if not daily_stats:
        logging.warning("ç•¶å¤©ç„¡æœ‰æ•ˆæ•¸æ“šï¼")
        return stats

    # Step 3: å‹•æ…‹æ±ºå®šéæ¿¾é–¾å€¼
    filter_threshold, filter_method, predicted_dist = determine_adaptive_threshold(
        daily_stats,
        config
    )

    # Step 3.5: ğŸ†• è¨ˆç®—å¯¦éš›æ¨™ç±¤åˆ†å¸ƒï¼ˆTB é è¦½ï¼‰
    tb_config = config.get('triple_barrier', {})
    all_label_previews = []

    logging.info("\n" + "="*70)
    logging.info("è¨ˆç®—æ¨™ç±¤é è¦½ï¼ˆTriple-Barrierï¼‰")
    logging.info("="*70)

    # Step 4: æ‡‰ç”¨éæ¿¾ä¸¦ä¿å­˜
    for sym, (features, mids, bucket_event_count, bucket_mask, vol_stats) in symbol_data.items():
        pass_filter = vol_stats['range_pct'] >= filter_threshold

        if pass_filter:
            stats["symbols_passed_filter"] += 1
        else:
            stats["symbols_filtered_out"] += 1

        # ğŸ†• è¨ˆç®—æ¨™ç±¤é è¦½ï¼ˆåƒ…å°é€šééæ¿¾çš„è‚¡ç¥¨ï¼‰
        label_preview = None
        labels_array = None
        if pass_filter:
            label_preview = compute_label_preview(mids, tb_config, return_labels=True)
            if label_preview is not None:
                all_label_previews.append(label_preview)
                # æå–æ¨™ç±¤é™£åˆ—
                labels_array = label_preview.get('labels_array')

        # ä¿å­˜ï¼ˆç„¡è«–æ˜¯å¦é€šééæ¿¾ï¼Œéƒ½ä¿å­˜ï¼Œä½†æ¨™è¨˜ç‹€æ…‹ï¼‰
        save_preprocessed_npz(
            output_dir=output_dir,
            date=date,
            symbol=sym,
            features=features,
            mids=mids,
            bucket_event_count=bucket_event_count,
            bucket_mask=bucket_mask,
            vol_stats=vol_stats,
            pass_filter=pass_filter,
            filter_threshold=filter_threshold,
            filter_method=filter_method,
            label_preview=label_preview,
            labels=labels_array  # ğŸ†• å‚³å…¥æ¨™ç±¤é™£åˆ—
        )

    # ğŸ†• èšåˆæ‰€æœ‰æ¨™ç±¤çµ±è¨ˆ
    label_stats = None
    if all_label_previews:
        total_down = sum(lp['label_counts'].get(-1, 0) for lp in all_label_previews)
        total_neutral = sum(lp['label_counts'].get(0, 0) for lp in all_label_previews)
        total_up = sum(lp['label_counts'].get(1, 0) for lp in all_label_previews)
        total_all = total_down + total_neutral + total_up

        if total_all > 0:
            label_stats = {
                "total_labels": total_all,
                "down_count": total_down,
                "neutral_count": total_neutral,
                "up_count": total_up,
                "down_pct": total_down / total_all,
                "neutral_pct": total_neutral / total_all,
                "up_pct": total_up / total_all,
                "stocks_with_labels": len(all_label_previews)
            }
            logging.info(f"âœ… æ¨™ç±¤é è¦½çµ±è¨ˆï¼ˆ{len(all_label_previews)} æª”è‚¡ç¥¨ï¼‰:")
            logging.info(f"   Down: {total_down:,} ({label_stats['down_pct']:.1%})")
            logging.info(f"   Neutral: {total_neutral:,} ({label_stats['neutral_pct']:.1%})")
            logging.info(f"   Up: {total_up:,} ({label_stats['up_pct']:.1%})")

    # Step 5: ç”Ÿæˆæ‘˜è¦
    generate_daily_summary(
        output_dir=output_dir,
        date=date,
        daily_stats=daily_stats,
        filter_threshold=filter_threshold,
        filter_method=filter_method,
        predicted_dist=predicted_dist,
        symbols_passed=stats["symbols_passed_filter"],
        symbols_filtered=stats["symbols_filtered_out"],
        label_stats=label_stats
    )

    # è¼¸å‡ºçµ±è¨ˆ
    logging.info(f"\n{'='*60}")
    logging.info(f"è™•ç†å®Œæˆ: {date}")
    logging.info(f"{'='*60}")
    logging.info(f"ç¸½äº‹ä»¶æ•¸: {stats['total_raw_events']:,}")
    logging.info(f"æ¸…æ´—å¾Œ: {stats['cleaned_events']:,}")
    logging.info(f"èšåˆæ™‚é–“é»: {stats['aggregated_points']:,}")
    logging.info(f"è™•ç†è‚¡ç¥¨: {stats['symbols_processed']}")
    logging.info(f"é€šééæ¿¾: {stats['symbols_passed_filter']} ({stats['symbols_passed_filter']/stats['symbols_processed']*100:.1f}%)")
    logging.info(f"è¢«éæ¿¾: {stats['symbols_filtered_out']} ({stats['symbols_filtered_out']/stats['symbols_processed']*100:.1f}%)")
    logging.info(f"{'='*60}\n")

    return stats


def parse_args():
    p = argparse.ArgumentParser("preprocess_single_day", description="å–®æª”é€æ—¥é è™•ç†è…³æœ¬")
    p.add_argument("--input", required=True, help="è¼¸å…¥ TXT æª”æ¡ˆè·¯å¾‘")
    p.add_argument("--output-dir", default="./data/preprocessed_v5", help="è¼¸å‡ºç›®éŒ„")
    p.add_argument("--config", default="./configs/config_intraday_swing.yaml", help="é…ç½®æ–‡ä»¶")
    return p.parse_args()


def main():
    args = parse_args()

    # è¼‰å…¥é…ç½®
    if not os.path.exists(args.config):
        logging.error(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {args.config}")
        return 1

    yaml_manager = YAMLManager(args.config)
    config = yaml_manager.as_dict()

    # é©—è­‰è¼¸å…¥æª”æ¡ˆ
    if not os.path.exists(args.input):
        logging.error(f"è¼¸å…¥æª”æ¡ˆä¸å­˜åœ¨: {args.input}")
        return 1

    # å»ºç«‹è¼¸å‡ºç›®éŒ„
    os.makedirs(args.output_dir, exist_ok=True)

    # è™•ç†
    process_single_day(args.input, args.output_dir, config)

    return 0


if __name__ == "__main__":
    import sys
    sys.exit(main())
