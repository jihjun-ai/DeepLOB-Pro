"""TensorBoard æ•¸æ“šåˆ†æè…³æœ¬

æ­¤è…³æœ¬è‡ªå‹•å¾ TensorBoard æ—¥èªŒä¸­æå–é—œéµæŒ‡æ¨™ï¼Œç”Ÿæˆçµæ§‹åŒ–å ±å‘Šï¼Œ
æ–¹ä¾¿ AI åˆ†æè¨“ç·´ç‹€æ³ä¸¦æä¾›å„ªåŒ–å»ºè­°ã€‚

ä½¿ç”¨æ–¹æ³•:
    python scripts/analyze_tensorboard.py --logdir logs/sb3_deeplob/PPO_1
    python scripts/analyze_tensorboard.py --logdir logs/sb3_deeplob/PPO_1 --output results/analysis.json

è¼¸å‡º:
    - JSON æ ¼å¼çš„çµæ§‹åŒ–æ•¸æ“š
    - Markdown æ ¼å¼çš„å ±å‘Š
    - å¯è¦–åŒ–åœ–è¡¨ï¼ˆå¯é¸ï¼‰
"""

import argparse
import json
import os
import sys
from pathlib import Path
from typing import Dict, List, Any, Optional
import numpy as np
from datetime import datetime

# æ·»åŠ é …ç›®æ ¹ç›®éŒ„åˆ° Python è·¯å¾‘
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))


def parse_args():
    """è§£æå‘½ä»¤è¡Œåƒæ•¸"""
    parser = argparse.ArgumentParser(
        description="åˆ†æ TensorBoard è¨“ç·´æ—¥èªŒ",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¯„ä¾‹:
    # åˆ†æå–®å€‹è¨“ç·´æ—¥èªŒ
    python scripts/analyze_tensorboard.py --logdir logs/sb3_deeplob/PPO_1

    # åˆ†æä¸¦ä¿å­˜ JSON å ±å‘Š
    python scripts/analyze_tensorboard.py --logdir logs/sb3_deeplob/PPO_1 --output results/analysis.json

    # åˆ†æä¸¦ç”Ÿæˆ Markdown å ±å‘Š
    python scripts/analyze_tensorboard.py --logdir logs/sb3_deeplob/PPO_1 --format markdown

    # æ¯”è¼ƒå¤šå€‹å¯¦é©—
    python scripts/analyze_tensorboard.py --compare logs/sb3_deeplob/
        """
    )

    parser.add_argument(
        "--logdir",
        type=str,
        required=True,
        help="TensorBoard æ—¥èªŒç›®éŒ„è·¯å¾‘"
    )

    parser.add_argument(
        "--output",
        type=str,
        default=None,
        help="è¼¸å‡ºæ–‡ä»¶è·¯å¾‘ï¼ˆä¸æŒ‡å®šå‰‡è¼¸å‡ºåˆ°çµ‚ç«¯ï¼‰"
    )

    parser.add_argument(
        "--format",
        type=str,
        choices=["json", "markdown", "both"],
        default="both",
        help="è¼¸å‡ºæ ¼å¼: json, markdown, æˆ– bothï¼ˆé è¨­ï¼‰"
    )

    parser.add_argument(
        "--compare",
        action="store_true",
        help="æ¯”è¼ƒæ¨¡å¼ï¼šåˆ†æç›®éŒ„ä¸‹æ‰€æœ‰å¯¦é©—ä¸¦å°æ¯”"
    )

    parser.add_argument(
        "--verbose",
        action="store_true",
        help="é¡¯ç¤ºè©³ç´°ä¿¡æ¯"
    )

    parser.add_argument(
        "--sampling",
        type=str,
        choices=["segment", "inflection", "both"],
        default="both",
        help="æ¡æ¨£æ¨¡å¼: segmentï¼ˆåˆ†æ®µæ¡æ¨£ï¼‰, inflectionï¼ˆè½‰æŠ˜é»æ¡æ¨£ï¼‰, bothï¼ˆå…©è€…éƒ½åŒ…å«ï¼Œé è¨­ï¼‰"
    )

    parser.add_argument(
        "--segments",
        type=int,
        default=20,
        help="åˆ†æ®µæ¡æ¨£çš„æ®µæ•¸ï¼ˆé è¨­ 20ï¼‰"
    )

    parser.add_argument(
        "--inflection-sensitivity",
        type=float,
        default=0.1,
        help="è½‰æŠ˜é»æ•æ„Ÿåº¦ (0.01-1.0)ï¼Œè¶Šå°è¶Šæ•æ„Ÿï¼Œé è¨­ 0.1"
    )

    return parser.parse_args()


class TensorBoardAnalyzer:
    """TensorBoard æ—¥èªŒåˆ†æå™¨"""

    def __init__(self, logdir: str, verbose: bool = False,
                 sampling_mode: str = "both", num_segments: int = 20,
                 inflection_sensitivity: float = 0.1):
        """
        åˆå§‹åŒ–åˆ†æå™¨

        åƒæ•¸:
            logdir: TensorBoard æ—¥èªŒç›®éŒ„
            verbose: æ˜¯å¦é¡¯ç¤ºè©³ç´°ä¿¡æ¯
            sampling_mode: æ¡æ¨£æ¨¡å¼ ("segment", "inflection", "both")
            num_segments: åˆ†æ®µæ¡æ¨£çš„æ®µæ•¸
            inflection_sensitivity: è½‰æŠ˜é»æ•æ„Ÿåº¦
        """
        self.logdir = Path(logdir)
        self.verbose = verbose
        self.sampling_mode = sampling_mode
        self.num_segments = num_segments
        self.inflection_sensitivity = inflection_sensitivity
        self.data = {}
        self.summary = {}

    def load_data(self) -> bool:
        """è¼‰å…¥ TensorBoard æ•¸æ“š"""
        try:
            from tensorboard.backend.event_processing import event_accumulator

            if self.verbose:
                print(f"[LOAD] è¼‰å…¥æ—¥èªŒ: {self.logdir}")

            # å‰µå»ºäº‹ä»¶ç´¯åŠ å™¨
            ea = event_accumulator.EventAccumulator(
                str(self.logdir),
                size_guidance={
                    event_accumulator.SCALARS: 0,  # è¼‰å…¥æ‰€æœ‰æ¨™é‡æ•¸æ“š
                }
            )
            ea.Reload()

            # ç²å–æ‰€æœ‰æ¨™é‡æ¨™ç±¤
            tags = ea.Tags()['scalars']

            if self.verbose:
                print(f"[OK] æ‰¾åˆ° {len(tags)} å€‹æŒ‡æ¨™")

            # æå–æ¯å€‹æŒ‡æ¨™çš„æ•¸æ“š
            for tag in tags:
                events = ea.Scalars(tag)
                self.data[tag] = {
                    'steps': [e.step for e in events],
                    'values': [e.value for e in events],
                    'wall_times': [e.wall_time for e in events]
                }

            return True

        except ImportError:
            print("[ERROR] éœ€è¦å®‰è£ tensorboard")
            print("   è«‹åŸ·è¡Œ: pip install tensorboard")
            return False

        except Exception as e:
            print(f"[ERROR] è¼‰å…¥æ•¸æ“šå¤±æ•—: {e}")
            return False

    def analyze(self) -> Dict[str, Any]:
        """åˆ†ææ•¸æ“šä¸¦ç”Ÿæˆæ‘˜è¦"""
        if not self.data:
            print("[WARNING] æ²’æœ‰æ•¸æ“šå¯åˆ†æ")
            return {}

        self.summary = {
            "metadata": self._analyze_metadata(),
            "training_progress": self._analyze_training_progress(),
            "performance_metrics": self._analyze_performance(),
            "stability_metrics": self._analyze_stability(),
            "diagnostic": self._analyze_diagnostic(),
            "recommendations": self._generate_recommendations()
        }

        return self.summary

    def _analyze_metadata(self) -> Dict[str, Any]:
        """åˆ†æå…ƒæ•¸æ“š"""
        # æ‰¾åˆ°ä»»æ„ä¸€å€‹æŒ‡æ¨™ä¾†ç²å–åŸºæœ¬ä¿¡æ¯
        first_tag = next(iter(self.data.keys()))
        steps = self.data[first_tag]['steps']
        wall_times = self.data[first_tag]['wall_times']

        total_steps = steps[-1] if steps else 0
        start_time = wall_times[0] if wall_times else 0
        end_time = wall_times[-1] if wall_times else 0
        duration = end_time - start_time

        return {
            "total_steps": int(total_steps),
            "num_metrics": len(self.data),
            "duration_seconds": float(duration),
            "duration_hours": float(duration / 3600),
            "start_time": datetime.fromtimestamp(start_time).isoformat() if start_time else None,
            "end_time": datetime.fromtimestamp(end_time).isoformat() if end_time else None,
            "steps_per_second": float(total_steps / duration) if duration > 0 else 0
        }

    def _analyze_training_progress(self) -> Dict[str, Any]:
        """åˆ†æè¨“ç·´é€²åº¦"""
        progress = {}

        # åˆ†æå¹³å‡ episode çå‹µ
        if 'rollout/ep_rew_mean' in self.data:
            values = self.data['rollout/ep_rew_mean']['values']
            steps = self.data['rollout/ep_rew_mean']['steps']

            progress['episode_reward'] = {
                'initial': float(values[0]) if values else None,
                'final': float(values[-1]) if values else None,
                'max': float(max(values)) if values else None,
                'min': float(min(values)) if values else None,
                'mean': float(np.mean(values)) if values else None,
                'std': float(np.std(values)) if values else None,
                'improvement': float(values[-1] - values[0]) if len(values) > 1 else None,
                'trend': self._calculate_trend(steps, values),
                'timeseries': self._sample_timeseries(steps, values)  # â­ æ–°å¢ï¼šæ¡æ¨£æ•¸æ“š
            }

        # åˆ†æå¹³å‡ episode é•·åº¦
        if 'rollout/ep_len_mean' in self.data:
            values = self.data['rollout/ep_len_mean']['values']

            progress['episode_length'] = {
                'mean': float(np.mean(values)) if values else None,
                'final': float(values[-1]) if values else None
            }

        return progress

    def _analyze_performance(self) -> Dict[str, Any]:
        """åˆ†ææ€§èƒ½æŒ‡æ¨™"""
        performance = {}

        # åˆ†ææå¤±
        if 'train/loss' in self.data:
            values = self.data['train/loss']['values']
            steps = self.data['train/loss']['steps']

            performance['total_loss'] = {
                'initial': float(values[0]) if values else None,
                'final': float(values[-1]) if values else None,
                'mean': float(np.mean(values)) if values else None,
                'trend': self._calculate_trend(steps, values),
                'converged': self._check_convergence(values),
                'timeseries': self._sample_timeseries(steps, values)  # â­ æ–°å¢
            }

        # åˆ†æç­–ç•¥æå¤±
        if 'train/policy_loss' in self.data:
            values = self.data['train/policy_loss']['values']

            performance['policy_loss'] = {
                'final': float(values[-1]) if values else None,
                'mean': float(np.mean(values)) if values else None,
                'std': float(np.std(values)) if values else None
            }

        # åˆ†æåƒ¹å€¼æå¤±
        if 'train/value_loss' in self.data:
            values = self.data['train/value_loss']['values']

            performance['value_loss'] = {
                'final': float(values[-1]) if values else None,
                'mean': float(np.mean(values)) if values else None,
                'std': float(np.std(values)) if values else None
            }

        # åˆ†æè§£é‡‹æ–¹å·®
        if 'train/explained_variance' in self.data:
            values = self.data['train/explained_variance']['values']

            performance['explained_variance'] = {
                'final': float(values[-1]) if values else None,
                'mean': float(np.mean(values)) if values else None,
                'is_good': float(np.mean(values)) > 0.7 if values else None
            }

        return performance

    def _analyze_stability(self) -> Dict[str, Any]:
        """åˆ†æè¨“ç·´ç©©å®šæ€§"""
        stability = {}

        # åˆ†æ KL æ•£åº¦
        if 'train/approx_kl' in self.data:
            values = self.data['train/approx_kl']['values']

            stability['kl_divergence'] = {
                'mean': float(np.mean(values)) if values else None,
                'max': float(max(values)) if values else None,
                'is_stable': float(np.mean(values)) < 0.02 if values else None
            }

        # åˆ†æ Clip æ¯”ä¾‹
        if 'train/clip_fraction' in self.data:
            values = self.data['train/clip_fraction']['values']

            stability['clip_fraction'] = {
                'mean': float(np.mean(values)) if values else None,
                'is_good': 0.05 < float(np.mean(values)) < 0.3 if values else None
            }

        # åˆ†æç†µæå¤±
        if 'train/entropy_loss' in self.data:
            values = self.data['train/entropy_loss']['values']

            stability['entropy'] = {
                'final': float(values[-1]) if values else None,
                'mean': float(np.mean(values)) if values else None,
                'trend': self._calculate_trend(
                    self.data['train/entropy_loss']['steps'],
                    values
                )
            }

        # åˆ†æçå‹µç©©å®šæ€§
        if 'rollout/ep_rew_mean' in self.data:
            values = self.data['rollout/ep_rew_mean']['values']

            # è¨ˆç®—æœ€è¿‘ 20% æ•¸æ“šçš„æ¨™æº–å·®
            recent_values = values[-len(values)//5:] if len(values) > 10 else values
            stability['reward_stability'] = {
                'recent_std': float(np.std(recent_values)) if recent_values else None,
                'recent_mean': float(np.mean(recent_values)) if recent_values else None,
                'coefficient_of_variation': float(
                    np.std(recent_values) / np.mean(recent_values)
                ) if recent_values and np.mean(recent_values) != 0 else None
            }

        return stability

    def _analyze_diagnostic(self) -> Dict[str, Any]:
        """è¨ºæ–·æ½›åœ¨å•é¡Œ"""
        issues = []
        warnings = []
        suggestions = []

        # æª¢æŸ¥çå‹µæ˜¯å¦ä¸Šå‡
        if 'rollout/ep_rew_mean' in self.data:
            values = self.data['rollout/ep_rew_mean']['values']
            trend = self._calculate_trend(
                self.data['rollout/ep_rew_mean']['steps'],
                values
            )

            if trend and trend['slope'] < 0:
                issues.append({
                    "type": "çå‹µä¸‹é™",
                    "severity": "high",
                    "message": "å¹³å‡çå‹µå‘ˆä¸‹é™è¶¨å‹¢ï¼Œè¨“ç·´å¯èƒ½å‡ºç¾å•é¡Œ"
                })
            elif trend and abs(trend['slope']) < 0.001:
                warnings.append({
                    "type": "çå‹µä¸è®Š",
                    "severity": "medium",
                    "message": "å¹³å‡çå‹µå¹¾ä¹ä¸è®Šï¼Œå¯èƒ½é™·å…¥å±€éƒ¨æœ€å„ª"
                })

        # æª¢æŸ¥ KL æ•£åº¦
        if 'train/approx_kl' in self.data:
            values = self.data['train/approx_kl']['values']
            mean_kl = np.mean(values) if values else 0

            if mean_kl > 0.02:
                issues.append({
                    "type": "KLæ•£åº¦éé«˜",
                    "severity": "high",
                    "message": f"KLæ•£åº¦å¹³å‡å€¼ {mean_kl:.4f} > 0.02ï¼Œè¨“ç·´å¯èƒ½ä¸ç©©å®š"
                })
                suggestions.append("é™ä½å­¸ç¿’ç‡æˆ–æ¸›å° clip_range")

        # æª¢æŸ¥è§£é‡‹æ–¹å·®
        if 'train/explained_variance' in self.data:
            values = self.data['train/explained_variance']['values']
            mean_ev = np.mean(values) if values else 0

            if mean_ev < 0.5:
                warnings.append({
                    "type": "è§£é‡‹æ–¹å·®ä½",
                    "severity": "medium",
                    "message": f"è§£é‡‹æ–¹å·® {mean_ev:.4f} < 0.5ï¼Œåƒ¹å€¼å‡½æ•¸æ“¬åˆä¸ä½³"
                })
                suggestions.append("å¢åŠ ç¶²çµ¡å®¹é‡æˆ–èª¿æ•´ vf_coef")

        # æª¢æŸ¥æ˜¯å¦æœ‰ NaN
        has_nan = False
        for tag, data in self.data.items():
            if any(np.isnan(v) for v in data['values']):
                has_nan = True
                break

        if has_nan:
            issues.append({
                "type": "NaNæ•¸å€¼",
                "severity": "critical",
                "message": "è¨“ç·´ä¸­å‡ºç¾ NaNï¼Œè¨“ç·´å·²å¤±æ•—"
            })
            suggestions.append("é™ä½å­¸ç¿’ç‡ã€æ¸›å° max_grad_norm")

        return {
            "issues": issues,
            "warnings": warnings,
            "suggestions": suggestions,
            "health_score": self._calculate_health_score(issues, warnings)
        }

    def _sample_timeseries(self, steps: List[int], values: List[float]) -> Dict[str, Any]:
        """æ¡æ¨£æ™‚é–“åºåˆ—æ•¸æ“š

        è¿”å›åˆ†æ®µæ¡æ¨£å’Œ/æˆ–è½‰æŠ˜é»æ¡æ¨£çš„çµæœ
        """
        if not steps or not values:
            return {}

        result = {}

        # åˆ†æ®µæ¡æ¨£
        if self.sampling_mode in ["segment", "both"]:
            result['segment_samples'] = self._segment_sampling(steps, values)

        # è½‰æŠ˜é»æ¡æ¨£
        if self.sampling_mode in ["inflection", "both"]:
            result['inflection_samples'] = self._inflection_sampling(steps, values)

        return result

    def _segment_sampling(self, steps: List[int], values: List[float]) -> List[Dict[str, float]]:
        """åˆ†æ®µæ¡æ¨£ï¼šæ ¹æ“šæ•¸æ“šé‡è‡ªå‹•èª¿æ•´æ®µæ•¸

        è‡ªå‹•èª¿æ•´è¦å‰‡ï¼š
        - < 10 é»ï¼šè¿”å›æ‰€æœ‰é»
        - 10-50 é»ï¼šåˆ†æˆ 5 æ®µ
        - 50-200 é»ï¼šåˆ†æˆ 10 æ®µ
        - 200-1000 é»ï¼šåˆ†æˆ 20 æ®µ
        - 1000-5000 é»ï¼šåˆ†æˆ 50 æ®µ
        - > 5000 é»ï¼šåˆ†æˆ 100 æ®µ
        """
        n = len(steps)

        # æ ¹æ“šæ•¸æ“šé‡è‡ªå‹•ç¢ºå®šæ®µæ•¸
        if n < 10:
            # å¤ªå°‘ï¼Œè¿”å›æ‰€æœ‰é»
            actual_segments = n
        elif n < 50:
            actual_segments = min(5, n)
        elif n < 200:
            actual_segments = min(10, n)
        elif n < 1000:
            actual_segments = min(20, n)
        elif n < 5000:
            actual_segments = min(50, n)
        else:
            actual_segments = min(100, n)

        # å¦‚æœç”¨æˆ¶æŒ‡å®šäº†æ®µæ•¸ï¼Œä½¿ç”¨ç”¨æˆ¶æŒ‡å®šçš„ï¼ˆä½†ä¸è¶…éæ•¸æ“šé»æ•¸ï¼‰
        if self.num_segments > 0:
            actual_segments = min(self.num_segments, n)

        if self.verbose:
            print(f"  æ•¸æ“šé»æ•¸: {n}, æ¡æ¨£æ®µæ•¸: {actual_segments}")

        # å¦‚æœæ®µæ•¸ç­‰æ–¼æ•¸æ“šé»æ•¸ï¼Œè¿”å›æ‰€æœ‰é»
        if actual_segments >= n:
            return [{
                'step': int(s),
                'value': float(v),
                'min': float(v),
                'max': float(v),
                'std': 0.0
            } for s, v in zip(steps, values)]

        segment_size = n // actual_segments
        samples = []

        for i in range(actual_segments):
            start_idx = i * segment_size
            end_idx = (i + 1) * segment_size if i < actual_segments - 1 else n

            # å–è©²æ®µçš„ä¸­é–“ step å’Œå¹³å‡ value
            segment_steps = steps[start_idx:end_idx]
            segment_values = values[start_idx:end_idx]

            mid_step = segment_steps[len(segment_steps) // 2]
            avg_value = np.mean(segment_values)

            samples.append({
                'step': int(mid_step),
                'value': float(avg_value),
                'min': float(np.min(segment_values)),
                'max': float(np.max(segment_values)),
                'std': float(np.std(segment_values))
            })

        return samples

    def _inflection_sampling(self, steps: List[int], values: List[float]) -> List[Dict[str, float]]:
        """è½‰æŠ˜é»æ¡æ¨£ï¼šæª¢æ¸¬è¶¨å‹¢è®ŠåŒ–çš„é—œéµé»

        ä½¿ç”¨æ»‘å‹•çª—å£æª¢æ¸¬æ–œç‡è®ŠåŒ–ï¼Œç•¶æ–œç‡è®ŠåŒ–è¶…éé–¾å€¼æ™‚è¨˜éŒ„ç‚ºè½‰æŠ˜é»
        """
        if len(values) < 10:
            return [{'step': int(s), 'value': float(v)} for s, v in zip(steps, values)]

        samples = []
        window_size = max(5, len(values) // 50)  # çª—å£å¤§å°ï¼šæœ€å°5ï¼Œæœ€å¤§ç‚ºæ•¸æ“šçš„2%

        # å§‹çµ‚åŒ…å«ç¬¬ä¸€å€‹é»
        samples.append({
            'step': int(steps[0]),
            'value': float(values[0]),
            'type': 'start'
        })

        # è¨ˆç®—æ»‘å‹•çª—å£çš„æ–œç‡
        slopes = []
        for i in range(len(values) - window_size):
            window_x = np.array(range(window_size))
            window_y = np.array(values[i:i+window_size])
            slope = np.polyfit(window_x, window_y, 1)[0]
            slopes.append(slope)

        # æª¢æ¸¬æ–œç‡è®ŠåŒ–ï¼ˆè½‰æŠ˜é»ï¼‰
        if slopes:
            slope_std = np.std(slopes)
            threshold = slope_std * self.inflection_sensitivity

            prev_slope = slopes[0]
            for i in range(1, len(slopes)):
                curr_slope = slopes[i]
                slope_change = abs(curr_slope - prev_slope)

                # å¦‚æœæ–œç‡è®ŠåŒ–è¶…éé–¾å€¼ï¼Œè¨˜éŒ„ç‚ºè½‰æŠ˜é»
                if slope_change > threshold:
                    idx = i + window_size // 2
                    if idx < len(steps):
                        # åˆ¤æ–·è½‰æŠ˜é¡å‹
                        if prev_slope < 0 and curr_slope > 0:
                            inflection_type = 'valley'  # è°·åº•ï¼ˆå‘ä¸Šè½‰ï¼‰
                        elif prev_slope > 0 and curr_slope < 0:
                            inflection_type = 'peak'    # å³°é ‚ï¼ˆå‘ä¸‹è½‰ï¼‰
                        else:
                            inflection_type = 'change'  # ä¸€èˆ¬è½‰æŠ˜

                        samples.append({
                            'step': int(steps[idx]),
                            'value': float(values[idx]),
                            'type': inflection_type,
                            'slope_before': float(prev_slope),
                            'slope_after': float(curr_slope)
                        })
                        prev_slope = curr_slope

        # å§‹çµ‚åŒ…å«æœ€å¾Œä¸€å€‹é»
        samples.append({
            'step': int(steps[-1]),
            'value': float(values[-1]),
            'type': 'end'
        })

        return samples

    def _calculate_trend(self, steps: List[int], values: List[float]) -> Optional[Dict[str, float]]:
        """è¨ˆç®—è¶¨å‹¢ï¼ˆç·šæ€§å›æ­¸ï¼‰"""
        if len(values) < 2:
            return None

        # ä½¿ç”¨ numpy é€²è¡Œç·šæ€§å›æ­¸
        x = np.array(steps)
        y = np.array(values)

        # è¨ˆç®—æ–œç‡å’Œæˆªè·
        coeffs = np.polyfit(x, y, 1)
        slope = float(coeffs[0])
        intercept = float(coeffs[1])

        # è¨ˆç®— RÂ²
        y_pred = slope * x + intercept
        ss_res = np.sum((y - y_pred) ** 2)
        ss_tot = np.sum((y - np.mean(y)) ** 2)
        r_squared = 1 - (ss_res / ss_tot) if ss_tot != 0 else 0

        return {
            'slope': slope,
            'intercept': intercept,
            'r_squared': float(r_squared),
            'direction': 'increasing' if slope > 0 else 'decreasing' if slope < 0 else 'stable'
        }

    def _check_convergence(self, values: List[float], window: int = 50) -> bool:
        """æª¢æŸ¥æ˜¯å¦æ”¶æ–‚"""
        if len(values) < window * 2:
            return False

        # æ¯”è¼ƒæœ€è¿‘å…©å€‹çª—å£çš„æ¨™æº–å·®
        recent = values[-window:]
        previous = values[-2*window:-window]

        std_recent = np.std(recent)
        std_previous = np.std(previous)

        # å¦‚æœæœ€è¿‘çš„æ¨™æº–å·®æ˜é¡¯å°æ–¼ä¹‹å‰çš„ï¼Œèªç‚ºå·²æ”¶æ–‚
        return std_recent < std_previous * 0.5

    def _calculate_health_score(self, issues: List[Dict], warnings: List[Dict]) -> float:
        """è¨ˆç®—è¨“ç·´å¥åº·åº¦åˆ†æ•¸ (0-100)"""
        score = 100.0

        # æ¯å€‹ critical issue æ‰£ 50 åˆ†
        critical_count = sum(1 for i in issues if i.get('severity') == 'critical')
        score -= critical_count * 50

        # æ¯å€‹ high issue æ‰£ 20 åˆ†
        high_count = sum(1 for i in issues if i.get('severity') == 'high')
        score -= high_count * 20

        # æ¯å€‹ medium warning æ‰£ 10 åˆ†
        medium_count = sum(1 for w in warnings if w.get('severity') == 'medium')
        score -= medium_count * 10

        return max(0.0, score)

    def _generate_recommendations(self) -> List[str]:
        """ç”Ÿæˆå„ªåŒ–å»ºè­°"""
        recommendations = []

        # åŸºæ–¼çå‹µè¶¨å‹¢
        if 'rollout/ep_rew_mean' in self.data:
            values = self.data['rollout/ep_rew_mean']['values']
            if values:
                final_reward = values[-1]
                improvement = values[-1] - values[0] if len(values) > 1 else 0

                if improvement < 0:
                    recommendations.append({
                        "category": "çå‹µå„ªåŒ–",
                        "priority": "high",
                        "suggestion": "çå‹µä¸‹é™ï¼Œå»ºè­°æª¢æŸ¥çå‹µå‡½æ•¸è¨­è¨ˆæˆ–é™ä½å­¸ç¿’ç‡"
                    })
                elif improvement < np.std(values) * 0.5:
                    recommendations.append({
                        "category": "çå‹µå„ªåŒ–",
                        "priority": "medium",
                        "suggestion": "çå‹µæå‡ç·©æ…¢ï¼Œå¯å˜—è©¦èª¿æ•´ pnl_scale æˆ– cost_penalty"
                    })

        # åŸºæ–¼ç©©å®šæ€§
        if 'train/approx_kl' in self.data:
            kl_values = self.data['train/approx_kl']['values']
            if kl_values and np.mean(kl_values) > 0.02:
                recommendations.append({
                    "category": "ç©©å®šæ€§",
                    "priority": "high",
                    "suggestion": "KLæ•£åº¦éé«˜ï¼Œå»ºè­°é™ä½ learning_rate æˆ– clip_range"
                })

        # åŸºæ–¼è§£é‡‹æ–¹å·®
        if 'train/explained_variance' in self.data:
            ev_values = self.data['train/explained_variance']['values']
            if ev_values and np.mean(ev_values) < 0.7:
                recommendations.append({
                    "category": "æ¨¡å‹å®¹é‡",
                    "priority": "medium",
                    "suggestion": "è§£é‡‹æ–¹å·®åä½ï¼Œå»ºè­°å¢åŠ  lstm_hidden_size æˆ–èª¿æ•´ç¶²çµ¡æ¶æ§‹"
                })

        # åŸºæ–¼è¨“ç·´æ­¥æ•¸
        metadata = self._analyze_metadata()
        if metadata['total_steps'] < 100000:
            recommendations.append({
                "category": "è¨“ç·´æ™‚é•·",
                "priority": "low",
                "suggestion": f"ç•¶å‰è¨“ç·´æ­¥æ•¸è¼ƒå°‘ ({metadata['total_steps']:,})ï¼Œå»ºè­°è‡³å°‘è¨“ç·´ 500K+ steps"
            })

        return recommendations

    def _convert_to_json_serializable(self, obj):
        """éæ­¸è½‰æ› NumPy é¡å‹ç‚º Python åŸç”Ÿé¡å‹"""
        if isinstance(obj, dict):
            return {k: self._convert_to_json_serializable(v) for k, v in obj.items()}
        elif isinstance(obj, list):
            return [self._convert_to_json_serializable(item) for item in obj]
        elif isinstance(obj, np.bool_):
            return bool(obj)
        elif isinstance(obj, (np.integer, np.int64, np.int32)):
            return int(obj)
        elif isinstance(obj, (np.floating, np.float64, np.float32)):
            return float(obj)
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        else:
            return obj

    def save_json(self, output_path: str):
        """ä¿å­˜ JSON å ±å‘Š"""
        # è½‰æ›æ‰€æœ‰ NumPy é¡å‹ç‚º Python åŸç”Ÿé¡å‹
        serializable_summary = self._convert_to_json_serializable(self.summary)

        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(serializable_summary, f, indent=2, ensure_ascii=False)

        print(f"[OK] JSON å ±å‘Šå·²ä¿å­˜: {output_path}")

    def save_markdown(self, output_path: str):
        """ä¿å­˜ Markdown å ±å‘Š"""
        md_content = self._generate_markdown()

        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(md_content)

        print(f"[OK] Markdown å ±å‘Šå·²ä¿å­˜: {output_path}")

    def _generate_markdown(self) -> str:
        """ç”Ÿæˆ Markdown æ ¼å¼å ±å‘Š"""
        lines = []

        # æ¨™é¡Œ
        lines.append("# TensorBoard è¨“ç·´åˆ†æå ±å‘Š\n")
        lines.append(f"**ç”Ÿæˆæ™‚é–“**: {datetime.now().isoformat()}\n")
        lines.append(f"**æ—¥èªŒç›®éŒ„**: `{self.logdir}`\n")
        lines.append("---\n")

        # å…ƒæ•¸æ“š
        if 'metadata' in self.summary:
            meta = self.summary['metadata']
            lines.append("## [STATS] è¨“ç·´æ¦‚æ³\n")
            lines.append(f"- **ç¸½è¨“ç·´æ­¥æ•¸**: {meta['total_steps']:,}")
            lines.append(f"- **è¨“ç·´æ™‚é•·**: {meta['duration_hours']:.2f} å°æ™‚")
            lines.append(f"- **è¨“ç·´é€Ÿåº¦**: {meta['steps_per_second']:.1f} steps/ç§’")
            lines.append(f"- **æŒ‡æ¨™æ•¸é‡**: {meta['num_metrics']}")
            lines.append("")

        # è¨“ç·´é€²åº¦
        if 'training_progress' in self.summary:
            progress = self.summary['training_progress']
            lines.append("## [PROGRESS] è¨“ç·´é€²åº¦\n")

            if 'episode_reward' in progress:
                rew = progress['episode_reward']
                lines.append("### Episode çå‹µ\n")
                lines.append(f"- **åˆå§‹å€¼**: {rew['initial']:.2f}")
                lines.append(f"- **æœ€çµ‚å€¼**: {rew['final']:.2f}")
                lines.append(f"- **æœ€å¤§å€¼**: {rew['max']:.2f}")
                lines.append(f"- **å¹³å‡å€¼**: {rew['mean']:.2f} Â± {rew['std']:.2f}")
                lines.append(f"- **ç¸½æå‡**: {rew['improvement']:.2f}")

                if rew['trend']:
                    trend = rew['trend']
                    lines.append(f"- **è¶¨å‹¢**: {trend['direction']} (RÂ² = {trend['r_squared']:.3f})")

                lines.append("")

        # æ€§èƒ½æŒ‡æ¨™
        if 'performance_metrics' in self.summary:
            perf = self.summary['performance_metrics']
            lines.append("## [METRICS] æ€§èƒ½æŒ‡æ¨™\n")

            if 'total_loss' in perf:
                loss = perf['total_loss']
                lines.append("### ç¸½æå¤±\n")
                lines.append(f"- **åˆå§‹å€¼**: {loss['initial']:.4f}")
                lines.append(f"- **æœ€çµ‚å€¼**: {loss['final']:.4f}")
                lines.append(f"- **å¹³å‡å€¼**: {loss['mean']:.4f}")
                lines.append(f"- **è¶¨å‹¢**: {loss['trend']['direction'] if loss['trend'] else 'N/A'}")
                lines.append(f"- **å·²æ”¶æ–‚**: {'[OK] æ˜¯' if loss['converged'] else '[NO] å¦'}")
                lines.append("")

            if 'explained_variance' in perf:
                ev = perf['explained_variance']
                lines.append("### è§£é‡‹æ–¹å·®\n")
                lines.append(f"- **æœ€çµ‚å€¼**: {ev['final']:.4f}")
                lines.append(f"- **å¹³å‡å€¼**: {ev['mean']:.4f}")
                lines.append(f"- **è©•ä¼°**: {'[OK] è‰¯å¥½ (>0.7)' if ev['is_good'] else '[WARN] åä½ (<0.7)'}")
                lines.append("")

        # ç©©å®šæ€§æŒ‡æ¨™
        if 'stability_metrics' in self.summary:
            stab = self.summary['stability_metrics']
            lines.append("## ğŸ”’ ç©©å®šæ€§åˆ†æ\n")

            if 'kl_divergence' in stab:
                kl = stab['kl_divergence']
                lines.append("### KL æ•£åº¦\n")
                lines.append(f"- **å¹³å‡å€¼**: {kl['mean']:.6f}")
                lines.append(f"- **æœ€å¤§å€¼**: {kl['max']:.6f}")
                lines.append(f"- **ç©©å®šæ€§**: {'[OK] ç©©å®š (<0.02)' if kl['is_stable'] else '[WARN] ä¸ç©©å®š (>0.02)'}")
                lines.append("")

            if 'reward_stability' in stab:
                rs = stab['reward_stability']
                lines.append("### çå‹µç©©å®šæ€§\n")
                lines.append(f"- **æœ€è¿‘å¹³å‡**: {rs['recent_mean']:.2f}")
                lines.append(f"- **æœ€è¿‘æ¨™æº–å·®**: {rs['recent_std']:.2f}")
                if rs['coefficient_of_variation']:
                    lines.append(f"- **è®Šç•°ä¿‚æ•¸**: {rs['coefficient_of_variation']:.3f}")
                lines.append("")

        # è¨ºæ–·çµæœ
        if 'diagnostic' in self.summary:
            diag = self.summary['diagnostic']
            lines.append("## ğŸ¥ è¨ºæ–·å ±å‘Š\n")

            lines.append(f"### å¥åº·åº¦è©•åˆ†: {diag['health_score']:.0f}/100\n")

            if diag['issues']:
                lines.append("### [ISSUES] å•é¡Œ\n")
                for issue in diag['issues']:
                    lines.append(f"- **[{issue['severity'].upper()}]** {issue['type']}: {issue['message']}")
                lines.append("")

            if diag['warnings']:
                lines.append("### [WARNINGS] è­¦å‘Š\n")
                for warning in diag['warnings']:
                    lines.append(f"- **[{warning['severity'].upper()}]** {warning['type']}: {warning['message']}")
                lines.append("")

            if diag['suggestions']:
                lines.append("### ğŸ’¡ å»ºè­°\n")
                for suggestion in diag['suggestions']:
                    lines.append(f"- {suggestion}")
                lines.append("")

        # å„ªåŒ–å»ºè­°
        if 'recommendations' in self.summary:
            recs = self.summary['recommendations']
            lines.append("## ğŸš€ å„ªåŒ–å»ºè­°\n")

            # æŒ‰å„ªå…ˆç´šæ’åº
            high_priority = [r for r in recs if r.get('priority') == 'high']
            medium_priority = [r for r in recs if r.get('priority') == 'medium']
            low_priority = [r for r in recs if r.get('priority') == 'low']

            if high_priority:
                lines.append("### ğŸ”´ é«˜å„ªå…ˆç´š\n")
                for rec in high_priority:
                    lines.append(f"- **{rec['category']}**: {rec['suggestion']}")
                lines.append("")

            if medium_priority:
                lines.append("### ğŸŸ¡ ä¸­å„ªå…ˆç´š\n")
                for rec in medium_priority:
                    lines.append(f"- **{rec['category']}**: {rec['suggestion']}")
                lines.append("")

            if low_priority:
                lines.append("### ğŸŸ¢ ä½å„ªå…ˆç´š\n")
                for rec in low_priority:
                    lines.append(f"- **{rec['category']}**: {rec['suggestion']}")
                lines.append("")

        # çµè«–
        lines.append("## [CONCLUSION] çµè«–\n")

        if 'diagnostic' in self.summary:
            health = self.summary['diagnostic']['health_score']
            if health >= 80:
                lines.append("[OK] **è¨“ç·´ç‹€æ³è‰¯å¥½**ï¼Œå¯ä»¥ç¹¼çºŒç•¶å‰é…ç½®ã€‚")
            elif health >= 60:
                lines.append("[WARN] **è¨“ç·´ç‹€æ³å°šå¯**ï¼Œå»ºè­°æ ¹æ“šä¸Šè¿°å»ºè­°é€²è¡Œå„ªåŒ–ã€‚")
            else:
                lines.append("[ERROR] **è¨“ç·´å­˜åœ¨å•é¡Œ**ï¼Œè«‹å„ªå…ˆè™•ç†é«˜å„ªå…ˆç´šå•é¡Œã€‚")

        lines.append("\n---\n")
        lines.append(f"*æ­¤å ±å‘Šç”± analyze_tensorboard.py è‡ªå‹•ç”Ÿæˆ*")

        return "\n".join(lines)

    def print_summary(self):
        """æ‰“å°æ‘˜è¦åˆ°çµ‚ç«¯"""
        print("\n" + "=" * 70)
        print("TensorBoard è¨“ç·´åˆ†æå ±å‘Š")
        print("=" * 70)

        # æ‰“å° Markdown å…§å®¹
        print(self._generate_markdown())


def compare_experiments(base_dir: str, verbose: bool = False) -> Dict[str, Any]:
    """æ¯”è¼ƒå¤šå€‹å¯¦é©—"""
    base_path = Path(base_dir)

    # æ‰¾åˆ°æ‰€æœ‰ TensorBoard æ—¥èªŒç›®éŒ„ï¼ˆåŒ…å« events.out.tfevents æ–‡ä»¶ï¼‰
    exp_dirs = []
    for item in base_path.rglob("events.out.tfevents*"):
        exp_dir = item.parent
        if exp_dir not in exp_dirs:
            exp_dirs.append(exp_dir)

    if not exp_dirs:
        print(f"[WARNING] åœ¨ {base_dir} ä¸­æœªæ‰¾åˆ° TensorBoard æ—¥èªŒ")
        return {}

    print(f"[INFO] æ‰¾åˆ° {len(exp_dirs)} å€‹å¯¦é©—")

    # åˆ†ææ¯å€‹å¯¦é©—
    results = {}
    for exp_dir in exp_dirs:
        exp_name = exp_dir.name
        print(f"\nåˆ†æå¯¦é©—: {exp_name}")

        analyzer = TensorBoardAnalyzer(exp_dir, verbose=verbose)
        if analyzer.load_data():
            analyzer.analyze()
            results[exp_name] = analyzer.summary

    # ç”Ÿæˆå°æ¯”å ±å‘Š
    comparison = {
        "num_experiments": len(results),
        "experiments": results,
        "ranking": _rank_experiments(results)
    }

    return comparison


def _rank_experiments(results: Dict[str, Dict]) -> List[Dict[str, Any]]:
    """å°å¯¦é©—é€²è¡Œæ’å"""
    rankings = []

    for exp_name, summary in results.items():
        score = 0.0

        # åŸºæ–¼æœ€çµ‚çå‹µ
        if 'training_progress' in summary and 'episode_reward' in summary['training_progress']:
            final_reward = summary['training_progress']['episode_reward'].get('final', 0)
            score += final_reward

        # åŸºæ–¼å¥åº·åº¦
        if 'diagnostic' in summary:
            health = summary['diagnostic'].get('health_score', 0)
            score += health / 10  # æ­¸ä¸€åŒ–

        rankings.append({
            "name": exp_name,
            "score": score,
            "final_reward": summary['training_progress']['episode_reward'].get('final')
            if 'training_progress' in summary and 'episode_reward' in summary['training_progress']
            else None,
            "health_score": summary['diagnostic'].get('health_score')
            if 'diagnostic' in summary else None
        })

    # æŒ‰åˆ†æ•¸æ’åº
    rankings.sort(key=lambda x: x['score'], reverse=True)

    return rankings


def main():
    """ä¸»å‡½æ•¸"""
    args = parse_args()

    if args.compare:
        # æ¯”è¼ƒæ¨¡å¼
        print("[COMPARE] æ¯”è¼ƒæ¨¡å¼: åˆ†æå¤šå€‹å¯¦é©—")
        comparison = compare_experiments(args.logdir, verbose=args.verbose)

        if comparison:
            # ä¿å­˜å°æ¯”çµæœ
            if args.output:
                output_path = args.output
            else:
                output_path = "results/experiment_comparison.json"

            os.makedirs(os.path.dirname(output_path), exist_ok=True)

            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(comparison, f, indent=2, ensure_ascii=False)

            print(f"\n[OK] å°æ¯”å ±å‘Šå·²ä¿å­˜: {output_path}")

            # æ‰“å°æ’å
            print("\n" + "=" * 70)
            print("å¯¦é©—æ’å")
            print("=" * 70)
            for i, rank in enumerate(comparison['ranking'], 1):
                print(f"{i}. {rank['name']}")
                print(f"   åˆ†æ•¸: {rank['score']:.2f}")
                if rank['final_reward'] is not None:
                    print(f"   æœ€çµ‚çå‹µ: {rank['final_reward']:.2f}")
                if rank['health_score'] is not None:
                    print(f"   å¥åº·åº¦: {rank['health_score']:.0f}/100")
                print()

    else:
        # å–®å€‹å¯¦é©—åˆ†æ
        analyzer = TensorBoardAnalyzer(
            args.logdir,
            verbose=args.verbose,
            sampling_mode=args.sampling,
            num_segments=args.segments,
            inflection_sensitivity=args.inflection_sensitivity
        )

        # è¼‰å…¥æ•¸æ“š
        if not analyzer.load_data():
            return 1

        # åˆ†ææ•¸æ“š
        print("\n[ANALYZE] åˆ†ææ•¸æ“š...")
        analyzer.analyze()

        # è¼¸å‡ºçµæœ
        if args.format in ['json', 'both']:
            if args.output:
                json_path = args.output if args.output.endswith('.json') else f"{args.output}.json"
            else:
                json_path = "results/tensorboard_analysis.json"

            os.makedirs(os.path.dirname(json_path), exist_ok=True)
            analyzer.save_json(json_path)

        if args.format in ['markdown', 'both']:
            if args.output:
                md_path = args.output.replace('.json', '.md') if args.output.endswith('.json') else f"{args.output}.md"
            else:
                md_path = "results/tensorboard_analysis.md"

            os.makedirs(os.path.dirname(md_path), exist_ok=True)
            analyzer.save_markdown(md_path)

        # æ‰“å°æ‘˜è¦
        if not args.output:
            analyzer.print_summary()

    return 0


if __name__ == "__main__":
    exit(main())
